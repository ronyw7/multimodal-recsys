{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Restaurant Recommender System based on LightGCN"
      ],
      "metadata": {
        "id": "9A2d2MjyxqG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Packages and Libraries"
      ],
      "metadata": {
        "id": "aO6HhJV4NsX6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxoenua1-u5r"
      },
      "source": [
        "# Install required packages.\n",
        "%%capture\n",
        "import torch\n",
        "import os\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OBSacx6Q03m"
      },
      "source": [
        "# import required modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, nn, optim\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.utils import negative_sampling, structured_negative_sampling\n",
        "from torch_sparse import SparseTensor, matmul"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Loading and Preprocessing\n",
        "\n",
        "Google Restaurants Dataset (the filtered subset ~112M)"
      ],
      "metadata": {
        "id": "sXnpUp2yNqzH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "6cRC_IazQ4Oj",
        "outputId": "9c891a00-2cbc-4a46-a98c-4151b02d514e"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-159adcb1-6329-4f7b-9bf1-83f58d56da63\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-159adcb1-6329-4f7b-9bf1-83f58d56da63\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving filter_all_t.json to filter_all_t.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load JSON file\n",
        "with open('filter_all_t.json', 'r') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "mLEBcsyv5abc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset contains train, val, and test sets.\n",
        "train_data = pd.DataFrame(data[\"train\"])\n",
        "val_data = pd.DataFrame(data[\"val\"])\n",
        "test_data = pd.DataFrame(data[\"test\"])\n",
        "\n",
        "print(\"The dim of training data:\", train_data.shape)\n",
        "print(\"The dim of validation data:\", val_data.shape)\n",
        "print(\"The dim of test data:\", test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atCltB97OqLS",
        "outputId": "1a1c9263-934c-400c-d954-d06162d7bdb3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dim of training data: (87013, 6)\n",
            "The dim of validation data: (10860, 6)\n",
            "The dim of test data: (11015, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "l2WF5WW6OuVe",
        "outputId": "1bd50be0-aea1-4098-a775-b1868aeac224"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                business_id                user_id  rating  \\\n",
              "0  60567465d335d0abfb415b26  101074926318992653684       4   \n",
              "1  6050fa9f5b4ccec8d5cae994  117065749986299237881       5   \n",
              "2  604be10877e81aaed3cc9a1e  106700937793048450809       4   \n",
              "3  60411e017cd8bf130362365a  101643045857250355161       5   \n",
              "4  604139dd7cd8bf1303624208  109802745326785766951       4   \n",
              "\n",
              "                                         review_text  \\\n",
              "0  The tang of the tomato sauce is outstanding. A...   \n",
              "1              Chicken and waffles were really good!   \n",
              "2  The appetizer of colossal shrimp was very good...   \n",
              "3  The fish tacos here  omg! The salad was great ...   \n",
              "4  Ribs are great, as are the mac and cheese, fri...   \n",
              "\n",
              "                                                pics  \\\n",
              "0  [AF1QipM-2IRmvitARbcJr7deWfe5hyVBg_ArPMQSYvq0,...   \n",
              "1     [AF1QipMpfxIZUT_aymQ3qPGO-QgGYzxbtLZGmHufAp2s]   \n",
              "2  [AF1QipMNnqM5X9sSyZ9pXRZ1jvrURHN9bZhGdzuEXoP8,...   \n",
              "3  [AF1QipM-a6AGGp4Hgk5RD0gY5sDRp5kEfB1hZLvlRkft,...   \n",
              "4     [AF1QipNVys4yq-5w_3EsDdHpSc9ZNb7Nl30Mfb6Y0Gup]   \n",
              "\n",
              "                                     history_reviews  \n",
              "0  [[101074926318992653684_6056272797d555cc6fb0d1...  \n",
              "1  [[117065749986299237881_605206f8d8c08f462b93e8...  \n",
              "2  [[106700937793048450809_6044300b27f39b7b5d1dbf...  \n",
              "3  [[101643045857250355161_604fbdd099686c10168c91...  \n",
              "4  [[109802745326785766951_60524fa9f09a4ffff042f9...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3080ea8a-0295-4ceb-85e1-32e23684441c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>review_text</th>\n",
              "      <th>pics</th>\n",
              "      <th>history_reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60567465d335d0abfb415b26</td>\n",
              "      <td>101074926318992653684</td>\n",
              "      <td>4</td>\n",
              "      <td>The tang of the tomato sauce is outstanding. A...</td>\n",
              "      <td>[AF1QipM-2IRmvitARbcJr7deWfe5hyVBg_ArPMQSYvq0,...</td>\n",
              "      <td>[[101074926318992653684_6056272797d555cc6fb0d1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6050fa9f5b4ccec8d5cae994</td>\n",
              "      <td>117065749986299237881</td>\n",
              "      <td>5</td>\n",
              "      <td>Chicken and waffles were really good!</td>\n",
              "      <td>[AF1QipMpfxIZUT_aymQ3qPGO-QgGYzxbtLZGmHufAp2s]</td>\n",
              "      <td>[[117065749986299237881_605206f8d8c08f462b93e8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>604be10877e81aaed3cc9a1e</td>\n",
              "      <td>106700937793048450809</td>\n",
              "      <td>4</td>\n",
              "      <td>The appetizer of colossal shrimp was very good...</td>\n",
              "      <td>[AF1QipMNnqM5X9sSyZ9pXRZ1jvrURHN9bZhGdzuEXoP8,...</td>\n",
              "      <td>[[106700937793048450809_6044300b27f39b7b5d1dbf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60411e017cd8bf130362365a</td>\n",
              "      <td>101643045857250355161</td>\n",
              "      <td>5</td>\n",
              "      <td>The fish tacos here  omg! The salad was great ...</td>\n",
              "      <td>[AF1QipM-a6AGGp4Hgk5RD0gY5sDRp5kEfB1hZLvlRkft,...</td>\n",
              "      <td>[[101643045857250355161_604fbdd099686c10168c91...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>604139dd7cd8bf1303624208</td>\n",
              "      <td>109802745326785766951</td>\n",
              "      <td>4</td>\n",
              "      <td>Ribs are great, as are the mac and cheese, fri...</td>\n",
              "      <td>[AF1QipNVys4yq-5w_3EsDdHpSc9ZNb7Nl30Mfb6Y0Gup]</td>\n",
              "      <td>[[109802745326785766951_60524fa9f09a4ffff042f9...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3080ea8a-0295-4ceb-85e1-32e23684441c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3080ea8a-0295-4ceb-85e1-32e23684441c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3080ea8a-0295-4ceb-85e1-32e23684441c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cb104056-5165-4742-a2c4-174dbec93f83\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cb104056-5165-4742-a2c4-174dbec93f83')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cb104056-5165-4742-a2c4-174dbec93f83 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_data",
              "summary": "{\n  \"name\": \"train_data\",\n  \"rows\": 87013,\n  \"fields\": [\n    {\n      \"column\": \"business_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 27896,\n        \"samples\": [\n          \"60422933b9a6829e686e8033\",\n          \"6046238810ec061e056b3e1e\",\n          \"60442be5cc4f7990c6579e18\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 29596,\n        \"samples\": [\n          \"101991077691640187791\",\n          \"112353495221479251272\",\n          \"109375221199738480469\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5,\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 86138,\n        \"samples\": [\n          \"Stuffed Strawberry cheesecake waffles were to die for.\",\n          \"I got the Melbourne burger and it was cooked perfectly with an excellent runny egg on top.\",\n          \"You can choose pizza, Mexican food, sandwiches or a bowl of fresh fruit and smoothies. Pobresito cantina has great cocktails.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pics\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"history_reviews\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(data):\n",
        "  '''\n",
        "  Args:\n",
        "    data (pd.DataFrame)\n",
        "  Returns:\n",
        "    data (pd.DataFrame)\n",
        "    user_id_map (dict)\n",
        "    business_id_map (dict)\n",
        "  '''\n",
        "  # Create a dictionary mapping user_id to consecutive values [0,..., n]\n",
        "  user_id_map = {idx: i for i, idx in enumerate(data[\"user_id\"].unique())}\n",
        "  # Create a dictionary mapping business_id to consecutive values [0,..., m]\n",
        "  business_id_map = {idx: i for i, idx in enumerate(data[\"business_id\"].unique())}\n",
        "\n",
        "  # Extend the dataframe with new ids\n",
        "  data[\"u_id\"] = data[\"user_id\"].map(user_id_map)\n",
        "  data[\"b_id\"] = data[\"business_id\"].map(business_id_map)\n",
        "  data[\"r_id\"] = data.index\n",
        "\n",
        "  return data, user_id_map, business_id_map"
      ],
      "metadata": {
        "id": "B9rpyRBqvLmC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_user_id_map, train_business_id_map = preprocessing(train_data)\n",
        "val_data, val_user_id_map, val_business_id_map = preprocessing(train_data)\n",
        "test_data, test_user_id_map, test_business_id_map = preprocessing(test_data)\n",
        "\n",
        "all_data = pd.concat([train_data, val_data, test_data])\n",
        "all_data, all_user_id_map, all_business_id_map = preprocessing(all_data)\n",
        "\n",
        "train_num_users, train_num_businesses = len(train_user_id_map), len(train_business_id_map)\n",
        "val_num_users, val_num_businesses = len(val_user_id_map), len(val_business_id_map)\n",
        "test_num_users, test_num_businesses = len(test_user_id_map), len(test_business_id_map)\n",
        "num_users, num_businesses = len(all_user_id_map), len(all_business_id_map)\n",
        "\n",
        "print(\"=============== Training Data ===============\")\n",
        "print(\"The number of users in training data: \", train_num_users)\n",
        "print(\"The number of businesses in training data: \", train_num_businesses)\n",
        "print(\"=============== Validation Data ===============\")\n",
        "print(\"The number of users in validation data: \", val_num_users)\n",
        "print(\"The number of businesses in validation data: \", val_num_businesses)\n",
        "print(\"=============== Test Data ===============\")\n",
        "print(\"The number of users in test data: \", test_num_users)\n",
        "print(\"The number of businesses in test data: \", test_num_businesses)\n",
        "print(\"=============== All Data ===============\")\n",
        "print(\"The number of users in all data: \", num_users)\n",
        "print(\"The number of businesses in training data: \", num_businesses)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcw3IqwXOyy0",
        "outputId": "31476dec-0c55-467b-ab04-dc6cd3c62b91"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== Training Data ===============\n",
            "The number of users in training data:  29596\n",
            "The number of businesses in training data:  27896\n",
            "=============== Validation Data ===============\n",
            "The number of users in validation data:  29596\n",
            "The number of businesses in validation data:  27896\n",
            "=============== Test Data ===============\n",
            "The number of users in test data:  3700\n",
            "The number of businesses in test data:  7880\n",
            "=============== All Data ===============\n",
            "The number of users in all data:  33296\n",
            "The number of businesses in training data:  29454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_edge_index(data, user_id_map, business_id_map):\n",
        "  '''\n",
        "  Args:\n",
        "    data              (pd.DataFrame)\n",
        "    user_id_map       (dict)\n",
        "    business_id_map   (dict)\n",
        "  Returns:\n",
        "    edge_index        (torch.tensor)\n",
        "    edge_index_sparse (SparseTensor)\n",
        "  '''\n",
        "  ##############################################################################\n",
        "  # Generate edge_index and edge_index_sparse (adjacency matrix)\n",
        "  # For now, we assume there is only one edge type between user and business, i.e. review (interaction)\n",
        "  # TODO: segregate reviews into different groups based on review text sentiment and/or rating (positive/neutral/negative)\n",
        "  ##############################################################################\n",
        "  edge_index = [[],[]] # dim = 2 x (# of reviews)\n",
        "  for i in range(len(data)):\n",
        "    edge_index[0].append(user_id_map[data[\"user_id\"][i]])\n",
        "    edge_index[1].append(business_id_map[data[\"business_id\"][i]])\n",
        "  edge_index = torch.tensor(edge_index)\n",
        "\n",
        "  num_nodes = num_users + num_businesses\n",
        "  edge_index_sparse = SparseTensor(\n",
        "      row=torch.tensor(edge_index[0]),\n",
        "      col=torch.tensor(edge_index[1]),\n",
        "      sparse_sizes=(num_nodes, num_nodes))\n",
        "  return edge_index, edge_index_sparse"
      ],
      "metadata": {
        "id": "VpVc_OVIO1bo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a dictionary with key being user and value being positive items for the user\n",
        "def get_user_positive_items(edge_index: Tensor):\n",
        "    '''\n",
        "    Args:\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "    Returns:\n",
        "        user_pos_items (dict): dictionary of positive items for each user\n",
        "    '''\n",
        "    user_pos_items = {}\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        user = edge_index[0][i].item()\n",
        "        item = edge_index[1][i].item()\n",
        "        if user not in user_pos_items:\n",
        "            user_pos_items[user] = []\n",
        "        user_pos_items[user].append(item)\n",
        "    return user_pos_items"
      ],
      "metadata": {
        "id": "aTwKOCt99Ius"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def negative_sampling(edge_index):\n",
        "  # Get all unique users\n",
        "  users = torch.unique(edge_index[0])\n",
        "\n",
        "  # Get all unique items\n",
        "  items = torch.unique(edge_index[1])\n",
        "\n",
        "  user_pos_items = get_user_positive_items(edge_index) # (dict)\n",
        "  user_neg_items = {}\n",
        "  for user in users:\n",
        "    pos_items = user_pos_items[user.item()]\n",
        "    neg_items = list(set(items) - set(pos_items))\n",
        "    user_neg_items[user.item()] = neg_items\n",
        "\n",
        "  random_neg_item = []\n",
        "  for j in range(edge_index.shape[1]):\n",
        "    user = edge_index[0][j].item()\n",
        "    neg_item = random.choice(user_neg_items[user])\n",
        "    random_neg_item = neg_item\n",
        "\n",
        "  random_neg_item = torch.LongTensor(random_neg_item)\n",
        "  edges = torch.stack((edge_index, random_neg_item), dim=0)\n",
        "  return edges\n"
      ],
      "metadata": {
        "id": "YiyrHXcN7_UF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_mini_batch(batch_size, edge_index):\n",
        "  #edges = structured_negative_sampling(edge_index)\n",
        "  #edges = negative_sampling(edge_index,num_nodes=None)\n",
        "  indices = torch.randperm(edge_index.shape[1])[:batch_size]\n",
        "  batch = edge_index[:, indices]\n",
        "  batch_edges = negative_sampling(edge_index)\n",
        "  user_indices, pos_item_indices, neg_item_indices = batch_edges[0], batch_edges[1], batch_edges[2]\n",
        "  return user_indices, pos_item_indices, neg_item_indices\n"
      ],
      "metadata": {
        "id": "kqYkiQrvO3_H"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. LightGCN\n",
        "*   Adjacency matrix of an undirected bipartite graph to indicate the existence of interaction between user and item.\n",
        "\n",
        "*   Normalized adjacency matrix $\\tilde{A}$ as $$\\tilde{A} = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$$\n",
        "\n",
        "*   Final node embedding matrix: $$E^{(K)} = \\tilde{A}^{(K)} E W$$\n",
        "\n",
        "*   Multi-scale diffusion: $$\\alpha_0 E^{(0)} + \\alpha_1 E^{(1)} + \\cdots + \\alpha_k E^{(K)}$$\n",
        "and for simplicity, LightGCN uses the uniform coefficient, i.e., $$\\alpha_k = \\frac{1}{K+1} \\quad \\text{for} \\quad k = 0, \\dots, K$$\n"
      ],
      "metadata": {
        "id": "JQlVv7RSO6W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCNConv(MessagePassing):\n",
        "  def __init__(self, num_users, num_items, embedding_dim, num_layers, add_self_loops=False):\n",
        "    '''\n",
        "    Args:\n",
        "      num_users: number of users\n",
        "      num_items: number of items\n",
        "      embedding_dim: dimensionality of embeddings\n",
        "      num_layers: number of message passing layers\n",
        "      add_self_loops(optional): whether to add self\n",
        "    '''\n",
        "    super().__init__()\n",
        "    self.num_users = num_users\n",
        "    self.num_items = num_items\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_layers = num_layers\n",
        "    self.add_self_loops = add_self_loops\n",
        "\n",
        "    # Substitute the initial embeddings here\n",
        "    self.user_emb = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.embedding_dim)\n",
        "    self.item_emb = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.embedding_dim)\n",
        "\n",
        "    # Random normal initialization\n",
        "    # (other options: uniform/normal Xavier initialization)\n",
        "    nn.init.normal_(self.user_emb.weight, std=0.1)\n",
        "    nn.init.normal_(self.item_emb.weight, std=0.1)\n",
        "\n",
        "  def forward(self, edge_index: SparseTensor):\n",
        "    # Compute normalized adjacency matrix\n",
        "    A_tilde = gcn_norm(edge_index, add_self_loops=self.add_self_loops)\n",
        "\n",
        "    # Concat the embeddings of users and items\n",
        "    emb_0 = torch.cat([self.user_emb.weight, self.item_emb.weight])\n",
        "    embs = [emb_0]\n",
        "\n",
        "    # Multi-scale diffusion\n",
        "    emb_k = emb_0\n",
        "    for k in range(self.num_layers):\n",
        "      emb_k = self.propagate(A_tilde, x=emb_k)\n",
        "      embs.append(emb_k)\n",
        "\n",
        "    embs = torch.stack(embs, dim=1)\n",
        "    emb_final = torch.mean(embs, dim=1) # Uniform coefficient / mean\n",
        "\n",
        "    # Split the embeddings to user embeddings and item embeddings\n",
        "    user_emb_final, item_emb_final = torch.split(emb_final, [self.num_users, self.num_items])\n",
        "    return user_emb_final, self.user_emb.weight, item_emb_final, self.item_emb.weight\n",
        "\n",
        "  def message(self, x_j: Tensor) -> Tensor:\n",
        "    return x_j\n",
        "\n",
        "  def propagate(self, edge_index, x):\n",
        "    x = self.message_and_aggregate(edge_index, x)\n",
        "    return x\n",
        "\n",
        "  def message_and_aggregate(self, adj: SparseTensor, x: Tensor) -> Tensor:\n",
        "    return matmul(adj, x)\n"
      ],
      "metadata": {
        "id": "pB5dQTg4O_ik"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.b Loss Function\n",
        "* We use **Bayesian Personalized Ranking (BPR) loss** here (a personalized surrogate\n",
        "loss that aligns between with the recall@K metric)\n",
        "$$LL_{\\text{BPR}} = -\\frac{1}{|U|} \\sum_{u \\in U} \\frac{1}{|N_u|} \\frac{1}{|N_u^c|} \\sum_{i \\in N_u} \\sum_{j \\notin N_u} \\ln \\sigma(f_{\\theta}(u,i) - f_{\\theta}(u,j))$$ where $f_{\\theta}(u,v)$ is the score function\n",
        "\n",
        "* Mini-batch training for the BPR loss"
      ],
      "metadata": {
        "id": "dQ63w9JQPDuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bpr_loss(user_emb_final, user_emb_0, pos_item_emb_final, pos_item_emb_0, neg_item_emb_final, neg_item_emb_0, lambda_val):\n",
        "  pos_scores = torch.mul(user_emb_final, pos_item_emb_final).sum(dim=-1)\n",
        "  neg_scores = torch.mul(user_emb_final, neg_item_emb_final).sum(dim=-1)\n",
        "  # Add L2 regularization\n",
        "  reg_loss = lambda_val * (torch.norm(user_emb_0) + torch.norm(pos_item_emb_0) + torch.norm(neg_item_emb_0))\n",
        "  loss = -torch.mean(F.softplus(pos_scores - neg_scores)) + reg_loss\n",
        "  return loss"
      ],
      "metadata": {
        "id": "HDcigJblPEp-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.c Evaluation Metrics\n",
        "*   **Recall@K** = # of correctly recommended items in K / Total # of items the user interacted with\n",
        "*   **Precision@K** = # of correctly recommended items in K / K"
      ],
      "metadata": {
        "id": "gGto8VBlPI4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_precision_at_K(P_u, R_u, k):\n",
        "  '''\n",
        "  Args:\n",
        "    P_u: the set of positive items the user will interact in the future\n",
        "    R_u: the set of items recommended by the model\n",
        "  Returns:\n",
        "    recall_at_k (float)\n",
        "    precision_at_k (float)\n",
        "  '''\n",
        "  num_correct_rec = R_u[:, :k].sum(dim=1)\n",
        "  true_pos = torch.Tensor([len(P_u[i]) for i in range(len(P_u))])\n",
        "  recall_at_k = torch.mean(num_correct_rec / true_pos)\n",
        "  precision_at_k = torch.mean(num_correct_rec) / k\n",
        "  return recall_at_k.item(), precision_at_k.item()"
      ],
      "metadata": {
        "id": "8Nak0lNrPIEn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(model, edge_index, edge_index_sparse, exclude_edge_indices, k, lambda_val):\n",
        "    '''\n",
        "    Args:\n",
        "        model (LighGCN): lightGCN model\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "        edge_index_sparse (SparseTensor): sparse adjacency matrix\n",
        "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
        "        k (int): determines the top k items to compute metrics on\n",
        "        lambda_val (float): determines lambda for bpr loss\n",
        "\n",
        "    Returns:\n",
        "        tuple: bpr loss, recall @ k, precision @ k\n",
        "    '''\n",
        "    # Get embeddings\n",
        "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(edge_index_sparse)\n",
        "    edges = negative_sampling(edge_index_sparse)\n",
        "\n",
        "    user_indices, pos_item_indices, neg_item_indices = edges[0], edges[1], edges[2]\n",
        "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[pos_item_indices], items_emb_0[pos_item_indices]\n",
        "    neg_items_emb_final, neg_items_emb_0 = items_emb_final[neg_item_indices], items_emb_0[neg_item_indices]\n",
        "\n",
        "    # Compute loss\n",
        "    loss = bpr_loss(users_emb_final, users_emb_0,\n",
        "                    pos_items_emb_final, pos_items_emb_0,\n",
        "                    neg_items_emb_final, neg_items_emb_0,\n",
        "                    lambda_val).item()\n",
        "\n",
        "    user_embedding = model.users_emb.weight\n",
        "    item_embedding = model.items_emb.weight\n",
        "\n",
        "    # Compute score between users and items\n",
        "    score = torch.matmul(user_embedding, item_embedding.T)\n",
        "\n",
        "    # Exclude existing edges\n",
        "    for exclude_edge_index in exclude_edge_indices:\n",
        "        # Get all the positive items for each user from exclude_edge_index\n",
        "        user_pos_items = get_user_positive_items(exclude_edge_index)\n",
        "        exclude_users = []\n",
        "        exclude_items = []\n",
        "        for user, items in user_pos_items.items():\n",
        "            exclude_users.extend([user] * len(items))\n",
        "            exclude_items.extend(items)\n",
        "        score[exclude_users, exclude_items] = float('-inf')\n",
        "\n",
        "    # Get the top k recommended items for each user\n",
        "    _, top_K_items = torch.topk(score, k=k)\n",
        "\n",
        "    # Get all unique users\n",
        "    users = edge_index[0].unique()\n",
        "\n",
        "    # Get P_u, the set of positive items the user will interact in the future\n",
        "    user_pos_items = get_user_positive_items(edge_index) # (dict)\n",
        "    P_u = [user_pos_items[user.item()] for user in users] # (list)\n",
        "\n",
        "    # Get the correctly recommended labels in R_u (intersection of P_u and R_u)\n",
        "    R_u = []\n",
        "    for user in users:\n",
        "        pos_items = user_pos_items[user.item()]\n",
        "        label = list(map(lambda x: x in pos_items, top_K_items[user]))\n",
        "        R_u.append(label)\n",
        "    R_u = torch.Tensor(np.array(R_u).astype('float'))\n",
        "\n",
        "    # Compute recall@k & precision@k\n",
        "    recall, precision = recall_precision_at_K(P_u, R_u, k)\n",
        "\n",
        "    return loss, recall, precision"
      ],
      "metadata": {
        "id": "Ca2NtjFKaLqP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training"
      ],
      "metadata": {
        "id": "xb2m85L6UzN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model args\n",
        "args = {\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'embedding_dim': 32,\n",
        "    'num_layers': 3,\n",
        "    'topK': 20,\n",
        "    'lr': 1e-3,\n",
        "    'weight_decay': 5e-2,\n",
        "    'batch_size': 256,\n",
        "    'num_epoch': 50,\n",
        "    'epoch_size': 100,\n",
        "    'lambda': 1e-6,\n",
        "}"
      ],
      "metadata": {
        "id": "TOJl3pdZiMu9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is not used for now. We can use different optimizers if needed.\n",
        "def build_optimizer(args, params):\n",
        "    weight_decay = args.weight_decay\n",
        "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
        "    if args.opt == 'adam':\n",
        "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'sgd':\n",
        "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
        "    elif args.opt == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'adagrad':\n",
        "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    if args.opt_scheduler == 'none':\n",
        "        return None, optimizer\n",
        "    elif args.opt_scheduler == 'step':\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
        "    elif args.opt_scheduler == 'cos':\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
        "    return scheduler, optimizer"
      ],
      "metadata": {
        "id": "8vnbYmVsXO4s"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device {device}.\")\n",
        "\n",
        "model = LightGCNConv(num_users, num_businesses, args['embedding_dim'], args['num_layers']) # LightGCN model\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=args['weight_decay'])\n",
        "\n",
        "train_edge_index, train_edge_index_sparse = generate_edge_index(train_data, train_user_id_map, train_business_id_map)\n",
        "train_edge_index = train_edge_index.to(device)\n",
        "train_edge_index_sparse = train_edge_index_sparse.to(device)\n",
        "\n",
        "val_edge_index, val_edge_index_sparse = generate_edge_index(val_data, val_user_id_map, val_business_id_map)\n",
        "val_edge_index = val_edge_index.to(device)\n",
        "val_edge_index_sparse = val_edge_index_sparse.to(device)\n",
        "\n",
        "test_edge_index, test_edge_index_sparse = generate_edge_index(test_data, test_user_id_map, test_business_id_map)\n",
        "test_edge_index = test_edge_index.to(device)\n",
        "test_edge_index_sparse = test_edge_index_sparse.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3Y_5eGLUuzy",
        "outputId": "5d041aa6-cddf-48cd-e6b5-8c9b272ad59b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-bb132707cafa>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  row=torch.tensor(edge_index[0]),\n",
            "<ipython-input-9-bb132707cafa>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  col=torch.tensor(edge_index[1]),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(args['num_epoch']):\n",
        "  for iter in range(args['epoch_size']):\n",
        "    # Forward propagation\n",
        "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(train_edge_index_sparse)\n",
        "\n",
        "    # Mini batching\n",
        "    user_indices, pos_item_indices, neg_item_indices = sample_mini_batch(args['batch_size'], train_edge_index)\n",
        "    user_indices = user_indices.to(device)\n",
        "    pos_item_indices = pos_item_indices.to(device)\n",
        "    neg_item_indices = neg_item_indices.to(device)\n",
        "\n",
        "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[pos_item_indices], items_emb_0[pos_item_indices]\n",
        "    neg_items_emb_0 = items_emb_0[neg_item_indices]\n",
        "    neg_items_emb_final = items_emb_final[neg_item_indices]\n",
        "    neg_items_emb_0 = items_emb_0[neg_item_indices]\n",
        "\n",
        "    # Compute loss\n",
        "    train_loss = bpr_loss(users_emb_final, users_emb_0,\n",
        "                          pos_items_emb_final, pos_items_emb_0,\n",
        "                          neg_items_emb_final, neg_items_emb_0,\n",
        "                          args['lambda'])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  val_loss, recall, precision = evaluation(model, val_edge_index, val_edge_index_sparse, [train_edge_index], args['topK'], args['lambda'])\n",
        "  print('Epoch {:d}: train_loss: {:.4f}, val_loss: {:.4f}, recall: {:.4f}, precision: {:.4f}'\\\n",
        "        .format(epoch, train_loss, val_loss, recall, precision))\n",
        "  train_losses.append(train_loss.item())\n",
        "  val_losses.append(val_loss)\n",
        "  scheduler.step()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Ri2Cf5CcV8Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w4yacM6k6xiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8HmRrxm6yeF",
        "outputId": "ff3f1811-46bf-4804-9609-306f5e84eaa6"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87013"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "items_emb_final.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMx9MiQ8utoU",
        "outputId": "20179e89-089e-4f3e-beed-035a2f384163"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([27896, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(args['num_epoch'])\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "ax.plot(epochs, train_losses, color='r', label='Train', alpha=1)\n",
        "ax.plot(epochs, val_losses, color='b', label='Validation', alpha=1)\n",
        "ax.grid(color='g', ls='-.', lw=0.5)\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Losses')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9xwPayiYoMPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate on test set"
      ],
      "metadata": {
        "id": "iukbgXq8ocs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_recall, test_precision = evaluation(model, test_edge_index, test_edge_index_sparse, [train_edge_index, val_edge_index], args['topK'], args['lambda'])\n",
        "print('Evaluation on test set: loss: {:.4f}, recall: {:.4f}, precision: {:.4f}'\\\n",
        "        .format(test_loss, test_recall, test_precision))"
      ],
      "metadata": {
        "id": "wIdwM1XvoRl8"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}