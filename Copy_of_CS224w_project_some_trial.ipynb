{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Restaurant Recommender System based on LightGCN"
      ],
      "metadata": {
        "id": "9A2d2MjyxqG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Packages and Libraries"
      ],
      "metadata": {
        "id": "aO6HhJV4NsX6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxoenua1-u5r"
      },
      "source": [
        "# Install required packages.\n",
        "%%capture\n",
        "import torch\n",
        "import os\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OBSacx6Q03m"
      },
      "source": [
        "# import required modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, nn, optim\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.utils import negative_sampling, structured_negative_sampling\n",
        "from torch_sparse import SparseTensor, matmul"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Loading and Preprocessing\n",
        "\n",
        "Google Restaurants Dataset (the filtered subset ~112M)\n",
        "\n",
        "We read the json file, and transform the data into tensor node embeddings that are suitable for model training"
      ],
      "metadata": {
        "id": "sXnpUp2yNqzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal_restaurants/filter_all_t.json -O filter_all_t.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYb5Gt1gH5dN",
        "outputId": "0083be3b-3393-4b4d-ac97-163f3fa346e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-09 07:54:03--  https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal_restaurants/filter_all_t.json\n",
            "Resolving datarepo.eng.ucsd.edu (datarepo.eng.ucsd.edu)... 132.239.8.30\n",
            "Connecting to datarepo.eng.ucsd.edu (datarepo.eng.ucsd.edu)|132.239.8.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 117893558 (112M) [application/json]\n",
            "Saving to: ‘filter_all_t.json’\n",
            "\n",
            "filter_all_t.json   100%[===================>] 112.43M  65.1MB/s    in 1.7s    \n",
            "\n",
            "2024-12-09 07:54:05 (65.1 MB/s) - ‘filter_all_t.json’ saved [117893558/117893558]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load JSON file\n",
        "with open('filter_all_t.json', 'r') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "mLEBcsyv5abc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset contains train, val, and test sets.\n",
        "train_data = pd.DataFrame(data[\"train\"])\n",
        "val_data = pd.DataFrame(data[\"val\"])\n",
        "test_data = pd.DataFrame(data[\"test\"])\n",
        "\n",
        "print(\"The dim of training data:\", train_data.shape)\n",
        "print(\"The dim of validation data:\", val_data.shape)\n",
        "print(\"The dim of test data:\", test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atCltB97OqLS",
        "outputId": "19b25629-5202-461e-c779-b1bdb34a6df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dim of training data: (87013, 6)\n",
            "The dim of validation data: (10860, 6)\n",
            "The dim of test data: (11015, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "l2WF5WW6OuVe",
        "outputId": "37ba63cc-e419-486a-d556-516d304c6af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                business_id                user_id  rating  \\\n",
              "0  60567465d335d0abfb415b26  101074926318992653684       4   \n",
              "1  6050fa9f5b4ccec8d5cae994  117065749986299237881       5   \n",
              "2  604be10877e81aaed3cc9a1e  106700937793048450809       4   \n",
              "3  60411e017cd8bf130362365a  101643045857250355161       5   \n",
              "4  604139dd7cd8bf1303624208  109802745326785766951       4   \n",
              "\n",
              "                                         review_text  \\\n",
              "0  The tang of the tomato sauce is outstanding. A...   \n",
              "1              Chicken and waffles were really good!   \n",
              "2  The appetizer of colossal shrimp was very good...   \n",
              "3  The fish tacos here  omg! The salad was great ...   \n",
              "4  Ribs are great, as are the mac and cheese, fri...   \n",
              "\n",
              "                                                pics  \\\n",
              "0  [AF1QipM-2IRmvitARbcJr7deWfe5hyVBg_ArPMQSYvq0,...   \n",
              "1     [AF1QipMpfxIZUT_aymQ3qPGO-QgGYzxbtLZGmHufAp2s]   \n",
              "2  [AF1QipMNnqM5X9sSyZ9pXRZ1jvrURHN9bZhGdzuEXoP8,...   \n",
              "3  [AF1QipM-a6AGGp4Hgk5RD0gY5sDRp5kEfB1hZLvlRkft,...   \n",
              "4     [AF1QipNVys4yq-5w_3EsDdHpSc9ZNb7Nl30Mfb6Y0Gup]   \n",
              "\n",
              "                                     history_reviews  \n",
              "0  [[101074926318992653684_6056272797d555cc6fb0d1...  \n",
              "1  [[117065749986299237881_605206f8d8c08f462b93e8...  \n",
              "2  [[106700937793048450809_6044300b27f39b7b5d1dbf...  \n",
              "3  [[101643045857250355161_604fbdd099686c10168c91...  \n",
              "4  [[109802745326785766951_60524fa9f09a4ffff042f9...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1276043c-e154-45dd-8572-ed47c4645aaa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>review_text</th>\n",
              "      <th>pics</th>\n",
              "      <th>history_reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60567465d335d0abfb415b26</td>\n",
              "      <td>101074926318992653684</td>\n",
              "      <td>4</td>\n",
              "      <td>The tang of the tomato sauce is outstanding. A...</td>\n",
              "      <td>[AF1QipM-2IRmvitARbcJr7deWfe5hyVBg_ArPMQSYvq0,...</td>\n",
              "      <td>[[101074926318992653684_6056272797d555cc6fb0d1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6050fa9f5b4ccec8d5cae994</td>\n",
              "      <td>117065749986299237881</td>\n",
              "      <td>5</td>\n",
              "      <td>Chicken and waffles were really good!</td>\n",
              "      <td>[AF1QipMpfxIZUT_aymQ3qPGO-QgGYzxbtLZGmHufAp2s]</td>\n",
              "      <td>[[117065749986299237881_605206f8d8c08f462b93e8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>604be10877e81aaed3cc9a1e</td>\n",
              "      <td>106700937793048450809</td>\n",
              "      <td>4</td>\n",
              "      <td>The appetizer of colossal shrimp was very good...</td>\n",
              "      <td>[AF1QipMNnqM5X9sSyZ9pXRZ1jvrURHN9bZhGdzuEXoP8,...</td>\n",
              "      <td>[[106700937793048450809_6044300b27f39b7b5d1dbf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60411e017cd8bf130362365a</td>\n",
              "      <td>101643045857250355161</td>\n",
              "      <td>5</td>\n",
              "      <td>The fish tacos here  omg! The salad was great ...</td>\n",
              "      <td>[AF1QipM-a6AGGp4Hgk5RD0gY5sDRp5kEfB1hZLvlRkft,...</td>\n",
              "      <td>[[101643045857250355161_604fbdd099686c10168c91...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>604139dd7cd8bf1303624208</td>\n",
              "      <td>109802745326785766951</td>\n",
              "      <td>4</td>\n",
              "      <td>Ribs are great, as are the mac and cheese, fri...</td>\n",
              "      <td>[AF1QipNVys4yq-5w_3EsDdHpSc9ZNb7Nl30Mfb6Y0Gup]</td>\n",
              "      <td>[[109802745326785766951_60524fa9f09a4ffff042f9...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1276043c-e154-45dd-8572-ed47c4645aaa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1276043c-e154-45dd-8572-ed47c4645aaa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1276043c-e154-45dd-8572-ed47c4645aaa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fe77c299-40ad-41ce-99cd-8b3135f0d128\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fe77c299-40ad-41ce-99cd-8b3135f0d128')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fe77c299-40ad-41ce-99cd-8b3135f0d128 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_data",
              "summary": "{\n  \"name\": \"train_data\",\n  \"rows\": 87013,\n  \"fields\": [\n    {\n      \"column\": \"business_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 27896,\n        \"samples\": [\n          \"60422933b9a6829e686e8033\",\n          \"6046238810ec061e056b3e1e\",\n          \"60442be5cc4f7990c6579e18\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 29596,\n        \"samples\": [\n          \"101991077691640187791\",\n          \"112353495221479251272\",\n          \"109375221199738480469\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5,\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 86138,\n        \"samples\": [\n          \"Stuffed Strawberry cheesecake waffles were to die for.\",\n          \"I got the Melbourne burger and it was cooked perfectly with an excellent runny egg on top.\",\n          \"You can choose pizza, Mexican food, sandwiches or a bowl of fresh fruit and smoothies. Pobresito cantina has great cocktails.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pics\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"history_reviews\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(data):\n",
        "  '''\n",
        "  Args:\n",
        "    data (pd.DataFrame)\n",
        "  Returns:\n",
        "    data (pd.DataFrame)\n",
        "    user_id_map (dict)\n",
        "    business_id_map (dict)\n",
        "  '''\n",
        "  # Create a dictionary mapping user_id to consecutive values [0,..., n]\n",
        "  user_id_map = {idx: i for i, idx in enumerate(data[\"user_id\"].unique())}\n",
        "  # Create a dictionary mapping business_id to consecutive values [0,..., m]\n",
        "  business_id_map = {idx: i for i, idx in enumerate(data[\"business_id\"].unique())}\n",
        "\n",
        "  # Extend the dataframe with new ids\n",
        "  data[\"u_id\"] = data[\"user_id\"].map(user_id_map)\n",
        "  data[\"b_id\"] = data[\"business_id\"].map(business_id_map)\n",
        "  data[\"r_id\"] = data.index\n",
        "\n",
        "  return data, user_id_map, business_id_map"
      ],
      "metadata": {
        "id": "B9rpyRBqvLmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_user_id_map, train_business_id_map = preprocessing(train_data)\n",
        "val_data, val_user_id_map, val_business_id_map = preprocessing(val_data)\n",
        "test_data, test_user_id_map, test_business_id_map = preprocessing(test_data)\n",
        "\n",
        "all_data = pd.concat([train_data, val_data, test_data])\n",
        "all_data, all_user_id_map, all_business_id_map = preprocessing(all_data)\n",
        "\n",
        "train_num_users, train_num_businesses = len(train_user_id_map), len(train_business_id_map)\n",
        "val_num_users, val_num_businesses = len(val_user_id_map), len(val_business_id_map)\n",
        "test_num_users, test_num_businesses = len(test_user_id_map), len(test_business_id_map)\n",
        "num_users, num_businesses = len(all_user_id_map), len(all_business_id_map)\n",
        "\n",
        "print(\"=============== Training Data ===============\")\n",
        "print(\"The number of users in training data: \", train_num_users)\n",
        "print(\"The number of businesses in training data: \", train_num_businesses)\n",
        "print(\"=============== Validation Data ===============\")\n",
        "print(\"The number of users in validation data: \", val_num_users)\n",
        "print(\"The number of businesses in validation data: \", val_num_businesses)\n",
        "print(\"=============== Test Data ===============\")\n",
        "print(\"The number of users in test data: \", test_num_users)\n",
        "print(\"The number of businesses in test data: \", test_num_businesses)\n",
        "print(\"=============== All Data ===============\")\n",
        "print(\"The number of users in all data: \", num_users)\n",
        "print(\"The number of businesses in training data: \", num_businesses)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcw3IqwXOyy0",
        "outputId": "1024e38e-9945-4cae-fb1e-fbe91b5ae29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== Training Data ===============\n",
            "The number of users in training data:  29596\n",
            "The number of businesses in training data:  27896\n",
            "=============== Validation Data ===============\n",
            "The number of users in validation data:  3700\n",
            "The number of businesses in validation data:  7835\n",
            "=============== Test Data ===============\n",
            "The number of users in test data:  3700\n",
            "The number of businesses in test data:  7880\n",
            "=============== All Data ===============\n",
            "The number of users in all data:  36996\n",
            "The number of businesses in training data:  30831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write code to check if val_data['business_id'] is in train_data['business_id']\n",
        "print(len(set(val_data['business_id']) - set(train_data['business_id'])))\n",
        "print(len(set(val_data['business_id'])))\n",
        "print(len(set(train_data['business_id'])))\n",
        "\n",
        "print(len(set(val_data['user_id']) - set(train_data['user_id'])))\n",
        "print(len(set(val_data['user_id'])))\n",
        "print(len(set(train_data['user_id'])))"
      ],
      "metadata": {
        "id": "tSCgiYxNc1bq",
        "outputId": "10ee2e0a-f9ff-4604-eada-73745000a81c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1553\n",
            "7835\n",
            "27896\n",
            "3700\n",
            "3700\n",
            "29596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(set(test_data['business_id']) - set(train_data['business_id'])))\n",
        "print(len(set(test_data['business_id'])))\n",
        "print(len(set(train_data['business_id'])))\n",
        "print(len(set(val_data['user_id']) - set(train_data['user_id'])))\n",
        "print(len(set(val_data['user_id'])))\n",
        "print(len(set(train_data['user_id'])))"
      ],
      "metadata": {
        "id": "YwFk70obdTz2",
        "outputId": "ed12b8e7-3cdb-4e78-8828-95bda03dcab3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1558\n",
            "7880\n",
            "27896\n",
            "3700\n",
            "3700\n",
            "29596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_edge_index(data, user_id_map, business_id_map):\n",
        "  '''\n",
        "  Args:\n",
        "    data              (pd.DataFrame)\n",
        "    user_id_map       (dict)\n",
        "    business_id_map   (dict)\n",
        "  Returns:\n",
        "    edge_index        (torch.tensor)\n",
        "    edge_index_sparse (SparseTensor)\n",
        "  '''\n",
        "  ##############################################################################\n",
        "  # Generate edge_index and edge_index_sparse (adjacency matrix)\n",
        "  # For now, we assume there is only one edge type between user and business, i.e. review (interaction)\n",
        "  # TODO: segregate reviews into different groups based on review text sentiment and/or rating (positive/neutral/negative)\n",
        "  ##############################################################################\n",
        "  edge_index = [[],[]] # dim = 2 x (# of reviews)\n",
        "  for i in range(len(data)):\n",
        "    edge_index[0].append(user_id_map[data[\"user_id\"][i]])\n",
        "    edge_index[1].append(business_id_map[data[\"business_id\"][i]] + len(user_id_map))\n",
        "  edge_index = torch.tensor(edge_index)\n",
        "\n",
        "  num_nodes = num_users + num_businesses\n",
        "  edge_index_sparse = SparseTensor(\n",
        "      row=torch.tensor(edge_index[0]),\n",
        "      col=torch.tensor(edge_index[1]),\n",
        "      sparse_sizes=(num_nodes, num_nodes))\n",
        "  return edge_index, edge_index_sparse"
      ],
      "metadata": {
        "id": "VpVc_OVIO1bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_users, num_businesses = len(train_user_id_map), len(train_business_id_map)\n",
        "\n",
        "num_nodes = num_users + num_businesses"
      ],
      "metadata": {
        "id": "90XVu4k7tx53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "\n",
        "train_edge_index, train_edge_index_sparse = generate_edge_index(train_data, train_user_id_map, train_business_id_map)\n",
        "\n",
        "train_num_users, train_num_businesses = len(train_user_id_map), len(train_business_id_map)\n",
        "\n",
        "num_users, num_businesses = len(train_user_id_map), len(train_business_id_map)\n",
        "\n",
        "num_nodes = num_users + num_businesses\n",
        "\n",
        "graph_data = Data(edge_index = train_edge_index, num_nodes = num_nodes)\n",
        "\n",
        "# convert to train/val/test splits\n",
        "transform = RandomLinkSplit(\n",
        "    is_undirected=True,\n",
        "    add_negative_train_samples=False,\n",
        "    neg_sampling_ratio=0,\n",
        "    num_val=0.15, num_test=0.15\n",
        ")\n",
        "train_split, val_split, test_split = transform(graph_data)"
      ],
      "metadata": {
        "id": "Xp0bQXuMOa8k",
        "outputId": "14f3f22e-f3bd-4241-efdc-ebe71d9a4a3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-fc709e669145>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  row=torch.tensor(edge_index[0]),\n",
            "<ipython-input-9-fc709e669145>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  col=torch.tensor(edge_index[1]),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Edge index: message passing edges\n",
        "train_split.edge_index = train_split.edge_index.type(torch.int64)\n",
        "val_split.edge_index = val_split.edge_index.type(torch.int64)\n",
        "test_split.edge_index = test_split.edge_index.type(torch.int64)\n",
        "# Edge label index: supervision edges\n",
        "train_split.edge_label_index = train_split.edge_label_index.type(torch.int64)\n",
        "val_split.edge_label_index = val_split.edge_label_index.type(torch.int64)\n",
        "test_split.edge_label_index = test_split.edge_label_index.type(torch.int64)\n",
        "\n",
        "print(f\"Train set has {train_split.edge_label_index.shape[1]} positive supervision edges\")\n",
        "print(f\"Validation set has {val_split.edge_label_index.shape[1]} positive supervision edges\")\n",
        "print(f\"Test set has {test_split.edge_label_index.shape[1]} positive supervision edges\")\n",
        "\n",
        "print(f\"Train set has {train_split.edge_index.shape[1]} message passing edges\")\n",
        "print(f\"Validation set has {val_split.edge_index.shape[1]} message passing edges\")\n",
        "print(f\"Test set has {test_split.edge_index.shape[1]} message passing edges\")"
      ],
      "metadata": {
        "id": "vWxMxGURQTer",
        "outputId": "4af244ed-6b57-429a-ce89-b57d6e45c64c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set has 60911 positive supervision edges\n",
            "Validation set has 13051 positive supervision edges\n",
            "Test set has 13051 positive supervision edges\n",
            "Train set has 121822 message passing edges\n",
            "Validation set has 121822 message passing edges\n",
            "Test set has 147924 message passing edges\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try something"
      ],
      "metadata": {
        "id": "nr34rToTQmKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_playlists = num_users\n",
        "n_tracks = num_businesses"
      ],
      "metadata": {
        "id": "Z-DytQk7S6oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import relevant ML libraries\n",
        "from typing import Optional, Union\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.nn import Embedding, ModuleList, Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric\n",
        "import torch_geometric.nn as pyg_nn\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch.nn.modules.loss import _Loss\n",
        "\n",
        "from torch_geometric.nn.conv import LGConv, GATConv, SAGEConv\n",
        "from torch_geometric.typing import Adj, OptTensor, SparseTensor"
      ],
      "metadata": {
        "id": "d05uJfldQ3Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BFmIpTDPQoRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "      Here we adapt the LightGCN model from Torch Geometric for our purposes. We allow\n",
        "      for customizable convolutional layers, custom embeddings. In addition, we deifne some\n",
        "      additional custom functions.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_nodes: int,\n",
        "        embedding_dim: int,\n",
        "        num_layers: int,\n",
        "        alpha: Optional[Union[float, Tensor]] = None,\n",
        "        alpha_learnable = False,\n",
        "        conv_layer = \"LGC\",\n",
        "        name = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        alpha_string = \"alpha\" if alpha_learnable else \"\"\n",
        "        self.name = f\"LGCN_{conv_layer}_{num_layers}_e{embedding_dim}_nodes{num_nodes}_{alpha_string}\"\n",
        "        self.num_nodes = num_nodes\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        if alpha_learnable == True:\n",
        "          alpha_vals = torch.rand(num_layers+1)\n",
        "          alpha = nn.Parameter(alpha_vals/torch.sum(alpha_vals))\n",
        "          print(f\"Alpha learnable, initialized to: {alpha.softmax(dim=-1)}\")\n",
        "        else:\n",
        "          if alpha is None:\n",
        "              alpha = 1. / (num_layers + 1)\n",
        "\n",
        "          if isinstance(alpha, Tensor):\n",
        "              assert alpha.size(0) == num_layers + 1\n",
        "          else:\n",
        "              alpha = torch.tensor([alpha] * (num_layers + 1))\n",
        "\n",
        "        self.register_buffer('alpha', alpha)\n",
        "\n",
        "        self.embedding = Embedding(num_nodes, embedding_dim)\n",
        "\n",
        "        # initialize convolutional layers\n",
        "        self.conv_layer = conv_layer\n",
        "        if conv_layer == \"LGC\":\n",
        "          self.convs = ModuleList([LGConv(**kwargs) for _ in range(num_layers)])\n",
        "        elif conv_layer == \"GAT\":\n",
        "          # initialize Graph Attention layer with multiple heads\n",
        "          # initialize linear layers to aggregate heads\n",
        "          n_heads = 5\n",
        "          self.convs = ModuleList(\n",
        "              [GATConv(in_channels = embedding_dim, out_channels = embedding_dim, heads = n_heads, dropout = 0.5, **kwargs) for _ in range(num_layers)]\n",
        "          )\n",
        "          self.linears = ModuleList([Linear(n_heads * embedding_dim, embedding_dim) for _ in range(num_layers)])\n",
        "\n",
        "        elif conv_layer == \"SAGE\":\n",
        "          #  initialize GraphSAGE conv\n",
        "          self.convs = ModuleList(\n",
        "              [SAGEConv(in_channels = embedding_dim, out_channels = embedding_dim, **kwargs) for _ in range(num_layers)]\n",
        "          )\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def get_embedding(self, edge_index: Adj) -> Tensor:\n",
        "        x = self.embedding.weight\n",
        "\n",
        "        weights = self.alpha.softmax(dim=-1)\n",
        "        out = x * weights[0]\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            if self.conv_layer == \"GAT\":\n",
        "              x = self.linears[i](x)\n",
        "            out = out + x * weights[i + 1]\n",
        "\n",
        "        return out\n",
        "\n",
        "    def initialize_embeddings(self, data):\n",
        "      # initialize with the data node features\n",
        "        self.embedding.weight.data.copy_(data.node_feature)\n",
        "\n",
        "\n",
        "    def forward(self, edge_index: Adj,\n",
        "                edge_label_index: OptTensor = None) -> Tensor:\n",
        "        if edge_label_index is None:\n",
        "            if isinstance(edge_index, SparseTensor):\n",
        "                edge_label_index = torch.stack(edge_index.coo()[:2], dim=0)\n",
        "            else:\n",
        "                edge_label_index = edge_index\n",
        "\n",
        "        out = self.get_embedding(edge_index)\n",
        "\n",
        "        return self.predict_link_embedding(out, edge_label_index)\n",
        "\n",
        "    def predict_link(self, edge_index: Adj, edge_label_index: OptTensor = None,\n",
        "                     prob: bool = False) -> Tensor:\n",
        "\n",
        "        pred = self(edge_index, edge_label_index).sigmoid()\n",
        "        return pred if prob else pred.round()\n",
        "\n",
        "    def predict_link_embedding(self, embed: Adj, edge_label_index: Adj) -> Tensor:\n",
        "\n",
        "        embed_src = embed[edge_label_index[0]]\n",
        "        embed_dst = embed[edge_label_index[1]]\n",
        "        return (embed_src * embed_dst).sum(dim=-1)\n",
        "\n",
        "\n",
        "    def recommend(self, edge_index: Adj, src_index: OptTensor = None,\n",
        "                  dst_index: OptTensor = None, k: int = 1) -> Tensor:\n",
        "        out_src = out_dst = self.get_embedding(edge_index)\n",
        "\n",
        "        if src_index is not None:\n",
        "            out_src = out_src[src_index]\n",
        "\n",
        "        if dst_index is not None:\n",
        "            out_dst = out_dst[dst_index]\n",
        "\n",
        "        pred = out_src @ out_dst.t()\n",
        "        top_index = pred.topk(k, dim=-1).indices\n",
        "\n",
        "        if dst_index is not None:  # Map local top-indices to original indices.\n",
        "            top_index = dst_index[top_index.view(-1)].view(*top_index.size())\n",
        "\n",
        "        return top_index\n",
        "\n",
        "\n",
        "    def link_pred_loss(self, pred: Tensor, edge_label: Tensor,\n",
        "                       **kwargs) -> Tensor:\n",
        "        loss_fn = torch.nn.BCEWithLogitsLoss(**kwargs)\n",
        "        return loss_fn(pred, edge_label.to(pred.dtype))\n",
        "\n",
        "\n",
        "    def recommendation_loss(self, pos_edge_rank: Tensor, neg_edge_rank: Tensor,\n",
        "                            lambda_reg: float = 1e-4, **kwargs) -> Tensor:\n",
        "        r\"\"\"Computes the model loss for a ranking objective via the Bayesian\n",
        "        Personalized Ranking (BPR) loss.\"\"\"\n",
        "        loss_fn = BPRLoss(lambda_reg, **kwargs)\n",
        "        return loss_fn(pos_edge_rank, neg_edge_rank, self.embedding.weight)\n",
        "\n",
        "    def bpr_loss(self, pos_scores, neg_scores):\n",
        "      return - torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.num_nodes}, '\n",
        "                f'{self.embedding_dim}, num_layers={self.num_layers})')"
      ],
      "metadata": {
        "id": "oHYbqECwQ8u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPRLoss(_Loss):\n",
        "    r\"\"\"The Bayesian Personalized Ranking (BPR) loss.\n",
        "\n",
        "    The BPR loss is a pairwise loss that encourages the prediction of an\n",
        "    observed entry to be higher than its unobserved counterparts\n",
        "    (see `here <https://arxiv.org/abs/2002.02126>`__).\n",
        "\n",
        "    .. math::\n",
        "        L_{\\text{BPR}} = - \\sum_{u=1}^{M} \\sum_{i \\in \\mathcal{N}_u}\n",
        "        \\sum_{j \\not\\in \\mathcal{N}_u} \\ln \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})\n",
        "        + \\lambda \\vert\\vert \\textbf{x}^{(0)} \\vert\\vert^2\n",
        "\n",
        "    where :math:`lambda` controls the :math:`L_2` regularization strength.\n",
        "    We compute the mean BPR loss for simplicity.\n",
        "\n",
        "    Args:\n",
        "        lambda_reg (float, optional): The :math:`L_2` regularization strength\n",
        "            (default: 0).\n",
        "        **kwargs (optional): Additional arguments of the underlying\n",
        "            :class:`torch.nn.modules.loss._Loss` class.\n",
        "    \"\"\"\n",
        "    __constants__ = ['lambda_reg']\n",
        "    lambda_reg: float\n",
        "\n",
        "    def __init__(self, lambda_reg: float = 0, **kwargs):\n",
        "        super().__init__(None, None, \"sum\", **kwargs)\n",
        "        self.lambda_reg = lambda_reg\n",
        "\n",
        "    def forward(self, positives: Tensor, negatives: Tensor,\n",
        "                parameters: Tensor = None) -> Tensor:\n",
        "        r\"\"\"Compute the mean Bayesian Personalized Ranking (BPR) loss.\n",
        "\n",
        "        .. note::\n",
        "\n",
        "            The i-th entry in the :obj:`positives` vector and i-th entry\n",
        "            in the :obj:`negatives` entry should correspond to the same\n",
        "            entity (*.e.g*, user), as the BPR is a personalized ranking loss.\n",
        "\n",
        "        Args:\n",
        "            positives (Tensor): The vector of positive-pair rankings.\n",
        "            negatives (Tensor): The vector of negative-pair rankings.\n",
        "            parameters (Tensor, optional): The tensor of parameters which\n",
        "                should be used for :math:`L_2` regularization\n",
        "                (default: :obj:`None`).\n",
        "        \"\"\"\n",
        "        n_pairs = positives.size(0)\n",
        "        log_prob = F.logsigmoid(positives - negatives).sum()\n",
        "        regularization = 0\n",
        "\n",
        "        if self.lambda_reg != 0:\n",
        "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n",
        "\n",
        "        return (-log_prob + regularization) / n_pairs"
      ],
      "metadata": {
        "id": "KZeukOyWY_s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_negative_edges(data, num_playlists, num_tracks, device=None):\n",
        "    positive_playlists, positive_tracks = data.edge_label_index\n",
        "\n",
        "    # Create a mask tensor with the shape (num_playlists, num_tracks)\n",
        "    mask = torch.zeros(num_playlists, num_tracks, device=device, dtype=torch.bool)\n",
        "    mask[positive_playlists, positive_tracks - num_playlists] = True\n",
        "\n",
        "    # Flatten the mask tensor and get the indices of the negative edges\n",
        "    flat_mask = mask.flatten()\n",
        "    negative_indices = torch.where(~flat_mask)[0]\n",
        "\n",
        "    # Sample negative edges from the negative_indices tensor\n",
        "    sampled_negative_indices = negative_indices[\n",
        "        torch.randint(0, negative_indices.size(0), size=(positive_playlists.size(0),), device=device)\n",
        "    ]\n",
        "\n",
        "    # Convert the indices back to playlists and tracks tensors\n",
        "    playlists = torch.floor_divide(sampled_negative_indices, num_tracks)\n",
        "    tracks = torch.remainder(sampled_negative_indices, num_tracks)\n",
        "    tracks = tracks + num_playlists\n",
        "\n",
        "    neg_edge_index = torch.stack((playlists, tracks), dim=0)\n",
        "    neg_edge_label = torch.zeros(neg_edge_index.shape[1], device=device)\n",
        "\n",
        "    return neg_edge_index, neg_edge_label"
      ],
      "metadata": {
        "id": "xe7CW_AWRBCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(data, model, k = 300, batch_size = 64, device = None):\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.get_embedding(data.edge_index)\n",
        "        playlists_embeddings = embeddings[:n_playlists]\n",
        "        tracks_embeddings = embeddings[n_playlists:]\n",
        "\n",
        "    hits_list = []\n",
        "    relevant_counts_list = []\n",
        "\n",
        "    for batch_start in range(0, n_playlists, batch_size):\n",
        "        batch_end = min(batch_start + batch_size, n_playlists)\n",
        "        batch_playlists_embeddings = playlists_embeddings[batch_start:batch_end]\n",
        "\n",
        "        # Calculate scores for all possible item pairs\n",
        "        scores = torch.matmul(batch_playlists_embeddings, tracks_embeddings.t())\n",
        "\n",
        "        # Set the scores of message passing edges to negative infinity\n",
        "        mp_indices = ((data.edge_index[0] >= batch_start) & (data.edge_index[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
        "        scores[data.edge_index[0, mp_indices] - batch_start, data.edge_index[1, mp_indices] - n_playlists] = -float(\"inf\")\n",
        "\n",
        "        # Find the top k highest scoring items for each playlist in the batch\n",
        "        _, top_k_indices = torch.topk(scores, k, dim=1)\n",
        "\n",
        "        # Ground truth supervision edges\n",
        "        ground_truth_edges = data.edge_label_index\n",
        "\n",
        "        # Create a mask to indicate if the top k items are in the ground truth supervision edges\n",
        "        mask = torch.zeros(scores.shape, device=device, dtype=torch.bool)\n",
        "        gt_indices = ((ground_truth_edges[0] >= batch_start) & (ground_truth_edges[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
        "        mask[ground_truth_edges[0, gt_indices] - batch_start, ground_truth_edges[1, gt_indices] - n_playlists] = True\n",
        "\n",
        "        # Check how many of the top k items are in the ground truth supervision edges\n",
        "        hits = mask.gather(1, top_k_indices).sum(dim=1)\n",
        "        hits_list.append(hits)\n",
        "\n",
        "        # Calculate the total number of relevant items for each playlist in the batch\n",
        "        relevant_counts = torch.bincount(ground_truth_edges[0, gt_indices] - batch_start, minlength=batch_end - batch_start)\n",
        "        relevant_counts_list.append(relevant_counts)\n",
        "\n",
        "    # Compute recall@k\n",
        "    hits_tensor = torch.cat(hits_list, dim=0)\n",
        "    relevant_counts_tensor = torch.cat(relevant_counts_list, dim=0)\n",
        "    # Handle division by zero case\n",
        "    recall_at_k = torch.where(\n",
        "        relevant_counts_tensor != 0,\n",
        "        hits_tensor.true_divide(relevant_counts_tensor),\n",
        "        torch.ones_like(hits_tensor)\n",
        "    )\n",
        "    # take average\n",
        "    recall_at_k = torch.mean(recall_at_k)\n",
        "\n",
        "    if recall_at_k.numel() == 1:\n",
        "        return recall_at_k.item()\n",
        "    else:\n",
        "        raise ValueError(\"recall_at_k contains more than one item.\")"
      ],
      "metadata": {
        "id": "UIQTZiseRElr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "def metrics(labels, preds):\n",
        "  roc = roc_auc_score(labels.flatten().cpu().numpy(), preds.flatten().data.cpu().numpy())\n",
        "  return roc"
      ],
      "metadata": {
        "id": "lqZb31tFRHVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "def train(datasets, model, optimizer, loss_fn, args, neg_samp = \"random\"):\n",
        "  print(f\"Beginning training for {model.name}\")\n",
        "\n",
        "  train_data = datasets[\"train\"]\n",
        "  val_data = datasets[\"val\"]\n",
        "\n",
        "  stats = {\n",
        "      'train': {\n",
        "        'loss': [],\n",
        "        'roc' : []\n",
        "      },\n",
        "      'val': {\n",
        "        'loss': [],\n",
        "        'recall': [],\n",
        "        'roc' : []\n",
        "      }\n",
        "\n",
        "  }\n",
        "  val_neg_edge, val_neg_label = None, None\n",
        "  for epoch in range(args[\"epochs\"]): # loop over each epoch\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # obtain negative sample\n",
        "    if neg_samp == \"random\":\n",
        "      neg_edge_index, neg_edge_label = sample_negative_edges(train_data, num_users, num_businesses, args[\"device\"])\n",
        "\n",
        "    # calculate embedding\n",
        "    embed = model.get_embedding(train_data.edge_index)\n",
        "    # calculate pos, negative scores using embedding\n",
        "    pos_scores = model.predict_link_embedding(embed, train_data.edge_label_index)\n",
        "    neg_scores = model.predict_link_embedding(embed, neg_edge_index)\n",
        "\n",
        "    # concatenate pos, neg scores together and evaluate loss\n",
        "    scores = torch.cat((pos_scores, neg_scores), dim = 0)\n",
        "    labels = torch.cat((train_data.edge_label, neg_edge_label), dim = 0)\n",
        "\n",
        "    # calculate loss function\n",
        "    if loss_fn == \"BCE\":\n",
        "      loss = model.link_pred_loss(scores, labels)\n",
        "    elif loss_fn == \"BPR\":\n",
        "      loss = model.recommendation_loss(pos_scores, neg_scores, lambda_reg = 0)\n",
        "\n",
        "    train_roc = metrics(labels, scores)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    val_loss, val_roc, val_neg_edge, val_neg_label = test(\n",
        "        model, val_data, loss_fn, neg_samp, epoch, val_neg_edge, val_neg_label\n",
        "    )\n",
        "\n",
        "    stats['train']['loss'].append(loss)\n",
        "    stats['train']['roc'].append(train_roc)\n",
        "    stats['val']['loss'].append(val_loss)\n",
        "    stats['val']['roc'].append(val_roc)\n",
        "\n",
        "    print(f\"Epoch {epoch}; Train loss {loss}; Val loss {val_loss}; Train ROC {train_roc}; Val ROC {val_roc}\")\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      # calculate recall @ K\n",
        "      val_recall = recall_at_k(val_data, model, k = 300, device = args[\"device\"])\n",
        "      print(f\"Val recall {val_recall}\")\n",
        "      stats['val']['recall'].append(val_recall)\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "\n",
        "      # save embeddings for future visualization\n",
        "      path = os.path.join(\"model_embeddings\", model.name)\n",
        "      if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "      torch.save(model.embedding.weight, os.path.join(\"model_embeddings\", model.name, f\"{model.name}_{loss_fn}_{neg_samp}_{epoch}.pt\"))\n",
        "\n",
        "  return stats\n",
        "\n",
        "def test(model, data, loss_fn, neg_samp, epoch = 0, neg_edge_index = None, neg_edge_label = None):\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad(): # want to save RAM\n",
        "\n",
        "    # conduct negative sampling\n",
        "    if neg_samp == \"random\":\n",
        "      neg_edge_index, neg_edge_label = sample_negative_edges(data, num_users, num_businesses, args[\"device\"])\n",
        "\n",
        "    # obtain model embedding\n",
        "    embed = model.get_embedding(data.edge_index)\n",
        "    # calculate pos, neg scores using embedding\n",
        "    pos_scores = model.predict_link_embedding(embed, data.edge_label_index)\n",
        "    neg_scores = model.predict_link_embedding(embed, neg_edge_index)\n",
        "    # concatenate pos, neg scores together and evaluate loss\n",
        "    scores = torch.cat((pos_scores, neg_scores), dim = 0)\n",
        "    labels = torch.cat((data.edge_label, neg_edge_label), dim = 0)\n",
        "    # calculate loss\n",
        "    if loss_fn == \"BCE\":\n",
        "      loss = model.link_pred_loss(scores, labels)\n",
        "    elif loss_fn == \"BPR\":\n",
        "      loss = model.recommendation_loss(pos_scores, neg_scores, lambda_reg = 0)\n",
        "\n",
        "    roc = metrics(labels, scores)\n",
        "\n",
        "  return loss, roc, neg_edge_index, neg_edge_label"
      ],
      "metadata": {
        "id": "P5226dfQRK5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary of the dataset splits\n",
        "datasets = {\n",
        "    'train':train_split,\n",
        "    'val':val_split,\n",
        "    'test': test_split\n",
        "}"
      ],
      "metadata": {
        "id": "vlQequiPRUX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize our arguments\n",
        "args = {\n",
        "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'num_layers' :  3,\n",
        "    'emb_size' : 64,\n",
        "    'weight_decay': 1e-5,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 301\n",
        "}"
      ],
      "metadata": {
        "id": "Dbf8VwdsRWhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize model and and optimizer\n",
        "model = GCN(\n",
        "    num_nodes = num_nodes, num_layers = args['num_layers'],\n",
        "    embedding_dim = args[\"emb_size\"], conv_layer = \"LGC\"\n",
        ")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])"
      ],
      "metadata": {
        "id": "hu8CDEJNRZKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets['train'].to(args['device'])\n",
        "datasets['val'].to(args['device'])\n",
        "datasets['test'].to(args['device'])\n",
        "model.to(args[\"device\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPSj9vFER36o",
        "outputId": "9fefc6b0-7e7f-4105-adef-c6c337a15049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCN(57492, 64, num_layers=3)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(datasets, model, optimizer, \"BPR\", args, neg_samp = \"random\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsEKznL1SCHM",
        "outputId": "71e81dfd-a645-4856-f3fa-d5cd87c9fe41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning training for LGCN_LGC_3_e64_nodes57492_\n",
            "Epoch 0; Train loss 0.6930004358291626; Val loss 0.6931406259536743; Train ROC 0.9759589919816611; Val ROC 0.5164678684820525\n",
            "Val recall 0.65614914894104\n",
            "Epoch 1; Train loss 0.6928196549415588; Val loss 0.6930937170982361; Train ROC 0.998071464050684; Val ROC 0.5254031294485886\n",
            "Epoch 2; Train loss 0.6919453144073486; Val loss 0.6929563283920288; Train ROC 0.9993645481853293; Val ROC 0.5404252601123636\n",
            "Epoch 3; Train loss 0.6898828148841858; Val loss 0.6926974058151245; Train ROC 0.9996059223936682; Val ROC 0.5494661815486876\n",
            "Epoch 4; Train loss 0.6861544847488403; Val loss 0.6922910809516907; Train ROC 0.9993859070257233; Val ROC 0.5539210675487201\n",
            "Epoch 5; Train loss 0.6803699135780334; Val loss 0.6916607618331909; Train ROC 0.9992409550665163; Val ROC 0.5587480137877725\n",
            "Epoch 6; Train loss 0.6722613573074341; Val loss 0.6908628940582275; Train ROC 0.9988358574203289; Val ROC 0.5613677646539232\n",
            "Epoch 7; Train loss 0.6616120338439941; Val loss 0.6897440552711487; Train ROC 0.9985147787509042; Val ROC 0.5686710448587551\n",
            "Epoch 8; Train loss 0.6484026312828064; Val loss 0.6884401440620422; Train ROC 0.9981943349614847; Val ROC 0.5687426652438718\n",
            "Epoch 9; Train loss 0.6325688362121582; Val loss 0.687027096748352; Train ROC 0.9977658882588318; Val ROC 0.5697999216232628\n",
            "Epoch 10; Train loss 0.6142303943634033; Val loss 0.685218334197998; Train ROC 0.9974758984948306; Val ROC 0.573900580560748\n",
            "Val recall 0.6786224246025085\n",
            "Epoch 11; Train loss 0.5935296416282654; Val loss 0.683245837688446; Train ROC 0.9970068146203086; Val ROC 0.5752966173895833\n",
            "Epoch 12; Train loss 0.5707255005836487; Val loss 0.6807773113250732; Train ROC 0.9967230655744705; Val ROC 0.5825423001037858\n",
            "Epoch 13; Train loss 0.5463048815727234; Val loss 0.6787091493606567; Train ROC 0.996307462422891; Val ROC 0.5796834672528075\n",
            "Epoch 14; Train loss 0.5203496813774109; Val loss 0.6764394044876099; Train ROC 0.9960504601398827; Val ROC 0.5794468011863727\n",
            "Epoch 15; Train loss 0.49365636706352234; Val loss 0.6733331084251404; Train ROC 0.9957639961094179; Val ROC 0.5857844391030957\n",
            "Epoch 16; Train loss 0.4665318429470062; Val loss 0.6705582141876221; Train ROC 0.9955612941927797; Val ROC 0.5884744247972775\n",
            "Epoch 17; Train loss 0.43981537222862244; Val loss 0.6676969528198242; Train ROC 0.9952149971354215; Val ROC 0.5901999453397729\n",
            "Epoch 18; Train loss 0.4138556122779846; Val loss 0.6644591689109802; Train ROC 0.9950764143258456; Val ROC 0.5932661097827018\n",
            "Epoch 19; Train loss 0.38919273018836975; Val loss 0.6618554592132568; Train ROC 0.9949722701515596; Val ROC 0.5955966843172744\n",
            "Epoch 20; Train loss 0.36652833223342896; Val loss 0.6590447425842285; Train ROC 0.9947813518557813; Val ROC 0.5939696293284298\n",
            "Val recall 0.6850169897079468\n",
            "Epoch 21; Train loss 0.3461923599243164; Val loss 0.6566082835197449; Train ROC 0.994614725570277; Val ROC 0.5959737730717345\n",
            "Epoch 22; Train loss 0.32820600271224976; Val loss 0.6541803479194641; Train ROC 0.9946230690336572; Val ROC 0.5955117455582224\n",
            "Epoch 23; Train loss 0.3128943145275116; Val loss 0.6519612669944763; Train ROC 0.9944752880782576; Val ROC 0.6008584048664851\n",
            "Epoch 24; Train loss 0.30007195472717285; Val loss 0.6497532725334167; Train ROC 0.9945726802342875; Val ROC 0.6024941988456771\n",
            "Epoch 25; Train loss 0.28998374938964844; Val loss 0.6474632620811462; Train ROC 0.9946136491986788; Val ROC 0.6053524915642323\n",
            "Epoch 26; Train loss 0.28240522742271423; Val loss 0.6464649438858032; Train ROC 0.9948116973950174; Val ROC 0.607431907457515\n",
            "Epoch 27; Train loss 0.2773715555667877; Val loss 0.6462100744247437; Train ROC 0.9948085560125265; Val ROC 0.6048600170208642\n",
            "Epoch 28; Train loss 0.27427446842193604; Val loss 0.6443443298339844; Train ROC 0.9948761167594878; Val ROC 0.6079977049773337\n",
            "Epoch 29; Train loss 0.27326318621635437; Val loss 0.6430588364601135; Train ROC 0.9949107840917353; Val ROC 0.6118422442746418\n",
            "Epoch 30; Train loss 0.27322593331336975; Val loss 0.6431999802589417; Train ROC 0.9950686557444911; Val ROC 0.6094047293912782\n",
            "Val recall 0.6873975992202759\n",
            "Epoch 31; Train loss 0.2749324440956116; Val loss 0.64259934425354; Train ROC 0.995114050406051; Val ROC 0.611155348478439\n",
            "Epoch 32; Train loss 0.27758070826530457; Val loss 0.6416999697685242; Train ROC 0.9952771603916002; Val ROC 0.6156700189183143\n",
            "Epoch 33; Train loss 0.2809859812259674; Val loss 0.6423383951187134; Train ROC 0.995358393227582; Val ROC 0.6110585972581317\n",
            "Epoch 34; Train loss 0.2848450839519501; Val loss 0.6408116221427917; Train ROC 0.9954293187981391; Val ROC 0.6176374600763614\n",
            "Epoch 35; Train loss 0.2887573838233948; Val loss 0.6415196061134338; Train ROC 0.9953601408658549; Val ROC 0.6163167130105178\n",
            "Epoch 36; Train loss 0.29282984137535095; Val loss 0.639826774597168; Train ROC 0.9953527175809238; Val ROC 0.6215475051074951\n",
            "Epoch 37; Train loss 0.29667529463768005; Val loss 0.6413785219192505; Train ROC 0.9953234837218321; Val ROC 0.6170940134710552\n",
            "Epoch 38; Train loss 0.30037426948547363; Val loss 0.6388915181159973; Train ROC 0.9950923813356058; Val ROC 0.623718270896853\n",
            "Epoch 39; Train loss 0.30309560894966125; Val loss 0.6402698159217834; Train ROC 0.9952680654761066; Val ROC 0.6180516447733873\n",
            "Epoch 40; Train loss 0.30566155910491943; Val loss 0.6396474838256836; Train ROC 0.9950984385032349; Val ROC 0.6195036381470661\n",
            "Val recall 0.6916568875312805\n",
            "Epoch 41; Train loss 0.3074909746646881; Val loss 0.638990044593811; Train ROC 0.9949439208928398; Val ROC 0.6211285854452595\n",
            "Epoch 42; Train loss 0.30875831842422485; Val loss 0.638188898563385; Train ROC 0.9948527139316102; Val ROC 0.6193260901614521\n",
            "Epoch 43; Train loss 0.30933117866516113; Val loss 0.6366444230079651; Train ROC 0.9944091043645996; Val ROC 0.6254974612279003\n",
            "Epoch 44; Train loss 0.30922064185142517; Val loss 0.6354596018791199; Train ROC 0.994681728253536; Val ROC 0.6265234222172704\n",
            "Epoch 45; Train loss 0.3089115023612976; Val loss 0.635807991027832; Train ROC 0.9945077595423669; Val ROC 0.6230233787923849\n",
            "Epoch 46; Train loss 0.30792251229286194; Val loss 0.6343696713447571; Train ROC 0.9943275797614325; Val ROC 0.6247408589940805\n",
            "Epoch 47; Train loss 0.306470662355423; Val loss 0.6338870525360107; Train ROC 0.9944358908832353; Val ROC 0.6267339036031887\n",
            "Epoch 48; Train loss 0.3054906129837036; Val loss 0.632491946220398; Train ROC 0.9939658497697674; Val ROC 0.6261800623842381\n",
            "Epoch 49; Train loss 0.30384504795074463; Val loss 0.6301902532577515; Train ROC 0.9938162225277903; Val ROC 0.6288781852907956\n",
            "Epoch 50; Train loss 0.3022785186767578; Val loss 0.6312688589096069; Train ROC 0.993475476189524; Val ROC 0.6276267366277493\n",
            "Val recall 0.6929010152816772\n",
            "Epoch 51; Train loss 0.30061396956443787; Val loss 0.630767822265625; Train ROC 0.9934926399164233; Val ROC 0.6253233800705027\n",
            "Epoch 52; Train loss 0.2990722358226776; Val loss 0.6289354562759399; Train ROC 0.993482621183814; Val ROC 0.6276346008384112\n",
            "Epoch 53; Train loss 0.29831692576408386; Val loss 0.6283880472183228; Train ROC 0.992803417767872; Val ROC 0.6285483786718825\n",
            "Epoch 54; Train loss 0.296516090631485; Val loss 0.6274859309196472; Train ROC 0.993049076034898; Val ROC 0.6297417807124477\n",
            "Epoch 55; Train loss 0.2959860861301422; Val loss 0.6263455748558044; Train ROC 0.9927265476935966; Val ROC 0.6329410056036332\n",
            "Epoch 56; Train loss 0.29489681124687195; Val loss 0.6257031559944153; Train ROC 0.9927284176988922; Val ROC 0.6304984533983227\n",
            "Epoch 57; Train loss 0.2946425676345825; Val loss 0.6264554262161255; Train ROC 0.9926510647600324; Val ROC 0.6276614430714428\n",
            "Epoch 58; Train loss 0.2942558825016022; Val loss 0.624860405921936; Train ROC 0.9924508853829683; Val ROC 0.6325333113021928\n",
            "Epoch 59; Train loss 0.29441195726394653; Val loss 0.6263802647590637; Train ROC 0.9921622659139979; Val ROC 0.6284002062577851\n",
            "Epoch 60; Train loss 0.29467350244522095; Val loss 0.62541264295578; Train ROC 0.9918719350586587; Val ROC 0.6319558275477176\n",
            "Val recall 0.6946167945861816\n",
            "Epoch 61; Train loss 0.2947944402694702; Val loss 0.6252201199531555; Train ROC 0.9919615976079045; Val ROC 0.6308119621084658\n",
            "Epoch 62; Train loss 0.29526758193969727; Val loss 0.624663770198822; Train ROC 0.991743142689031; Val ROC 0.6321743463389334\n",
            "Epoch 63; Train loss 0.2953453063964844; Val loss 0.6229997873306274; Train ROC 0.991910911785497; Val ROC 0.6356356176494398\n",
            "Epoch 64; Train loss 0.2959732115268707; Val loss 0.6224433779716492; Train ROC 0.9917710352276624; Val ROC 0.638103230237886\n",
            "Epoch 65; Train loss 0.2966180443763733; Val loss 0.6249639391899109; Train ROC 0.9917272403666838; Val ROC 0.6309136361661304\n",
            "Epoch 66; Train loss 0.29761382937431335; Val loss 0.6241779327392578; Train ROC 0.9913580104085503; Val ROC 0.6324859675210976\n",
            "Epoch 67; Train loss 0.29744258522987366; Val loss 0.623818576335907; Train ROC 0.9915747372301401; Val ROC 0.6324881985028457\n",
            "Epoch 68; Train loss 0.29822808504104614; Val loss 0.6224678754806519; Train ROC 0.9914184151104549; Val ROC 0.635396838021349\n",
            "Epoch 69; Train loss 0.2981841564178467; Val loss 0.6223119497299194; Train ROC 0.991396857895328; Val ROC 0.6366515245434323\n",
            "Epoch 70; Train loss 0.29873907566070557; Val loss 0.6215784549713135; Train ROC 0.9914377414184282; Val ROC 0.6358174866944395\n",
            "Val recall 0.6957470774650574\n",
            "Epoch 71; Train loss 0.29869207739830017; Val loss 0.6217209696769714; Train ROC 0.9912559506513805; Val ROC 0.6363411010462066\n",
            "Epoch 72; Train loss 0.29838457703590393; Val loss 0.6211335062980652; Train ROC 0.9913357650810682; Val ROC 0.639051365190277\n",
            "Epoch 73; Train loss 0.2992646396160126; Val loss 0.6216291189193726; Train ROC 0.990747224308729; Val ROC 0.6355678369013317\n",
            "Epoch 74; Train loss 0.29887068271636963; Val loss 0.6206526160240173; Train ROC 0.9910047467863495; Val ROC 0.6400557443667373\n",
            "Epoch 75; Train loss 0.298956036567688; Val loss 0.6211177110671997; Train ROC 0.9907594444348602; Val ROC 0.6352792917027481\n",
            "Epoch 76; Train loss 0.2983177602291107; Val loss 0.6216778755187988; Train ROC 0.9909621203417671; Val ROC 0.6343145506138455\n",
            "Epoch 77; Train loss 0.2987058162689209; Val loss 0.620262086391449; Train ROC 0.9904771778089018; Val ROC 0.6386703047012052\n",
            "Epoch 78; Train loss 0.2988225221633911; Val loss 0.6200780272483826; Train ROC 0.9901674207843958; Val ROC 0.6375798859523305\n",
            "Epoch 79; Train loss 0.2983492910861969; Val loss 0.619084358215332; Train ROC 0.9902007442356396; Val ROC 0.6364905357262929\n",
            "Epoch 80; Train loss 0.29844892024993896; Val loss 0.618377685546875; Train ROC 0.9899407869507492; Val ROC 0.6420281259751555\n",
            "Val recall 0.6961187720298767\n",
            "Epoch 81; Train loss 0.29777923226356506; Val loss 0.6201812624931335; Train ROC 0.9904776660641041; Val ROC 0.636568294246719\n",
            "Epoch 82; Train loss 0.29722362756729126; Val loss 0.6191489696502686; Train ROC 0.9907029805440577; Val ROC 0.6396405762764411\n",
            "Epoch 83; Train loss 0.2975757122039795; Val loss 0.6188666224479675; Train ROC 0.9900848842275125; Val ROC 0.639745306192\n",
            "Epoch 84; Train loss 0.2974703907966614; Val loss 0.6180195212364197; Train ROC 0.9899913522928497; Val ROC 0.639806646448062\n",
            "Epoch 85; Train loss 0.2971552014350891; Val loss 0.6192477345466614; Train ROC 0.990131640963411; Val ROC 0.6384032620569695\n",
            "Epoch 86; Train loss 0.2973349988460541; Val loss 0.6171191334724426; Train ROC 0.9900391382324412; Val ROC 0.6430669063030701\n",
            "Epoch 87; Train loss 0.29757818579673767; Val loss 0.6178082823753357; Train ROC 0.9899280851729225; Val ROC 0.6417630295689447\n",
            "Epoch 88; Train loss 0.297385573387146; Val loss 0.6181269884109497; Train ROC 0.9898547382716398; Val ROC 0.6384074745027701\n",
            "Epoch 89; Train loss 0.29753008484840393; Val loss 0.617453932762146; Train ROC 0.9899987825855837; Val ROC 0.6410140919316305\n",
            "Epoch 90; Train loss 0.2973414361476898; Val loss 0.6186016798019409; Train ROC 0.9899522603415574; Val ROC 0.6401689608194456\n",
            "Val recall 0.6970512866973877\n",
            "Epoch 91; Train loss 0.2972407341003418; Val loss 0.6183781623840332; Train ROC 0.9900475081098482; Val ROC 0.638600351681395\n",
            "Epoch 92; Train loss 0.2973153591156006; Val loss 0.6160612106323242; Train ROC 0.9899252046963306; Val ROC 0.6457253735090562\n",
            "Epoch 93; Train loss 0.2972322404384613; Val loss 0.6159830093383789; Train ROC 0.9897105726687965; Val ROC 0.646009938166521\n",
            "Epoch 94; Train loss 0.2972418963909149; Val loss 0.6154478192329407; Train ROC 0.9898446421836654; Val ROC 0.6438596651187196\n",
            "Epoch 95; Train loss 0.2980940639972687; Val loss 0.6178428530693054; Train ROC 0.9892377102407663; Val ROC 0.6385772668913073\n",
            "Epoch 96; Train loss 0.29800671339035034; Val loss 0.6180436015129089; Train ROC 0.9892780576669316; Val ROC 0.6400810660095776\n",
            "Epoch 97; Train loss 0.29747411608695984; Val loss 0.6155335307121277; Train ROC 0.9897615284263873; Val ROC 0.6435308741836023\n",
            "Epoch 98; Train loss 0.29749393463134766; Val loss 0.6162838339805603; Train ROC 0.9896342354570853; Val ROC 0.64002454291279\n",
            "Epoch 99; Train loss 0.29741406440734863; Val loss 0.6155427098274231; Train ROC 0.9895414702030311; Val ROC 0.643388787065773\n",
            "Epoch 100; Train loss 0.29733386635780334; Val loss 0.616142988204956; Train ROC 0.9896621774546345; Val ROC 0.6384358461325002\n",
            "Val recall 0.6977638602256775\n",
            "Epoch 101; Train loss 0.29778438806533813; Val loss 0.6161280274391174; Train ROC 0.9892261899515838; Val ROC 0.6396067915804698\n",
            "Epoch 102; Train loss 0.29713985323905945; Val loss 0.6141448616981506; Train ROC 0.989696275402883; Val ROC 0.64408288658462\n",
            "Epoch 103; Train loss 0.2978402078151703; Val loss 0.6144968867301941; Train ROC 0.9889116882401044; Val ROC 0.6433281043622263\n",
            "Epoch 104; Train loss 0.29747244715690613; Val loss 0.61214280128479; Train ROC 0.9892393497971534; Val ROC 0.6475162177842346\n",
            "Epoch 105; Train loss 0.2976241707801819; Val loss 0.6145167946815491; Train ROC 0.9889388950921587; Val ROC 0.6451096137400905\n",
            "Epoch 106; Train loss 0.29750585556030273; Val loss 0.6141217350959778; Train ROC 0.989295132044342; Val ROC 0.6451655057038836\n",
            "Epoch 107; Train loss 0.29698505997657776; Val loss 0.6142545938491821; Train ROC 0.9892019755662053; Val ROC 0.6454418098578758\n",
            "Epoch 108; Train loss 0.2973560690879822; Val loss 0.613325834274292; Train ROC 0.9892587712495297; Val ROC 0.6459803248193179\n",
            "Epoch 109; Train loss 0.29716819524765015; Val loss 0.615707278251648; Train ROC 0.989107440033284; Val ROC 0.6444934782268306\n",
            "Epoch 110; Train loss 0.29758068919181824; Val loss 0.6132501363754272; Train ROC 0.9889772862631445; Val ROC 0.6454354456888893\n",
            "Val recall 0.697799801826477\n",
            "Epoch 111; Train loss 0.2972404956817627; Val loss 0.6137356758117676; Train ROC 0.9891124898292216; Val ROC 0.644716356238962\n",
            "Epoch 112; Train loss 0.29726356267929077; Val loss 0.614990234375; Train ROC 0.9891733662101789; Val ROC 0.6424361402463465\n",
            "Epoch 113; Train loss 0.2975251078605652; Val loss 0.6130573749542236; Train ROC 0.9889764732232232; Val ROC 0.6459266168692361\n",
            "Epoch 114; Train loss 0.2979649305343628; Val loss 0.6118009686470032; Train ROC 0.9885007431213182; Val ROC 0.6500268941914223\n",
            "Epoch 115; Train loss 0.29803064465522766; Val loss 0.6132434010505676; Train ROC 0.9884900478661817; Val ROC 0.6449109741704506\n",
            "Epoch 116; Train loss 0.29785260558128357; Val loss 0.6135914921760559; Train ROC 0.9885205921844471; Val ROC 0.6462938305939587\n",
            "Epoch 117; Train loss 0.29758739471435547; Val loss 0.6122656464576721; Train ROC 0.9888987752848276; Val ROC 0.6486800358326197\n",
            "Epoch 118; Train loss 0.29762089252471924; Val loss 0.6108469367027283; Train ROC 0.9886952383615012; Val ROC 0.6519544154536911\n",
            "Epoch 119; Train loss 0.2978571057319641; Val loss 0.614332377910614; Train ROC 0.9884372173865046; Val ROC 0.6449725316536827\n",
            "Epoch 120; Train loss 0.297442764043808; Val loss 0.612269937992096; Train ROC 0.9890658809310148; Val ROC 0.6484225570548776\n",
            "Val recall 0.6979697942733765\n",
            "Epoch 121; Train loss 0.2976638674736023; Val loss 0.6118642091751099; Train ROC 0.9890151418762574; Val ROC 0.6490568956179004\n",
            "Epoch 122; Train loss 0.2973344326019287; Val loss 0.6116210222244263; Train ROC 0.9890390499397825; Val ROC 0.6503646207955409\n",
            "Epoch 123; Train loss 0.2971886098384857; Val loss 0.6131004691123962; Train ROC 0.9888917023900501; Val ROC 0.6455745826269071\n",
            "Epoch 124; Train loss 0.2982277274131775; Val loss 0.6141011714935303; Train ROC 0.9883588124685918; Val ROC 0.6440430899799383\n",
            "Epoch 125; Train loss 0.2975524365901947; Val loss 0.6127045154571533; Train ROC 0.9887358814630499; Val ROC 0.6484799519958483\n",
            "Epoch 126; Train loss 0.29737389087677; Val loss 0.6117652654647827; Train ROC 0.988678605071388; Val ROC 0.6469306085593928\n",
            "Epoch 127; Train loss 0.29721662402153015; Val loss 0.6110209226608276; Train ROC 0.988680209723525; Val ROC 0.6499596858662626\n",
            "Epoch 128; Train loss 0.29698050022125244; Val loss 0.6123958230018616; Train ROC 0.9888945687162706; Val ROC 0.6472483443928481\n",
            "Epoch 129; Train loss 0.2974949777126312; Val loss 0.6112281680107117; Train ROC 0.9886077850221731; Val ROC 0.650460887657969\n",
            "Epoch 130; Train loss 0.29739782214164734; Val loss 0.6107044219970703; Train ROC 0.988551834048649; Val ROC 0.6502682599970395\n",
            "Val recall 0.6982063055038452\n",
            "Epoch 131; Train loss 0.297130286693573; Val loss 0.6101532578468323; Train ROC 0.9884489875308196; Val ROC 0.6466505029299219\n",
            "Epoch 132; Train loss 0.29731106758117676; Val loss 0.6114606857299805; Train ROC 0.9886144660190405; Val ROC 0.64935816034795\n",
            "Epoch 133; Train loss 0.2969513237476349; Val loss 0.6101825833320618; Train ROC 0.9887727666302032; Val ROC 0.6489200542426812\n",
            "Epoch 134; Train loss 0.29711392521858215; Val loss 0.6107834577560425; Train ROC 0.9886290049193944; Val ROC 0.647585830285778\n",
            "Epoch 135; Train loss 0.2974635064601898; Val loss 0.6128247380256653; Train ROC 0.9883811201924743; Val ROC 0.6458878271418433\n",
            "Epoch 136; Train loss 0.2975499927997589; Val loss 0.6115376949310303; Train ROC 0.9882420072156431; Val ROC 0.6468842246875497\n",
            "Epoch 137; Train loss 0.2968669533729553; Val loss 0.609854519367218; Train ROC 0.9886555953543096; Val ROC 0.6481251378328411\n",
            "Epoch 138; Train loss 0.2972618639469147; Val loss 0.6102461218833923; Train ROC 0.9883995552965689; Val ROC 0.6495771987230728\n",
            "Epoch 139; Train loss 0.297315776348114; Val loss 0.611312747001648; Train ROC 0.9884718108672892; Val ROC 0.64833480314912\n",
            "Epoch 140; Train loss 0.2970484793186188; Val loss 0.6116936206817627; Train ROC 0.9886233513742692; Val ROC 0.6497538220254624\n",
            "Val recall 0.6986557245254517\n",
            "Epoch 141; Train loss 0.2974907159805298; Val loss 0.6099854707717896; Train ROC 0.988349041569633; Val ROC 0.6512549116751096\n",
            "Epoch 142; Train loss 0.29759833216667175; Val loss 0.6112916469573975; Train ROC 0.9884306712898463; Val ROC 0.6464971141282374\n",
            "Epoch 143; Train loss 0.29720360040664673; Val loss 0.6107770800590515; Train ROC 0.9883603637536135; Val ROC 0.6498704994353826\n",
            "Epoch 144; Train loss 0.29726704955101013; Val loss 0.6116275191307068; Train ROC 0.988686660541015; Val ROC 0.6490167262044265\n",
            "Epoch 145; Train loss 0.29755064845085144; Val loss 0.6096321940422058; Train ROC 0.9883198447710382; Val ROC 0.6534388784183109\n",
            "Epoch 146; Train loss 0.2975652515888214; Val loss 0.6099504828453064; Train ROC 0.9884657720277608; Val ROC 0.6503913045114484\n",
            "Epoch 147; Train loss 0.2974432408809662; Val loss 0.6088975071907043; Train ROC 0.9882139156284514; Val ROC 0.6525655107095021\n",
            "Epoch 148; Train loss 0.2974705100059509; Val loss 0.6112008094787598; Train ROC 0.9883179827169037; Val ROC 0.6502119746759383\n",
            "Epoch 149; Train loss 0.29746121168136597; Val loss 0.6111347079277039; Train ROC 0.9883006475683601; Val ROC 0.6519019668340962\n",
            "Epoch 150; Train loss 0.29725757241249084; Val loss 0.6098001599311829; Train ROC 0.9883479240945746; Val ROC 0.6524705149195701\n",
            "Val recall 0.6982023119926453\n",
            "Epoch 151; Train loss 0.29741016030311584; Val loss 0.6109662055969238; Train ROC 0.9883596459928607; Val ROC 0.6465193358806487\n",
            "Epoch 152; Train loss 0.2975825071334839; Val loss 0.6089836359024048; Train ROC 0.9882328852122955; Val ROC 0.654300536408445\n",
            "Epoch 153; Train loss 0.2977013885974884; Val loss 0.6109423637390137; Train ROC 0.9879352843003348; Val ROC 0.647267334157227\n",
            "Epoch 154; Train loss 0.29750192165374756; Val loss 0.6107030510902405; Train ROC 0.9879974582029836; Val ROC 0.6448949844894223\n",
            "Epoch 155; Train loss 0.2977193593978882; Val loss 0.6098587512969971; Train ROC 0.9876626220032471; Val ROC 0.6503267880418979\n",
            "Epoch 156; Train loss 0.297533243894577; Val loss 0.6104700565338135; Train ROC 0.9882020247342992; Val ROC 0.6492214187798091\n",
            "Epoch 157; Train loss 0.2969227135181427; Val loss 0.6097302436828613; Train ROC 0.9885424367194999; Val ROC 0.6495967814589165\n",
            "Epoch 158; Train loss 0.29798540472984314; Val loss 0.6108927130699158; Train ROC 0.9879651903694595; Val ROC 0.6458489111878516\n",
            "Epoch 159; Train loss 0.2974783480167389; Val loss 0.608343243598938; Train ROC 0.9882877356912069; Val ROC 0.6571627920551053\n",
            "Epoch 160; Train loss 0.29774948954582214; Val loss 0.6119338870048523; Train ROC 0.9880954820046475; Val ROC 0.646803809537542\n",
            "Val recall 0.6984727382659912\n",
            "Epoch 161; Train loss 0.2975304126739502; Val loss 0.6104634404182434; Train ROC 0.9879777651982381; Val ROC 0.6484740575072299\n",
            "Epoch 162; Train loss 0.29747673869132996; Val loss 0.6110792756080627; Train ROC 0.9881954190713147; Val ROC 0.6460778803672556\n",
            "Epoch 163; Train loss 0.2975798547267914; Val loss 0.611079752445221; Train ROC 0.9881753224440658; Val ROC 0.6459687090367165\n",
            "Epoch 164; Train loss 0.29768145084381104; Val loss 0.6096065640449524; Train ROC 0.9880766996100049; Val ROC 0.653139430764185\n",
            "Epoch 165; Train loss 0.29731127619743347; Val loss 0.6100361347198486; Train ROC 0.9882069748038087; Val ROC 0.6497097337164179\n",
            "Epoch 166; Train loss 0.2977806627750397; Val loss 0.610679566860199; Train ROC 0.9880533395027732; Val ROC 0.6469137763892043\n",
            "Epoch 167; Train loss 0.2974429428577423; Val loss 0.6093707084655762; Train ROC 0.9880981720576677; Val ROC 0.6508780049217924\n",
            "Epoch 168; Train loss 0.29747071862220764; Val loss 0.6097819805145264; Train ROC 0.9881050050699554; Val ROC 0.6522742354937794\n",
            "Epoch 169; Train loss 0.2974739968776703; Val loss 0.6077374219894409; Train ROC 0.9881524464143076; Val ROC 0.6524337360112527\n",
            "Epoch 170; Train loss 0.2973322570323944; Val loss 0.6090419888496399; Train ROC 0.9880208420289333; Val ROC 0.6511206770259329\n",
            "Val recall 0.697884202003479\n",
            "Epoch 171; Train loss 0.2977980375289917; Val loss 0.6094872355461121; Train ROC 0.9879927513850997; Val ROC 0.6505925126456009\n",
            "Epoch 172; Train loss 0.29754939675331116; Val loss 0.6079962849617004; Train ROC 0.988048621095061; Val ROC 0.6531797440172717\n",
            "Epoch 173; Train loss 0.29757604002952576; Val loss 0.6101070046424866; Train ROC 0.9879098512310496; Val ROC 0.6498565440574482\n",
            "Epoch 174; Train loss 0.2975349426269531; Val loss 0.608151912689209; Train ROC 0.9879843898631504; Val ROC 0.6519213117942535\n",
            "Epoch 175; Train loss 0.29730454087257385; Val loss 0.6085452437400818; Train ROC 0.9882843998421809; Val ROC 0.6527838768545982\n",
            "Epoch 176; Train loss 0.2973572909832001; Val loss 0.6100965738296509; Train ROC 0.9881167107963884; Val ROC 0.6490175275965544\n",
            "Epoch 177; Train loss 0.29762929677963257; Val loss 0.6089090704917908; Train ROC 0.9879893613603652; Val ROC 0.6514921237449722\n",
            "Epoch 178; Train loss 0.29700881242752075; Val loss 0.6102606654167175; Train ROC 0.9885129474798924; Val ROC 0.6508324987651369\n",
            "Epoch 179; Train loss 0.29744842648506165; Val loss 0.6100770831108093; Train ROC 0.9879500989307868; Val ROC 0.6492863374131748\n",
            "Epoch 180; Train loss 0.2971973717212677; Val loss 0.6091724634170532; Train ROC 0.9879754857755247; Val ROC 0.6503942047877209\n",
            "Val recall 0.6991794109344482\n",
            "Epoch 181; Train loss 0.29736608266830444; Val loss 0.6077027320861816; Train ROC 0.9880958690510017; Val ROC 0.654208631702435\n",
            "Epoch 182; Train loss 0.2972700595855713; Val loss 0.6094187498092651; Train ROC 0.9880048653160614; Val ROC 0.652973269591993\n",
            "Epoch 183; Train loss 0.29723939299583435; Val loss 0.6091626882553101; Train ROC 0.988110639208857; Val ROC 0.6504867729172508\n",
            "Epoch 184; Train loss 0.2973543107509613; Val loss 0.6084011197090149; Train ROC 0.9881067597160315; Val ROC 0.6512911445799991\n",
            "Epoch 185; Train loss 0.29695427417755127; Val loss 0.6093745231628418; Train ROC 0.9882846193750885; Val ROC 0.6501693805375646\n",
            "Epoch 186; Train loss 0.297454297542572; Val loss 0.6073819398880005; Train ROC 0.987637536494041; Val ROC 0.6535994592006307\n",
            "Epoch 187; Train loss 0.29716265201568604; Val loss 0.6087960004806519; Train ROC 0.9881027668854678; Val ROC 0.6530506083355901\n",
            "Epoch 188; Train loss 0.29753726720809937; Val loss 0.611277163028717; Train ROC 0.987827537845741; Val ROC 0.64683415676032\n",
            "Epoch 189; Train loss 0.29679927229881287; Val loss 0.6085212230682373; Train ROC 0.9884591764721844; Val ROC 0.6534064352468908\n",
            "Epoch 190; Train loss 0.29783958196640015; Val loss 0.6081905961036682; Train ROC 0.9875191817080214; Val ROC 0.6542048478399702\n",
            "Val recall 0.6988611817359924\n",
            "Epoch 191; Train loss 0.29780450463294983; Val loss 0.6093961000442505; Train ROC 0.9875342948439307; Val ROC 0.6508657345221781\n",
            "Epoch 192; Train loss 0.2973516583442688; Val loss 0.6094076037406921; Train ROC 0.9879026879086593; Val ROC 0.6508050459476269\n",
            "Epoch 193; Train loss 0.2973053753376007; Val loss 0.6095147728919983; Train ROC 0.9878809095434395; Val ROC 0.6495978235622332\n",
            "Epoch 194; Train loss 0.29689452052116394; Val loss 0.6103242039680481; Train ROC 0.9882581006353894; Val ROC 0.6481911895700946\n",
            "Epoch 195; Train loss 0.2972654402256012; Val loss 0.609027624130249; Train ROC 0.9878535469025322; Val ROC 0.6508728677927672\n",
            "Epoch 196; Train loss 0.2977627217769623; Val loss 0.6098859310150146; Train ROC 0.9875112526483805; Val ROC 0.649566545785226\n",
            "Epoch 197; Train loss 0.2977425754070282; Val loss 0.6101619005203247; Train ROC 0.9875677835176082; Val ROC 0.6496812476020983\n",
            "Epoch 198; Train loss 0.29747048020362854; Val loss 0.6092916131019592; Train ROC 0.9877058404724234; Val ROC 0.6511031990452385\n",
            "Epoch 199; Train loss 0.2979552149772644; Val loss 0.6100002527236938; Train ROC 0.9874931148368545; Val ROC 0.6489932363150215\n",
            "Epoch 200; Train loss 0.29782330989837646; Val loss 0.6092947721481323; Train ROC 0.9875635539041605; Val ROC 0.6509207546417879\n",
            "Val recall 0.698244571685791\n",
            "Epoch 201; Train loss 0.29757893085479736; Val loss 0.6084352731704712; Train ROC 0.9875966452893106; Val ROC 0.6518016724625126\n",
            "Epoch 202; Train loss 0.297624796628952; Val loss 0.6086843013763428; Train ROC 0.987925972412477; Val ROC 0.6517117550915598\n",
            "Epoch 203; Train loss 0.29749512672424316; Val loss 0.6079858541488647; Train ROC 0.9878532319556906; Val ROC 0.652966793873919\n",
            "Epoch 204; Train loss 0.29774487018585205; Val loss 0.6110888719558716; Train ROC 0.98764892821699; Val ROC 0.6494866619611348\n",
            "Epoch 205; Train loss 0.29751864075660706; Val loss 0.6086758971214294; Train ROC 0.9880840728969561; Val ROC 0.6504467972469286\n",
            "Epoch 206; Train loss 0.29755154252052307; Val loss 0.6083871722221375; Train ROC 0.9879437191346856; Val ROC 0.6521407582041961\n",
            "Epoch 207; Train loss 0.29761773347854614; Val loss 0.6088781952857971; Train ROC 0.9876345805488004; Val ROC 0.6495548859700901\n",
            "Epoch 208; Train loss 0.2977384626865387; Val loss 0.6087942719459534; Train ROC 0.9878196035302478; Val ROC 0.6503142681245881\n",
            "Epoch 209; Train loss 0.2973056435585022; Val loss 0.6077556014060974; Train ROC 0.9883075396079122; Val ROC 0.6495137537118619\n",
            "Epoch 210; Train loss 0.2977786362171173; Val loss 0.6097049117088318; Train ROC 0.9876846890360471; Val ROC 0.6483940181015166\n",
            "Val recall 0.6993849277496338\n",
            "Epoch 211; Train loss 0.2978265881538391; Val loss 0.6102709770202637; Train ROC 0.987706829246483; Val ROC 0.6486596252851276\n",
            "Epoch 212; Train loss 0.2974500060081482; Val loss 0.6069038510322571; Train ROC 0.9879849992724861; Val ROC 0.6564876646876234\n",
            "Epoch 213; Train loss 0.2975008189678192; Val loss 0.6092539429664612; Train ROC 0.9878513952374595; Val ROC 0.6491599112001161\n",
            "Epoch 214; Train loss 0.2972942292690277; Val loss 0.6080711483955383; Train ROC 0.9879915769042583; Val ROC 0.6526173135185911\n",
            "Epoch 215; Train loss 0.2971894443035126; Val loss 0.6091097593307495; Train ROC 0.9879522837481585; Val ROC 0.6509695720450378\n",
            "Epoch 216; Train loss 0.2972324788570404; Val loss 0.6093254089355469; Train ROC 0.9879821608427128; Val ROC 0.6501724452019658\n",
            "Epoch 217; Train loss 0.29761454463005066; Val loss 0.6099530458450317; Train ROC 0.9875237377233738; Val ROC 0.6509809471164505\n",
            "Epoch 218; Train loss 0.2967561185359955; Val loss 0.6086878776550293; Train ROC 0.9882749236752475; Val ROC 0.6511756560485107\n",
            "Epoch 219; Train loss 0.29713204503059387; Val loss 0.6094202995300293; Train ROC 0.9877089185151556; Val ROC 0.6504479098023004\n",
            "Epoch 220; Train loss 0.2973385155200958; Val loss 0.6081536412239075; Train ROC 0.9877201326172501; Val ROC 0.6544737545281665\n",
            "Val recall 0.6993623971939087\n",
            "Epoch 221; Train loss 0.2973010838031769; Val loss 0.607897937297821; Train ROC 0.9877660609499668; Val ROC 0.6517181016475325\n",
            "Epoch 222; Train loss 0.29663753509521484; Val loss 0.6086984276771545; Train ROC 0.9879530155245173; Val ROC 0.6552173290027785\n",
            "Epoch 223; Train loss 0.2972191870212555; Val loss 0.6070631146430969; Train ROC 0.9877490174338429; Val ROC 0.6556522236685312\n",
            "Epoch 224; Train loss 0.2972632944583893; Val loss 0.607520580291748; Train ROC 0.987753198396966; Val ROC 0.6540969505174296\n",
            "Epoch 225; Train loss 0.297187864780426; Val loss 0.607102632522583; Train ROC 0.9880051993995959; Val ROC 0.6534854941948358\n",
            "Epoch 226; Train loss 0.2969798147678375; Val loss 0.6089500784873962; Train ROC 0.9881231706431628; Val ROC 0.6528162847999908\n",
            "Epoch 227; Train loss 0.29747289419174194; Val loss 0.6080436706542969; Train ROC 0.9875863074321303; Val ROC 0.6523464106888308\n",
            "Epoch 228; Train loss 0.2971188426017761; Val loss 0.609277069568634; Train ROC 0.9876987613784355; Val ROC 0.6521888065058434\n",
            "Epoch 229; Train loss 0.29693055152893066; Val loss 0.6082466244697571; Train ROC 0.9879928591974545; Val ROC 0.6549005501430731\n",
            "Epoch 230; Train loss 0.2973560094833374; Val loss 0.6095638871192932; Train ROC 0.9874587654162883; Val ROC 0.6508042973945403\n",
            "Val recall 0.6989186406135559\n",
            "Epoch 231; Train loss 0.2972550392150879; Val loss 0.6087139844894409; Train ROC 0.9877166226512711; Val ROC 0.6519934811183004\n",
            "Epoch 232; Train loss 0.2972240149974823; Val loss 0.6109934449195862; Train ROC 0.9878576976781956; Val ROC 0.6494900730148074\n",
            "Epoch 233; Train loss 0.29743969440460205; Val loss 0.6083158254623413; Train ROC 0.9874379847196477; Val ROC 0.6519973677233455\n",
            "Epoch 234; Train loss 0.2977849543094635; Val loss 0.6079884171485901; Train ROC 0.9875718392836341; Val ROC 0.6524809799852698\n",
            "Epoch 235; Train loss 0.29748064279556274; Val loss 0.6085814833641052; Train ROC 0.9876039925665311; Val ROC 0.6522229287845791\n",
            "Epoch 236; Train loss 0.29738926887512207; Val loss 0.6096080541610718; Train ROC 0.9876921753912057; Val ROC 0.6501784512396717\n",
            "Epoch 237; Train loss 0.29710254073143005; Val loss 0.6082293391227722; Train ROC 0.9877984937363937; Val ROC 0.6526239741733099\n",
            "Epoch 238; Train loss 0.2972642183303833; Val loss 0.6081563830375671; Train ROC 0.9877692384495963; Val ROC 0.6550769855733154\n",
            "Epoch 239; Train loss 0.29708945751190186; Val loss 0.6100445985794067; Train ROC 0.9879602288448871; Val ROC 0.6495876843372886\n",
            "Epoch 240; Train loss 0.29700443148612976; Val loss 0.6097772121429443; Train ROC 0.9879883254184003; Val ROC 0.6482798006425239\n",
            "Val recall 0.6992706060409546\n",
            "Epoch 241; Train loss 0.29710301756858826; Val loss 0.6060414910316467; Train ROC 0.9881105357089961; Val ROC 0.6567791512595116\n",
            "Epoch 242; Train loss 0.2964732348918915; Val loss 0.6076962351799011; Train ROC 0.9882162015199063; Val ROC 0.655635473692407\n",
            "Epoch 243; Train loss 0.29699262976646423; Val loss 0.6105796098709106; Train ROC 0.9880157198639521; Val ROC 0.6501125991165746\n",
            "Epoch 244; Train loss 0.2973654568195343; Val loss 0.6085349321365356; Train ROC 0.9875731787173784; Val ROC 0.6525189330945071\n",
            "Epoch 245; Train loss 0.2971780300140381; Val loss 0.6089391112327576; Train ROC 0.9876199706270576; Val ROC 0.6519216640545296\n",
            "Epoch 246; Train loss 0.29718008637428284; Val loss 0.6085805892944336; Train ROC 0.9880024829325489; Val ROC 0.6535500811164414\n",
            "Epoch 247; Train loss 0.2965331971645355; Val loss 0.606916069984436; Train ROC 0.9880330897819801; Val ROC 0.6544680214921744\n",
            "Epoch 248; Train loss 0.29737618565559387; Val loss 0.6042947173118591; Train ROC 0.9875263556768816; Val ROC 0.6607110422987623\n",
            "Epoch 249; Train loss 0.2973695695400238; Val loss 0.6120367050170898; Train ROC 0.987568796818979; Val ROC 0.6460460125542862\n",
            "Epoch 250; Train loss 0.2970494031906128; Val loss 0.6078410744667053; Train ROC 0.9877384166492824; Val ROC 0.6543647358437471\n",
            "Val recall 0.6987243890762329\n",
            "Epoch 251; Train loss 0.2974259555339813; Val loss 0.6079829931259155; Train ROC 0.9876365014954337; Val ROC 0.6557082741494483\n",
            "Epoch 252; Train loss 0.2973089814186096; Val loss 0.6088629961013794; Train ROC 0.9875859864208436; Val ROC 0.65117915223175\n",
            "Epoch 253; Train loss 0.29744434356689453; Val loss 0.6080091595649719; Train ROC 0.9877113825665269; Val ROC 0.6546443306958177\n",
            "Epoch 254; Train loss 0.29713305830955505; Val loss 0.6085693836212158; Train ROC 0.9879906971554426; Val ROC 0.6514916599356089\n",
            "Epoch 255; Train loss 0.2980444133281708; Val loss 0.6099937558174133; Train ROC 0.9874257121697589; Val ROC 0.64983155706187\n",
            "Epoch 256; Train loss 0.2969178259372711; Val loss 0.6074900031089783; Train ROC 0.9878030110740639; Val ROC 0.65662314986078\n",
            "Epoch 257; Train loss 0.29729166626930237; Val loss 0.609685480594635; Train ROC 0.9877742734213353; Val ROC 0.650193216816241\n",
            "Epoch 258; Train loss 0.29764339327812195; Val loss 0.6075217127799988; Train ROC 0.9877376064394354; Val ROC 0.6536452765205298\n",
            "Epoch 259; Train loss 0.297061949968338; Val loss 0.6088348031044006; Train ROC 0.9879167071534627; Val ROC 0.6519146922365668\n",
            "Epoch 260; Train loss 0.29726800322532654; Val loss 0.6091445088386536; Train ROC 0.9880019683980851; Val ROC 0.6508740243806734\n",
            "Val recall 0.6995184421539307\n",
            "Epoch 261; Train loss 0.2973610460758209; Val loss 0.6083939075469971; Train ROC 0.9878845875619268; Val ROC 0.6530309199216636\n",
            "Epoch 262; Train loss 0.297453910112381; Val loss 0.606812596321106; Train ROC 0.9876123815806311; Val ROC 0.6549629383734561\n",
            "Epoch 263; Train loss 0.29756975173950195; Val loss 0.6086105108261108; Train ROC 0.9878241862291579; Val ROC 0.6517461533075117\n",
            "Epoch 264; Train loss 0.2974022924900055; Val loss 0.6078348755836487; Train ROC 0.9878599279115223; Val ROC 0.6532741409647345\n",
            "Epoch 265; Train loss 0.2973809242248535; Val loss 0.608940601348877; Train ROC 0.9877692856175015; Val ROC 0.6533977960636218\n",
            "Epoch 266; Train loss 0.29712018370628357; Val loss 0.6105714440345764; Train ROC 0.9877831873468382; Val ROC 0.6486264541091369\n",
            "Epoch 267; Train loss 0.2966438829898834; Val loss 0.6085873246192932; Train ROC 0.9880327804952873; Val ROC 0.6517416649244949\n",
            "Epoch 268; Train loss 0.2970244288444519; Val loss 0.6093326807022095; Train ROC 0.9880160800919828; Val ROC 0.651174954463461\n",
            "Epoch 269; Train loss 0.2971489727497101; Val loss 0.6083210706710815; Train ROC 0.9876617472407525; Val ROC 0.6510918210383234\n",
            "Epoch 270; Train loss 0.29731109738349915; Val loss 0.6081985831260681; Train ROC 0.9875394542041742; Val ROC 0.651715427404937\n",
            "Val recall 0.6993652582168579\n",
            "Epoch 271; Train loss 0.29718711972236633; Val loss 0.6086559295654297; Train ROC 0.9878282050694523; Val ROC 0.6513118839037491\n",
            "Epoch 272; Train loss 0.29729607701301575; Val loss 0.6090938448905945; Train ROC 0.9875423442491127; Val ROC 0.6495203527210325\n",
            "Epoch 273; Train loss 0.29678240418434143; Val loss 0.6098759770393372; Train ROC 0.9879859327927142; Val ROC 0.6499495642543321\n",
            "Epoch 274; Train loss 0.297299861907959; Val loss 0.6075960397720337; Train ROC 0.9877176374350616; Val ROC 0.6528677911233475\n",
            "Epoch 275; Train loss 0.29705810546875; Val loss 0.6094868779182434; Train ROC 0.9878492404727814; Val ROC 0.6502137154288022\n",
            "Epoch 276; Train loss 0.2969573140144348; Val loss 0.6074541211128235; Train ROC 0.9876972929741618; Val ROC 0.6537298424707898\n",
            "Epoch 277; Train loss 0.29694509506225586; Val loss 0.609400749206543; Train ROC 0.9878148301382348; Val ROC 0.6498888639377717\n",
            "Epoch 278; Train loss 0.2974836826324463; Val loss 0.6085403561592102; Train ROC 0.9876985305252304; Val ROC 0.6522349173759725\n",
            "Epoch 279; Train loss 0.2968432605266571; Val loss 0.6068523526191711; Train ROC 0.987950135182691; Val ROC 0.6555735023033507\n",
            "Epoch 280; Train loss 0.297252357006073; Val loss 0.6079756021499634; Train ROC 0.987510189214265; Val ROC 0.6528597272985293\n",
            "Val recall 0.6990572214126587\n",
            "Epoch 281; Train loss 0.29763758182525635; Val loss 0.6093209385871887; Train ROC 0.9875404099607004; Val ROC 0.649307150124482\n",
            "Epoch 282; Train loss 0.29675137996673584; Val loss 0.6091592311859131; Train ROC 0.9878574650730402; Val ROC 0.6486645774775077\n",
            "Epoch 283; Train loss 0.297393798828125; Val loss 0.6083884835243225; Train ROC 0.987606682889082; Val ROC 0.651076104359009\n",
            "Epoch 284; Train loss 0.29745787382125854; Val loss 0.6083813905715942; Train ROC 0.9876750405041113; Val ROC 0.6537197590203891\n",
            "Epoch 285; Train loss 0.29713180661201477; Val loss 0.6088493466377258; Train ROC 0.9878979669404038; Val ROC 0.6517574755398832\n",
            "Epoch 286; Train loss 0.2968321144580841; Val loss 0.6112627983093262; Train ROC 0.9880971529613829; Val ROC 0.6500596191710634\n",
            "Epoch 287; Train loss 0.2972084581851959; Val loss 0.6071296334266663; Train ROC 0.9878109305653581; Val ROC 0.6535419732590888\n",
            "Epoch 288; Train loss 0.2971784174442291; Val loss 0.6082914471626282; Train ROC 0.9877184302601663; Val ROC 0.6540115244649958\n",
            "Epoch 289; Train loss 0.2970198392868042; Val loss 0.6075935363769531; Train ROC 0.9879881143757157; Val ROC 0.6573062236329881\n",
            "Epoch 290; Train loss 0.2971387505531311; Val loss 0.6103284358978271; Train ROC 0.9875449930639069; Val ROC 0.6501550611573449\n",
            "Val recall 0.6992554664611816\n",
            "Epoch 291; Train loss 0.2972313463687897; Val loss 0.6079510450363159; Train ROC 0.9880557349585334; Val ROC 0.65528799828515\n",
            "Epoch 292; Train loss 0.2975517213344574; Val loss 0.6073582768440247; Train ROC 0.9876054023478369; Val ROC 0.6538110854324459\n",
            "Epoch 293; Train loss 0.2970494031906128; Val loss 0.6096630096435547; Train ROC 0.9877320999234089; Val ROC 0.6500645948474619\n",
            "Epoch 294; Train loss 0.29744166135787964; Val loss 0.6073883771896362; Train ROC 0.9874944139757311; Val ROC 0.6555864889655262\n",
            "Epoch 295; Train loss 0.29742902517318726; Val loss 0.6072491407394409; Train ROC 0.987672884526544; Val ROC 0.6532432448030263\n",
            "Epoch 296; Train loss 0.29727575182914734; Val loss 0.607257604598999; Train ROC 0.9877815105951885; Val ROC 0.6515685319343403\n",
            "Epoch 297; Train loss 0.29705849289894104; Val loss 0.6082531809806824; Train ROC 0.9879084682680671; Val ROC 0.6520004617427697\n",
            "Epoch 298; Train loss 0.29730677604675293; Val loss 0.606321394443512; Train ROC 0.9874755426358957; Val ROC 0.6555620567798829\n",
            "Epoch 299; Train loss 0.2972172200679779; Val loss 0.6091578602790833; Train ROC 0.9877950892917571; Val ROC 0.6517793186124977\n",
            "Epoch 300; Train loss 0.29728496074676514; Val loss 0.6079138517379761; Train ROC 0.9875697660520496; Val ROC 0.6550046988291767\n",
            "Val recall 0.6990166902542114\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'loss': [tensor(0.6930, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.6928, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.6919, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.6899, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.6862, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.6804, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.6723, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.6616, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.6484, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.6326, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.6142, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.5935, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.5707, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.5463, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.5203, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.4937, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.4665, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.4398, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.4139, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3892, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3665, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3462, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3282, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3129, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3001, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2900, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2824, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2774, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2743, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2733, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2732, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2749, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2776, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2810, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2848, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2888, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2928, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2967, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3004, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3031, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3057, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3075, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3088, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3093, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3092, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3089, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3079, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3065, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3055, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3038, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3023, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.3006, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2991, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2983, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2965, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2960, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2949, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2946, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2943, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2944, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2947, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2948, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2953, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2953, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2960, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2966, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2982, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2982, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2987, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2987, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2984, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2993, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2989, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2990, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2983, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2987, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2988, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2983, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2984, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2978, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2981, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2980, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2978, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2978, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2980, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2980, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2979, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2979, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2977, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2982, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2969, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2977, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2977, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2969, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2980, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2977, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2977, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2978, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2978, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2968, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2978, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2978, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2969, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2978, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2977, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2980, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2978, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2977, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2977, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2978, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2978, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2968, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2966, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2969, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2978, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2965, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2965, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2980, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2969, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2966, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2968, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2969, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2968, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2968, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2975, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2968, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2976, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2970, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2974, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2971, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2972, device='cuda:0', grad_fn=<DivBackward0>),\n",
              "   tensor(0.2973, device='cuda:0', grad_fn=<DivBackward0>)],\n",
              "  'roc': [0.9759589919816611,\n",
              "   0.998071464050684,\n",
              "   0.9993645481853293,\n",
              "   0.9996059223936682,\n",
              "   0.9993859070257233,\n",
              "   0.9992409550665163,\n",
              "   0.9988358574203289,\n",
              "   0.9985147787509042,\n",
              "   0.9981943349614847,\n",
              "   0.9977658882588318,\n",
              "   0.9974758984948306,\n",
              "   0.9970068146203086,\n",
              "   0.9967230655744705,\n",
              "   0.996307462422891,\n",
              "   0.9960504601398827,\n",
              "   0.9957639961094179,\n",
              "   0.9955612941927797,\n",
              "   0.9952149971354215,\n",
              "   0.9950764143258456,\n",
              "   0.9949722701515596,\n",
              "   0.9947813518557813,\n",
              "   0.994614725570277,\n",
              "   0.9946230690336572,\n",
              "   0.9944752880782576,\n",
              "   0.9945726802342875,\n",
              "   0.9946136491986788,\n",
              "   0.9948116973950174,\n",
              "   0.9948085560125265,\n",
              "   0.9948761167594878,\n",
              "   0.9949107840917353,\n",
              "   0.9950686557444911,\n",
              "   0.995114050406051,\n",
              "   0.9952771603916002,\n",
              "   0.995358393227582,\n",
              "   0.9954293187981391,\n",
              "   0.9953601408658549,\n",
              "   0.9953527175809238,\n",
              "   0.9953234837218321,\n",
              "   0.9950923813356058,\n",
              "   0.9952680654761066,\n",
              "   0.9950984385032349,\n",
              "   0.9949439208928398,\n",
              "   0.9948527139316102,\n",
              "   0.9944091043645996,\n",
              "   0.994681728253536,\n",
              "   0.9945077595423669,\n",
              "   0.9943275797614325,\n",
              "   0.9944358908832353,\n",
              "   0.9939658497697674,\n",
              "   0.9938162225277903,\n",
              "   0.993475476189524,\n",
              "   0.9934926399164233,\n",
              "   0.993482621183814,\n",
              "   0.992803417767872,\n",
              "   0.993049076034898,\n",
              "   0.9927265476935966,\n",
              "   0.9927284176988922,\n",
              "   0.9926510647600324,\n",
              "   0.9924508853829683,\n",
              "   0.9921622659139979,\n",
              "   0.9918719350586587,\n",
              "   0.9919615976079045,\n",
              "   0.991743142689031,\n",
              "   0.991910911785497,\n",
              "   0.9917710352276624,\n",
              "   0.9917272403666838,\n",
              "   0.9913580104085503,\n",
              "   0.9915747372301401,\n",
              "   0.9914184151104549,\n",
              "   0.991396857895328,\n",
              "   0.9914377414184282,\n",
              "   0.9912559506513805,\n",
              "   0.9913357650810682,\n",
              "   0.990747224308729,\n",
              "   0.9910047467863495,\n",
              "   0.9907594444348602,\n",
              "   0.9909621203417671,\n",
              "   0.9904771778089018,\n",
              "   0.9901674207843958,\n",
              "   0.9902007442356396,\n",
              "   0.9899407869507492,\n",
              "   0.9904776660641041,\n",
              "   0.9907029805440577,\n",
              "   0.9900848842275125,\n",
              "   0.9899913522928497,\n",
              "   0.990131640963411,\n",
              "   0.9900391382324412,\n",
              "   0.9899280851729225,\n",
              "   0.9898547382716398,\n",
              "   0.9899987825855837,\n",
              "   0.9899522603415574,\n",
              "   0.9900475081098482,\n",
              "   0.9899252046963306,\n",
              "   0.9897105726687965,\n",
              "   0.9898446421836654,\n",
              "   0.9892377102407663,\n",
              "   0.9892780576669316,\n",
              "   0.9897615284263873,\n",
              "   0.9896342354570853,\n",
              "   0.9895414702030311,\n",
              "   0.9896621774546345,\n",
              "   0.9892261899515838,\n",
              "   0.989696275402883,\n",
              "   0.9889116882401044,\n",
              "   0.9892393497971534,\n",
              "   0.9889388950921587,\n",
              "   0.989295132044342,\n",
              "   0.9892019755662053,\n",
              "   0.9892587712495297,\n",
              "   0.989107440033284,\n",
              "   0.9889772862631445,\n",
              "   0.9891124898292216,\n",
              "   0.9891733662101789,\n",
              "   0.9889764732232232,\n",
              "   0.9885007431213182,\n",
              "   0.9884900478661817,\n",
              "   0.9885205921844471,\n",
              "   0.9888987752848276,\n",
              "   0.9886952383615012,\n",
              "   0.9884372173865046,\n",
              "   0.9890658809310148,\n",
              "   0.9890151418762574,\n",
              "   0.9890390499397825,\n",
              "   0.9888917023900501,\n",
              "   0.9883588124685918,\n",
              "   0.9887358814630499,\n",
              "   0.988678605071388,\n",
              "   0.988680209723525,\n",
              "   0.9888945687162706,\n",
              "   0.9886077850221731,\n",
              "   0.988551834048649,\n",
              "   0.9884489875308196,\n",
              "   0.9886144660190405,\n",
              "   0.9887727666302032,\n",
              "   0.9886290049193944,\n",
              "   0.9883811201924743,\n",
              "   0.9882420072156431,\n",
              "   0.9886555953543096,\n",
              "   0.9883995552965689,\n",
              "   0.9884718108672892,\n",
              "   0.9886233513742692,\n",
              "   0.988349041569633,\n",
              "   0.9884306712898463,\n",
              "   0.9883603637536135,\n",
              "   0.988686660541015,\n",
              "   0.9883198447710382,\n",
              "   0.9884657720277608,\n",
              "   0.9882139156284514,\n",
              "   0.9883179827169037,\n",
              "   0.9883006475683601,\n",
              "   0.9883479240945746,\n",
              "   0.9883596459928607,\n",
              "   0.9882328852122955,\n",
              "   0.9879352843003348,\n",
              "   0.9879974582029836,\n",
              "   0.9876626220032471,\n",
              "   0.9882020247342992,\n",
              "   0.9885424367194999,\n",
              "   0.9879651903694595,\n",
              "   0.9882877356912069,\n",
              "   0.9880954820046475,\n",
              "   0.9879777651982381,\n",
              "   0.9881954190713147,\n",
              "   0.9881753224440658,\n",
              "   0.9880766996100049,\n",
              "   0.9882069748038087,\n",
              "   0.9880533395027732,\n",
              "   0.9880981720576677,\n",
              "   0.9881050050699554,\n",
              "   0.9881524464143076,\n",
              "   0.9880208420289333,\n",
              "   0.9879927513850997,\n",
              "   0.988048621095061,\n",
              "   0.9879098512310496,\n",
              "   0.9879843898631504,\n",
              "   0.9882843998421809,\n",
              "   0.9881167107963884,\n",
              "   0.9879893613603652,\n",
              "   0.9885129474798924,\n",
              "   0.9879500989307868,\n",
              "   0.9879754857755247,\n",
              "   0.9880958690510017,\n",
              "   0.9880048653160614,\n",
              "   0.988110639208857,\n",
              "   0.9881067597160315,\n",
              "   0.9882846193750885,\n",
              "   0.987637536494041,\n",
              "   0.9881027668854678,\n",
              "   0.987827537845741,\n",
              "   0.9884591764721844,\n",
              "   0.9875191817080214,\n",
              "   0.9875342948439307,\n",
              "   0.9879026879086593,\n",
              "   0.9878809095434395,\n",
              "   0.9882581006353894,\n",
              "   0.9878535469025322,\n",
              "   0.9875112526483805,\n",
              "   0.9875677835176082,\n",
              "   0.9877058404724234,\n",
              "   0.9874931148368545,\n",
              "   0.9875635539041605,\n",
              "   0.9875966452893106,\n",
              "   0.987925972412477,\n",
              "   0.9878532319556906,\n",
              "   0.98764892821699,\n",
              "   0.9880840728969561,\n",
              "   0.9879437191346856,\n",
              "   0.9876345805488004,\n",
              "   0.9878196035302478,\n",
              "   0.9883075396079122,\n",
              "   0.9876846890360471,\n",
              "   0.987706829246483,\n",
              "   0.9879849992724861,\n",
              "   0.9878513952374595,\n",
              "   0.9879915769042583,\n",
              "   0.9879522837481585,\n",
              "   0.9879821608427128,\n",
              "   0.9875237377233738,\n",
              "   0.9882749236752475,\n",
              "   0.9877089185151556,\n",
              "   0.9877201326172501,\n",
              "   0.9877660609499668,\n",
              "   0.9879530155245173,\n",
              "   0.9877490174338429,\n",
              "   0.987753198396966,\n",
              "   0.9880051993995959,\n",
              "   0.9881231706431628,\n",
              "   0.9875863074321303,\n",
              "   0.9876987613784355,\n",
              "   0.9879928591974545,\n",
              "   0.9874587654162883,\n",
              "   0.9877166226512711,\n",
              "   0.9878576976781956,\n",
              "   0.9874379847196477,\n",
              "   0.9875718392836341,\n",
              "   0.9876039925665311,\n",
              "   0.9876921753912057,\n",
              "   0.9877984937363937,\n",
              "   0.9877692384495963,\n",
              "   0.9879602288448871,\n",
              "   0.9879883254184003,\n",
              "   0.9881105357089961,\n",
              "   0.9882162015199063,\n",
              "   0.9880157198639521,\n",
              "   0.9875731787173784,\n",
              "   0.9876199706270576,\n",
              "   0.9880024829325489,\n",
              "   0.9880330897819801,\n",
              "   0.9875263556768816,\n",
              "   0.987568796818979,\n",
              "   0.9877384166492824,\n",
              "   0.9876365014954337,\n",
              "   0.9875859864208436,\n",
              "   0.9877113825665269,\n",
              "   0.9879906971554426,\n",
              "   0.9874257121697589,\n",
              "   0.9878030110740639,\n",
              "   0.9877742734213353,\n",
              "   0.9877376064394354,\n",
              "   0.9879167071534627,\n",
              "   0.9880019683980851,\n",
              "   0.9878845875619268,\n",
              "   0.9876123815806311,\n",
              "   0.9878241862291579,\n",
              "   0.9878599279115223,\n",
              "   0.9877692856175015,\n",
              "   0.9877831873468382,\n",
              "   0.9880327804952873,\n",
              "   0.9880160800919828,\n",
              "   0.9876617472407525,\n",
              "   0.9875394542041742,\n",
              "   0.9878282050694523,\n",
              "   0.9875423442491127,\n",
              "   0.9879859327927142,\n",
              "   0.9877176374350616,\n",
              "   0.9878492404727814,\n",
              "   0.9876972929741618,\n",
              "   0.9878148301382348,\n",
              "   0.9876985305252304,\n",
              "   0.987950135182691,\n",
              "   0.987510189214265,\n",
              "   0.9875404099607004,\n",
              "   0.9878574650730402,\n",
              "   0.987606682889082,\n",
              "   0.9876750405041113,\n",
              "   0.9878979669404038,\n",
              "   0.9880971529613829,\n",
              "   0.9878109305653581,\n",
              "   0.9877184302601663,\n",
              "   0.9879881143757157,\n",
              "   0.9875449930639069,\n",
              "   0.9880557349585334,\n",
              "   0.9876054023478369,\n",
              "   0.9877320999234089,\n",
              "   0.9874944139757311,\n",
              "   0.987672884526544,\n",
              "   0.9877815105951885,\n",
              "   0.9879084682680671,\n",
              "   0.9874755426358957,\n",
              "   0.9877950892917571,\n",
              "   0.9875697660520496]},\n",
              " 'val': {'loss': [tensor(0.6931, device='cuda:0'),\n",
              "   tensor(0.6931, device='cuda:0'),\n",
              "   tensor(0.6930, device='cuda:0'),\n",
              "   tensor(0.6927, device='cuda:0'),\n",
              "   tensor(0.6923, device='cuda:0'),\n",
              "   tensor(0.6917, device='cuda:0'),\n",
              "   tensor(0.6909, device='cuda:0'),\n",
              "   tensor(0.6897, device='cuda:0'),\n",
              "   tensor(0.6884, device='cuda:0'),\n",
              "   tensor(0.6870, device='cuda:0'),\n",
              "   tensor(0.6852, device='cuda:0'),\n",
              "   tensor(0.6832, device='cuda:0'),\n",
              "   tensor(0.6808, device='cuda:0'),\n",
              "   tensor(0.6787, device='cuda:0'),\n",
              "   tensor(0.6764, device='cuda:0'),\n",
              "   tensor(0.6733, device='cuda:0'),\n",
              "   tensor(0.6706, device='cuda:0'),\n",
              "   tensor(0.6677, device='cuda:0'),\n",
              "   tensor(0.6645, device='cuda:0'),\n",
              "   tensor(0.6619, device='cuda:0'),\n",
              "   tensor(0.6590, device='cuda:0'),\n",
              "   tensor(0.6566, device='cuda:0'),\n",
              "   tensor(0.6542, device='cuda:0'),\n",
              "   tensor(0.6520, device='cuda:0'),\n",
              "   tensor(0.6498, device='cuda:0'),\n",
              "   tensor(0.6475, device='cuda:0'),\n",
              "   tensor(0.6465, device='cuda:0'),\n",
              "   tensor(0.6462, device='cuda:0'),\n",
              "   tensor(0.6443, device='cuda:0'),\n",
              "   tensor(0.6431, device='cuda:0'),\n",
              "   tensor(0.6432, device='cuda:0'),\n",
              "   tensor(0.6426, device='cuda:0'),\n",
              "   tensor(0.6417, device='cuda:0'),\n",
              "   tensor(0.6423, device='cuda:0'),\n",
              "   tensor(0.6408, device='cuda:0'),\n",
              "   tensor(0.6415, device='cuda:0'),\n",
              "   tensor(0.6398, device='cuda:0'),\n",
              "   tensor(0.6414, device='cuda:0'),\n",
              "   tensor(0.6389, device='cuda:0'),\n",
              "   tensor(0.6403, device='cuda:0'),\n",
              "   tensor(0.6396, device='cuda:0'),\n",
              "   tensor(0.6390, device='cuda:0'),\n",
              "   tensor(0.6382, device='cuda:0'),\n",
              "   tensor(0.6366, device='cuda:0'),\n",
              "   tensor(0.6355, device='cuda:0'),\n",
              "   tensor(0.6358, device='cuda:0'),\n",
              "   tensor(0.6344, device='cuda:0'),\n",
              "   tensor(0.6339, device='cuda:0'),\n",
              "   tensor(0.6325, device='cuda:0'),\n",
              "   tensor(0.6302, device='cuda:0'),\n",
              "   tensor(0.6313, device='cuda:0'),\n",
              "   tensor(0.6308, device='cuda:0'),\n",
              "   tensor(0.6289, device='cuda:0'),\n",
              "   tensor(0.6284, device='cuda:0'),\n",
              "   tensor(0.6275, device='cuda:0'),\n",
              "   tensor(0.6263, device='cuda:0'),\n",
              "   tensor(0.6257, device='cuda:0'),\n",
              "   tensor(0.6265, device='cuda:0'),\n",
              "   tensor(0.6249, device='cuda:0'),\n",
              "   tensor(0.6264, device='cuda:0'),\n",
              "   tensor(0.6254, device='cuda:0'),\n",
              "   tensor(0.6252, device='cuda:0'),\n",
              "   tensor(0.6247, device='cuda:0'),\n",
              "   tensor(0.6230, device='cuda:0'),\n",
              "   tensor(0.6224, device='cuda:0'),\n",
              "   tensor(0.6250, device='cuda:0'),\n",
              "   tensor(0.6242, device='cuda:0'),\n",
              "   tensor(0.6238, device='cuda:0'),\n",
              "   tensor(0.6225, device='cuda:0'),\n",
              "   tensor(0.6223, device='cuda:0'),\n",
              "   tensor(0.6216, device='cuda:0'),\n",
              "   tensor(0.6217, device='cuda:0'),\n",
              "   tensor(0.6211, device='cuda:0'),\n",
              "   tensor(0.6216, device='cuda:0'),\n",
              "   tensor(0.6207, device='cuda:0'),\n",
              "   tensor(0.6211, device='cuda:0'),\n",
              "   tensor(0.6217, device='cuda:0'),\n",
              "   tensor(0.6203, device='cuda:0'),\n",
              "   tensor(0.6201, device='cuda:0'),\n",
              "   tensor(0.6191, device='cuda:0'),\n",
              "   tensor(0.6184, device='cuda:0'),\n",
              "   tensor(0.6202, device='cuda:0'),\n",
              "   tensor(0.6191, device='cuda:0'),\n",
              "   tensor(0.6189, device='cuda:0'),\n",
              "   tensor(0.6180, device='cuda:0'),\n",
              "   tensor(0.6192, device='cuda:0'),\n",
              "   tensor(0.6171, device='cuda:0'),\n",
              "   tensor(0.6178, device='cuda:0'),\n",
              "   tensor(0.6181, device='cuda:0'),\n",
              "   tensor(0.6175, device='cuda:0'),\n",
              "   tensor(0.6186, device='cuda:0'),\n",
              "   tensor(0.6184, device='cuda:0'),\n",
              "   tensor(0.6161, device='cuda:0'),\n",
              "   tensor(0.6160, device='cuda:0'),\n",
              "   tensor(0.6154, device='cuda:0'),\n",
              "   tensor(0.6178, device='cuda:0'),\n",
              "   tensor(0.6180, device='cuda:0'),\n",
              "   tensor(0.6155, device='cuda:0'),\n",
              "   tensor(0.6163, device='cuda:0'),\n",
              "   tensor(0.6155, device='cuda:0'),\n",
              "   tensor(0.6161, device='cuda:0'),\n",
              "   tensor(0.6161, device='cuda:0'),\n",
              "   tensor(0.6141, device='cuda:0'),\n",
              "   tensor(0.6145, device='cuda:0'),\n",
              "   tensor(0.6121, device='cuda:0'),\n",
              "   tensor(0.6145, device='cuda:0'),\n",
              "   tensor(0.6141, device='cuda:0'),\n",
              "   tensor(0.6143, device='cuda:0'),\n",
              "   tensor(0.6133, device='cuda:0'),\n",
              "   tensor(0.6157, device='cuda:0'),\n",
              "   tensor(0.6133, device='cuda:0'),\n",
              "   tensor(0.6137, device='cuda:0'),\n",
              "   tensor(0.6150, device='cuda:0'),\n",
              "   tensor(0.6131, device='cuda:0'),\n",
              "   tensor(0.6118, device='cuda:0'),\n",
              "   tensor(0.6132, device='cuda:0'),\n",
              "   tensor(0.6136, device='cuda:0'),\n",
              "   tensor(0.6123, device='cuda:0'),\n",
              "   tensor(0.6108, device='cuda:0'),\n",
              "   tensor(0.6143, device='cuda:0'),\n",
              "   tensor(0.6123, device='cuda:0'),\n",
              "   tensor(0.6119, device='cuda:0'),\n",
              "   tensor(0.6116, device='cuda:0'),\n",
              "   tensor(0.6131, device='cuda:0'),\n",
              "   tensor(0.6141, device='cuda:0'),\n",
              "   tensor(0.6127, device='cuda:0'),\n",
              "   tensor(0.6118, device='cuda:0'),\n",
              "   tensor(0.6110, device='cuda:0'),\n",
              "   tensor(0.6124, device='cuda:0'),\n",
              "   tensor(0.6112, device='cuda:0'),\n",
              "   tensor(0.6107, device='cuda:0'),\n",
              "   tensor(0.6102, device='cuda:0'),\n",
              "   tensor(0.6115, device='cuda:0'),\n",
              "   tensor(0.6102, device='cuda:0'),\n",
              "   tensor(0.6108, device='cuda:0'),\n",
              "   tensor(0.6128, device='cuda:0'),\n",
              "   tensor(0.6115, device='cuda:0'),\n",
              "   tensor(0.6099, device='cuda:0'),\n",
              "   tensor(0.6102, device='cuda:0'),\n",
              "   tensor(0.6113, device='cuda:0'),\n",
              "   tensor(0.6117, device='cuda:0'),\n",
              "   tensor(0.6100, device='cuda:0'),\n",
              "   tensor(0.6113, device='cuda:0'),\n",
              "   tensor(0.6108, device='cuda:0'),\n",
              "   tensor(0.6116, device='cuda:0'),\n",
              "   tensor(0.6096, device='cuda:0'),\n",
              "   tensor(0.6100, device='cuda:0'),\n",
              "   tensor(0.6089, device='cuda:0'),\n",
              "   tensor(0.6112, device='cuda:0'),\n",
              "   tensor(0.6111, device='cuda:0'),\n",
              "   tensor(0.6098, device='cuda:0'),\n",
              "   tensor(0.6110, device='cuda:0'),\n",
              "   tensor(0.6090, device='cuda:0'),\n",
              "   tensor(0.6109, device='cuda:0'),\n",
              "   tensor(0.6107, device='cuda:0'),\n",
              "   tensor(0.6099, device='cuda:0'),\n",
              "   tensor(0.6105, device='cuda:0'),\n",
              "   tensor(0.6097, device='cuda:0'),\n",
              "   tensor(0.6109, device='cuda:0'),\n",
              "   tensor(0.6083, device='cuda:0'),\n",
              "   tensor(0.6119, device='cuda:0'),\n",
              "   tensor(0.6105, device='cuda:0'),\n",
              "   tensor(0.6111, device='cuda:0'),\n",
              "   tensor(0.6111, device='cuda:0'),\n",
              "   tensor(0.6096, device='cuda:0'),\n",
              "   tensor(0.6100, device='cuda:0'),\n",
              "   tensor(0.6107, device='cuda:0'),\n",
              "   tensor(0.6094, device='cuda:0'),\n",
              "   tensor(0.6098, device='cuda:0'),\n",
              "   tensor(0.6077, device='cuda:0'),\n",
              "   tensor(0.6090, device='cuda:0'),\n",
              "   tensor(0.6095, device='cuda:0'),\n",
              "   tensor(0.6080, device='cuda:0'),\n",
              "   tensor(0.6101, device='cuda:0'),\n",
              "   tensor(0.6082, device='cuda:0'),\n",
              "   tensor(0.6085, device='cuda:0'),\n",
              "   tensor(0.6101, device='cuda:0'),\n",
              "   tensor(0.6089, device='cuda:0'),\n",
              "   tensor(0.6103, device='cuda:0'),\n",
              "   tensor(0.6101, device='cuda:0'),\n",
              "   tensor(0.6092, device='cuda:0'),\n",
              "   tensor(0.6077, device='cuda:0'),\n",
              "   tensor(0.6094, device='cuda:0'),\n",
              "   tensor(0.6092, device='cuda:0'),\n",
              "   tensor(0.6084, device='cuda:0'),\n",
              "   tensor(0.6094, device='cuda:0'),\n",
              "   tensor(0.6074, device='cuda:0'),\n",
              "   tensor(0.6088, device='cuda:0'),\n",
              "   tensor(0.6113, device='cuda:0'),\n",
              "   tensor(0.6085, device='cuda:0'),\n",
              "   tensor(0.6082, device='cuda:0'),\n",
              "   tensor(0.6094, device='cuda:0'),\n",
              "   tensor(0.6094, device='cuda:0'),\n",
              "   tensor(0.6095, device='cuda:0'),\n",
              "   tensor(0.6103, device='cuda:0'),\n",
              "   tensor(0.6090, device='cuda:0'),\n",
              "   tensor(0.6099, device='cuda:0'),\n",
              "   tensor(0.6102, device='cuda:0'),\n",
              "   tensor(0.6093, device='cuda:0'),\n",
              "   tensor(0.6100, device='cuda:0'),\n",
              "   tensor(0.6093, device='cuda:0'),\n",
              "   tensor(0.6084, device='cuda:0'),\n",
              "   tensor(0.6087, device='cuda:0'),\n",
              "   tensor(0.6080, device='cuda:0'),\n",
              "   tensor(0.6111, device='cuda:0'),\n",
              "   tensor(0.6087, device='cuda:0'),\n",
              "   tensor(0.6084, device='cuda:0'),\n",
              "   tensor(0.6089, device='cuda:0'),\n",
              "   tensor(0.6088, device='cuda:0'),\n",
              "   tensor(0.6078, device='cuda:0'),\n",
              "   tensor(0.6097, device='cuda:0'),\n",
              "   tensor(0.6103, device='cuda:0'),\n",
              "   tensor(0.6069, device='cuda:0'),\n",
              "   tensor(0.6093, device='cuda:0'),\n",
              "   tensor(0.6081, device='cuda:0'),\n",
              "   tensor(0.6091, device='cuda:0'),\n",
              "   tensor(0.6093, device='cuda:0'),\n",
              "   tensor(0.6100, device='cuda:0'),\n",
              "   tensor(0.6087, device='cuda:0'),\n",
              "   tensor(0.6094, device='cuda:0'),\n",
              "   tensor(0.6082, device='cuda:0'),\n",
              "   tensor(0.6079, device='cuda:0'),\n",
              "   tensor(0.6087, device='cuda:0'),\n",
              "   tensor(0.6071, device='cuda:0'),\n",
              "   tensor(0.6075, device='cuda:0'),\n",
              "   tensor(0.6071, device='cuda:0'),\n",
              "   tensor(0.6090, device='cuda:0'),\n",
              "   tensor(0.6080, device='cuda:0'),\n",
              "   tensor(0.6093, device='cuda:0'),\n",
              "   tensor(0.6082, device='cuda:0'),\n",
              "   tensor(0.6096, device='cuda:0'),\n",
              "   tensor(0.6087, device='cuda:0'),\n",
              "   tensor(0.6110, device='cuda:0'),\n",
              "   tensor(0.6083, device='cuda:0'),\n",
              "   tensor(0.6080, device='cuda:0'),\n",
              "   tensor(0.6086, device='cuda:0'),\n",
              "   tensor(0.6096, device='cuda:0'),\n",
              "   tensor(0.6082, device='cuda:0'),\n",
              "   tensor(0.6082, device='cuda:0'),\n",
              "   tensor(0.6100, device='cuda:0'),\n",
              "   tensor(0.6098, device='cuda:0'),\n",
              "   tensor(0.6060, device='cuda:0'),\n",
              "   tensor(0.6077, device='cuda:0'),\n",
              "   tensor(0.6106, device='cuda:0'),\n",
              "   tensor(0.6085, device='cuda:0'),\n",
              "   tensor(0.6089, device='cuda:0'),\n",
              "   tensor(0.6086, device='cuda:0'),\n",
              "   tensor(0.6069, device='cuda:0'),\n",
              "   tensor(0.6043, device='cuda:0'),\n",
              "   tensor(0.6120, device='cuda:0'),\n",
              "   tensor(0.6078, device='cuda:0'),\n",
              "   tensor(0.6080, device='cuda:0'),\n",
              "   tensor(0.6089, device='cuda:0'),\n",
              "   tensor(0.6080, device='cuda:0'),\n",
              "   tensor(0.6086, device='cuda:0'),\n",
              "   tensor(0.6100, device='cuda:0'),\n",
              "   tensor(0.6075, device='cuda:0'),\n",
              "   tensor(0.6097, device='cuda:0'),\n",
              "   tensor(0.6075, device='cuda:0'),\n",
              "   tensor(0.6088, device='cuda:0'),\n",
              "   tensor(0.6091, device='cuda:0'),\n",
              "   tensor(0.6084, device='cuda:0'),\n",
              "   tensor(0.6068, device='cuda:0'),\n",
              "   tensor(0.6086, device='cuda:0'),\n",
              "   tensor(0.6078, device='cuda:0'),\n",
              "   tensor(0.6089, device='cuda:0'),\n",
              "   tensor(0.6106, device='cuda:0'),\n",
              "   tensor(0.6086, device='cuda:0'),\n",
              "   tensor(0.6093, device='cuda:0'),\n",
              "   tensor(0.6083, device='cuda:0'),\n",
              "   tensor(0.6082, device='cuda:0'),\n",
              "   tensor(0.6087, device='cuda:0'),\n",
              "   tensor(0.6091, device='cuda:0'),\n",
              "   tensor(0.6099, device='cuda:0'),\n",
              "   tensor(0.6076, device='cuda:0'),\n",
              "   tensor(0.6095, device='cuda:0'),\n",
              "   tensor(0.6075, device='cuda:0'),\n",
              "   tensor(0.6094, device='cuda:0'),\n",
              "   tensor(0.6085, device='cuda:0'),\n",
              "   tensor(0.6069, device='cuda:0'),\n",
              "   tensor(0.6080, device='cuda:0'),\n",
              "   tensor(0.6093, device='cuda:0'),\n",
              "   tensor(0.6092, device='cuda:0'),\n",
              "   tensor(0.6084, device='cuda:0'),\n",
              "   tensor(0.6084, device='cuda:0'),\n",
              "   tensor(0.6088, device='cuda:0'),\n",
              "   tensor(0.6113, device='cuda:0'),\n",
              "   tensor(0.6071, device='cuda:0'),\n",
              "   tensor(0.6083, device='cuda:0'),\n",
              "   tensor(0.6076, device='cuda:0'),\n",
              "   tensor(0.6103, device='cuda:0'),\n",
              "   tensor(0.6080, device='cuda:0'),\n",
              "   tensor(0.6074, device='cuda:0'),\n",
              "   tensor(0.6097, device='cuda:0'),\n",
              "   tensor(0.6074, device='cuda:0'),\n",
              "   tensor(0.6072, device='cuda:0'),\n",
              "   tensor(0.6073, device='cuda:0'),\n",
              "   tensor(0.6083, device='cuda:0'),\n",
              "   tensor(0.6063, device='cuda:0'),\n",
              "   tensor(0.6092, device='cuda:0'),\n",
              "   tensor(0.6079, device='cuda:0')],\n",
              "  'recall': [0.65614914894104,\n",
              "   0.6786224246025085,\n",
              "   0.6850169897079468,\n",
              "   0.6873975992202759,\n",
              "   0.6916568875312805,\n",
              "   0.6929010152816772,\n",
              "   0.6946167945861816,\n",
              "   0.6957470774650574,\n",
              "   0.6961187720298767,\n",
              "   0.6970512866973877,\n",
              "   0.6977638602256775,\n",
              "   0.697799801826477,\n",
              "   0.6979697942733765,\n",
              "   0.6982063055038452,\n",
              "   0.6986557245254517,\n",
              "   0.6982023119926453,\n",
              "   0.6984727382659912,\n",
              "   0.697884202003479,\n",
              "   0.6991794109344482,\n",
              "   0.6988611817359924,\n",
              "   0.698244571685791,\n",
              "   0.6993849277496338,\n",
              "   0.6993623971939087,\n",
              "   0.6989186406135559,\n",
              "   0.6992706060409546,\n",
              "   0.6987243890762329,\n",
              "   0.6995184421539307,\n",
              "   0.6993652582168579,\n",
              "   0.6990572214126587,\n",
              "   0.6992554664611816,\n",
              "   0.6990166902542114],\n",
              "  'roc': [0.5164678684820525,\n",
              "   0.5254031294485886,\n",
              "   0.5404252601123636,\n",
              "   0.5494661815486876,\n",
              "   0.5539210675487201,\n",
              "   0.5587480137877725,\n",
              "   0.5613677646539232,\n",
              "   0.5686710448587551,\n",
              "   0.5687426652438718,\n",
              "   0.5697999216232628,\n",
              "   0.573900580560748,\n",
              "   0.5752966173895833,\n",
              "   0.5825423001037858,\n",
              "   0.5796834672528075,\n",
              "   0.5794468011863727,\n",
              "   0.5857844391030957,\n",
              "   0.5884744247972775,\n",
              "   0.5901999453397729,\n",
              "   0.5932661097827018,\n",
              "   0.5955966843172744,\n",
              "   0.5939696293284298,\n",
              "   0.5959737730717345,\n",
              "   0.5955117455582224,\n",
              "   0.6008584048664851,\n",
              "   0.6024941988456771,\n",
              "   0.6053524915642323,\n",
              "   0.607431907457515,\n",
              "   0.6048600170208642,\n",
              "   0.6079977049773337,\n",
              "   0.6118422442746418,\n",
              "   0.6094047293912782,\n",
              "   0.611155348478439,\n",
              "   0.6156700189183143,\n",
              "   0.6110585972581317,\n",
              "   0.6176374600763614,\n",
              "   0.6163167130105178,\n",
              "   0.6215475051074951,\n",
              "   0.6170940134710552,\n",
              "   0.623718270896853,\n",
              "   0.6180516447733873,\n",
              "   0.6195036381470661,\n",
              "   0.6211285854452595,\n",
              "   0.6193260901614521,\n",
              "   0.6254974612279003,\n",
              "   0.6265234222172704,\n",
              "   0.6230233787923849,\n",
              "   0.6247408589940805,\n",
              "   0.6267339036031887,\n",
              "   0.6261800623842381,\n",
              "   0.6288781852907956,\n",
              "   0.6276267366277493,\n",
              "   0.6253233800705027,\n",
              "   0.6276346008384112,\n",
              "   0.6285483786718825,\n",
              "   0.6297417807124477,\n",
              "   0.6329410056036332,\n",
              "   0.6304984533983227,\n",
              "   0.6276614430714428,\n",
              "   0.6325333113021928,\n",
              "   0.6284002062577851,\n",
              "   0.6319558275477176,\n",
              "   0.6308119621084658,\n",
              "   0.6321743463389334,\n",
              "   0.6356356176494398,\n",
              "   0.638103230237886,\n",
              "   0.6309136361661304,\n",
              "   0.6324859675210976,\n",
              "   0.6324881985028457,\n",
              "   0.635396838021349,\n",
              "   0.6366515245434323,\n",
              "   0.6358174866944395,\n",
              "   0.6363411010462066,\n",
              "   0.639051365190277,\n",
              "   0.6355678369013317,\n",
              "   0.6400557443667373,\n",
              "   0.6352792917027481,\n",
              "   0.6343145506138455,\n",
              "   0.6386703047012052,\n",
              "   0.6375798859523305,\n",
              "   0.6364905357262929,\n",
              "   0.6420281259751555,\n",
              "   0.636568294246719,\n",
              "   0.6396405762764411,\n",
              "   0.639745306192,\n",
              "   0.639806646448062,\n",
              "   0.6384032620569695,\n",
              "   0.6430669063030701,\n",
              "   0.6417630295689447,\n",
              "   0.6384074745027701,\n",
              "   0.6410140919316305,\n",
              "   0.6401689608194456,\n",
              "   0.638600351681395,\n",
              "   0.6457253735090562,\n",
              "   0.646009938166521,\n",
              "   0.6438596651187196,\n",
              "   0.6385772668913073,\n",
              "   0.6400810660095776,\n",
              "   0.6435308741836023,\n",
              "   0.64002454291279,\n",
              "   0.643388787065773,\n",
              "   0.6384358461325002,\n",
              "   0.6396067915804698,\n",
              "   0.64408288658462,\n",
              "   0.6433281043622263,\n",
              "   0.6475162177842346,\n",
              "   0.6451096137400905,\n",
              "   0.6451655057038836,\n",
              "   0.6454418098578758,\n",
              "   0.6459803248193179,\n",
              "   0.6444934782268306,\n",
              "   0.6454354456888893,\n",
              "   0.644716356238962,\n",
              "   0.6424361402463465,\n",
              "   0.6459266168692361,\n",
              "   0.6500268941914223,\n",
              "   0.6449109741704506,\n",
              "   0.6462938305939587,\n",
              "   0.6486800358326197,\n",
              "   0.6519544154536911,\n",
              "   0.6449725316536827,\n",
              "   0.6484225570548776,\n",
              "   0.6490568956179004,\n",
              "   0.6503646207955409,\n",
              "   0.6455745826269071,\n",
              "   0.6440430899799383,\n",
              "   0.6484799519958483,\n",
              "   0.6469306085593928,\n",
              "   0.6499596858662626,\n",
              "   0.6472483443928481,\n",
              "   0.650460887657969,\n",
              "   0.6502682599970395,\n",
              "   0.6466505029299219,\n",
              "   0.64935816034795,\n",
              "   0.6489200542426812,\n",
              "   0.647585830285778,\n",
              "   0.6458878271418433,\n",
              "   0.6468842246875497,\n",
              "   0.6481251378328411,\n",
              "   0.6495771987230728,\n",
              "   0.64833480314912,\n",
              "   0.6497538220254624,\n",
              "   0.6512549116751096,\n",
              "   0.6464971141282374,\n",
              "   0.6498704994353826,\n",
              "   0.6490167262044265,\n",
              "   0.6534388784183109,\n",
              "   0.6503913045114484,\n",
              "   0.6525655107095021,\n",
              "   0.6502119746759383,\n",
              "   0.6519019668340962,\n",
              "   0.6524705149195701,\n",
              "   0.6465193358806487,\n",
              "   0.654300536408445,\n",
              "   0.647267334157227,\n",
              "   0.6448949844894223,\n",
              "   0.6503267880418979,\n",
              "   0.6492214187798091,\n",
              "   0.6495967814589165,\n",
              "   0.6458489111878516,\n",
              "   0.6571627920551053,\n",
              "   0.646803809537542,\n",
              "   0.6484740575072299,\n",
              "   0.6460778803672556,\n",
              "   0.6459687090367165,\n",
              "   0.653139430764185,\n",
              "   0.6497097337164179,\n",
              "   0.6469137763892043,\n",
              "   0.6508780049217924,\n",
              "   0.6522742354937794,\n",
              "   0.6524337360112527,\n",
              "   0.6511206770259329,\n",
              "   0.6505925126456009,\n",
              "   0.6531797440172717,\n",
              "   0.6498565440574482,\n",
              "   0.6519213117942535,\n",
              "   0.6527838768545982,\n",
              "   0.6490175275965544,\n",
              "   0.6514921237449722,\n",
              "   0.6508324987651369,\n",
              "   0.6492863374131748,\n",
              "   0.6503942047877209,\n",
              "   0.654208631702435,\n",
              "   0.652973269591993,\n",
              "   0.6504867729172508,\n",
              "   0.6512911445799991,\n",
              "   0.6501693805375646,\n",
              "   0.6535994592006307,\n",
              "   0.6530506083355901,\n",
              "   0.64683415676032,\n",
              "   0.6534064352468908,\n",
              "   0.6542048478399702,\n",
              "   0.6508657345221781,\n",
              "   0.6508050459476269,\n",
              "   0.6495978235622332,\n",
              "   0.6481911895700946,\n",
              "   0.6508728677927672,\n",
              "   0.649566545785226,\n",
              "   0.6496812476020983,\n",
              "   0.6511031990452385,\n",
              "   0.6489932363150215,\n",
              "   0.6509207546417879,\n",
              "   0.6518016724625126,\n",
              "   0.6517117550915598,\n",
              "   0.652966793873919,\n",
              "   0.6494866619611348,\n",
              "   0.6504467972469286,\n",
              "   0.6521407582041961,\n",
              "   0.6495548859700901,\n",
              "   0.6503142681245881,\n",
              "   0.6495137537118619,\n",
              "   0.6483940181015166,\n",
              "   0.6486596252851276,\n",
              "   0.6564876646876234,\n",
              "   0.6491599112001161,\n",
              "   0.6526173135185911,\n",
              "   0.6509695720450378,\n",
              "   0.6501724452019658,\n",
              "   0.6509809471164505,\n",
              "   0.6511756560485107,\n",
              "   0.6504479098023004,\n",
              "   0.6544737545281665,\n",
              "   0.6517181016475325,\n",
              "   0.6552173290027785,\n",
              "   0.6556522236685312,\n",
              "   0.6540969505174296,\n",
              "   0.6534854941948358,\n",
              "   0.6528162847999908,\n",
              "   0.6523464106888308,\n",
              "   0.6521888065058434,\n",
              "   0.6549005501430731,\n",
              "   0.6508042973945403,\n",
              "   0.6519934811183004,\n",
              "   0.6494900730148074,\n",
              "   0.6519973677233455,\n",
              "   0.6524809799852698,\n",
              "   0.6522229287845791,\n",
              "   0.6501784512396717,\n",
              "   0.6526239741733099,\n",
              "   0.6550769855733154,\n",
              "   0.6495876843372886,\n",
              "   0.6482798006425239,\n",
              "   0.6567791512595116,\n",
              "   0.655635473692407,\n",
              "   0.6501125991165746,\n",
              "   0.6525189330945071,\n",
              "   0.6519216640545296,\n",
              "   0.6535500811164414,\n",
              "   0.6544680214921744,\n",
              "   0.6607110422987623,\n",
              "   0.6460460125542862,\n",
              "   0.6543647358437471,\n",
              "   0.6557082741494483,\n",
              "   0.65117915223175,\n",
              "   0.6546443306958177,\n",
              "   0.6514916599356089,\n",
              "   0.64983155706187,\n",
              "   0.65662314986078,\n",
              "   0.650193216816241,\n",
              "   0.6536452765205298,\n",
              "   0.6519146922365668,\n",
              "   0.6508740243806734,\n",
              "   0.6530309199216636,\n",
              "   0.6549629383734561,\n",
              "   0.6517461533075117,\n",
              "   0.6532741409647345,\n",
              "   0.6533977960636218,\n",
              "   0.6486264541091369,\n",
              "   0.6517416649244949,\n",
              "   0.651174954463461,\n",
              "   0.6510918210383234,\n",
              "   0.651715427404937,\n",
              "   0.6513118839037491,\n",
              "   0.6495203527210325,\n",
              "   0.6499495642543321,\n",
              "   0.6528677911233475,\n",
              "   0.6502137154288022,\n",
              "   0.6537298424707898,\n",
              "   0.6498888639377717,\n",
              "   0.6522349173759725,\n",
              "   0.6555735023033507,\n",
              "   0.6528597272985293,\n",
              "   0.649307150124482,\n",
              "   0.6486645774775077,\n",
              "   0.651076104359009,\n",
              "   0.6537197590203891,\n",
              "   0.6517574755398832,\n",
              "   0.6500596191710634,\n",
              "   0.6535419732590888,\n",
              "   0.6540115244649958,\n",
              "   0.6573062236329881,\n",
              "   0.6501550611573449,\n",
              "   0.65528799828515,\n",
              "   0.6538110854324459,\n",
              "   0.6500645948474619,\n",
              "   0.6555864889655262,\n",
              "   0.6532432448030263,\n",
              "   0.6515685319343403,\n",
              "   0.6520004617427697,\n",
              "   0.6555620567798829,\n",
              "   0.6517793186124977,\n",
              "   0.6550046988291767]}}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End try"
      ],
      "metadata": {
        "id": "ZpLPGUhPSn2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a dictionary with key being user and value being positive items for the user\n",
        "def get_user_positive_items(edge_index: Tensor):\n",
        "    '''\n",
        "    Args:\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "    Returns:\n",
        "        user_pos_items (dict): dictionary of positive items for each user\n",
        "    '''\n",
        "    user_pos_items = {}\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        user = edge_index[0][i].item()\n",
        "        item = edge_index[1][i].item()\n",
        "        if user not in user_pos_items:\n",
        "            user_pos_items[user] = []\n",
        "        user_pos_items[user].append(item)\n",
        "    return user_pos_items"
      ],
      "metadata": {
        "id": "aTwKOCt99Ius"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_negative_edges(edge_index, num_users, num_businesses, device=None):\n",
        "  '''\n",
        "  # Get all unique users\n",
        "  users = torch.unique(edge_index[0])\n",
        "\n",
        "  # Get all unique items\n",
        "  items = torch.unique(edge_index[1])\n",
        "\n",
        "  user_pos_items = get_user_positive_items(edge_index) # (dict)\n",
        "  user_neg_items = {}\n",
        "  for user in users:\n",
        "    pos_items = user_pos_items[user.item()]\n",
        "    neg_items = list(set(items) - set(pos_items))\n",
        "    user_neg_items[user.item()] = neg_items\n",
        "\n",
        "  random_neg_item = []\n",
        "  for j in range(edge_index.shape[1]):\n",
        "    user = edge_index[0][j].item()\n",
        "    neg_item = random.choice(user_neg_items[user])\n",
        "    random_neg_item = neg_item\n",
        "\n",
        "  random_neg_item = torch.LongTensor(random_neg_item)\n",
        "  edges = torch.stack((edge_index, random_neg_item), dim=0)\n",
        "  return edges\n",
        "  '''\n",
        "  # Get all unique users\n",
        "  num_users = num_users\n",
        "\n",
        "  # Get all unique items\n",
        "  num_items = num_businesses\n",
        "\n",
        "  mask = torch.zeros(num_users, num_items, device=device, dtype=torch.bool)\n",
        "  mask[edge_index[0], edge_index[1]] = True\n",
        "\n",
        "  sampled_neg_edges = []\n",
        "  for j in range(edge_index.shape[1]):\n",
        "    user_id = edge_index[0][j].item()\n",
        "    user_mask = mask[user_id]\n",
        "\n",
        "    # Get the indices of items that the user has not interacted with (i.e., False in the mask)\n",
        "    user_neg_items = torch.where(~user_mask)[0]\n",
        "    sampled_user_neg_items = user_neg_items[torch.randint(0, len(user_neg_items), (1,), device=device)]\n",
        "    sampled_neg_edges.append(sampled_user_neg_items.item())  # Append the sampled negative item\n",
        "\n",
        "  sampled_neg_edges = torch.LongTensor(sampled_neg_edges).unsqueeze(0)\n",
        "  sampled_neg_edges = sampled_neg_edges.to(edge_index.device)\n",
        "  edges = torch.cat((edge_index, sampled_neg_edges), dim=0)\n",
        "  return edges\n"
      ],
      "metadata": {
        "id": "YiyrHXcN7_UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_mini_batch(batch_size, edge_index, device=None):\n",
        "  # print(edge_index.shape)\n",
        "  # print(\"unique edge_index 0\", edge_index[0].unique())\n",
        "  # print(\"unique edge_index 1\", edge_index[1].unique())\n",
        "\n",
        "\n",
        "  # edges = structured_negative_sampling(edge_index, contains_neg_self_loops=False)\n",
        "  edges = sample_negative_edges(edge_index, num_users, num_businesses, device=device) # customized sampling\n",
        "  # edges = negative_sampling(edge_index, num_nodes=(len(torch.unique(edge_index[0])), len(torch.unique(edge_index[1]))))\n",
        "  # indices = torch.randperm(edges.shape[1])[:batch_size]\n",
        "  indices = random.choices(\n",
        "        [i for i in range(edges[0].shape[0])], k=batch_size)\n",
        "  # print(edges[0])\n",
        "  # print(\"Indices\", indices)\n",
        "  batch = (edges[0][indices], edges[1][indices], edges[2][indices])\n",
        "  user_indices, pos_item_indices, neg_item_indices = batch[0], batch[1], batch[2]\n",
        "  return user_indices, pos_item_indices, neg_item_indices\n"
      ],
      "metadata": {
        "id": "kqYkiQrvO3_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. LightGCN\n",
        "*   Adjacency matrix of an undirected bipartite graph to indicate the existence of interaction between user and item.\n",
        "\n",
        "*   Normalized adjacency matrix $\\tilde{A}$ as $$\\tilde{A} = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$$\n",
        "\n",
        "*   Final node embedding matrix: $$E^{(K)} = \\tilde{A}^{(K)} E W$$\n",
        "\n",
        "*   Multi-scale diffusion: $$\\alpha_0 E^{(0)} + \\alpha_1 E^{(1)} + \\cdots + \\alpha_k E^{(K)}$$\n",
        "and for simplicity, LightGCN uses the uniform coefficient, i.e., $$\\alpha_k = \\frac{1}{K+1} \\quad \\text{for} \\quad k = 0, \\dots, K$$\n"
      ],
      "metadata": {
        "id": "JQlVv7RSO6W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightGCNConv(MessagePassing):\n",
        "  \"\"\" We adopt the LightGCN implementated in this tutorial:\n",
        "  https://colab.research.google.com/drive/1KKugoFyUdydYC0XRyddcROzfQdMwDcnO?usp=sharing#scrollTo=KqKI1VduKcwf\n",
        "  \"\"\"\n",
        "  def __init__(self, num_users, num_items, embedding_dim, num_layers, add_self_loops=False, embedding_initialization='random'):\n",
        "    '''\n",
        "    Args:\n",
        "      num_users: number of users\n",
        "      num_items: number of items\n",
        "      embedding_dim: dimensionality of embeddings\n",
        "      num_layers: number of message passing layers\n",
        "      add_self_loops(optional): whether to add self\n",
        "    '''\n",
        "    super().__init__()\n",
        "    self.num_users = num_users\n",
        "    self.num_items = num_items\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_layers = num_layers\n",
        "    self.add_self_loops = add_self_loops\n",
        "\n",
        "    print(\"Sanity check \", num_users, num_users + num_items)\n",
        "\n",
        "\n",
        "    # Substitute the initial embeddings here\n",
        "    self.user_emb = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.embedding_dim)\n",
        "    self.item_emb = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.embedding_dim)\n",
        "\n",
        "    if embedding_initialization == 'random':\n",
        "      # Random normal initialization\n",
        "      # (other options: uniform/normal Xavier initialization)\n",
        "      nn.init.normal_(self.user_emb.weight, std=0.1)\n",
        "      nn.init.normal_(self.item_emb.weight, std=0.1)\n",
        "    elif embedding_initialization == 'clip':\n",
        "      ### TODO: aggregate embeddings to get user and item features\n",
        "      self.user_emb.weight.data = load_features(\"user\", \"train\")\n",
        "      self.item_emb.weight.data = load_features(\"item\", \"train\")\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, edge_index: SparseTensor):\n",
        "    # Compute normalized adjacency matrix\n",
        "    A_tilde = gcn_norm(edge_index, add_self_loops=self.add_self_loops)\n",
        "\n",
        "    # Concat the embeddings of users and items\n",
        "    emb_0 = torch.cat([self.user_emb.weight, self.item_emb.weight])\n",
        "    embs = [emb_0]\n",
        "\n",
        "    # Multi-scale diffusion\n",
        "    emb_k = emb_0\n",
        "    for k in range(self.num_layers):\n",
        "      emb_k = self.propagate(A_tilde, x=emb_k)\n",
        "      embs.append(emb_k)\n",
        "\n",
        "    embs = torch.stack(embs, dim=1)\n",
        "    emb_final = torch.mean(embs, dim=1) # Uniform coefficient / mean\n",
        "\n",
        "    # Split the embeddings to user embeddings and item embeddings\n",
        "    user_emb_final, item_emb_final = torch.split(emb_final, [self.num_users, self.num_items])\n",
        "    return user_emb_final, self.user_emb.weight, item_emb_final, self.item_emb.weight\n",
        "\n",
        "  def message(self, x_j: Tensor) -> Tensor:\n",
        "    return x_j\n",
        "\n",
        "  def propagate(self, edge_index, x):\n",
        "    x = self.message_and_aggregate(edge_index, x)\n",
        "    return x\n",
        "\n",
        "  def message_and_aggregate(self, adj: SparseTensor, x: Tensor) -> Tensor:\n",
        "    return matmul(adj, x)\n"
      ],
      "metadata": {
        "id": "pB5dQTg4O_ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.b Loss Function\n",
        "* We use **Bayesian Personalized Ranking (BPR) loss** here (a personalized surrogate\n",
        "loss that aligns between with the recall@K metric)\n",
        "$$LL_{\\text{BPR}} = -\\frac{1}{|U|} \\sum_{u \\in U} \\frac{1}{|N_u|} \\frac{1}{|N_u^c|} \\sum_{i \\in N_u} \\sum_{j \\notin N_u} \\ln \\sigma(f_{\\theta}(u,i) - f_{\\theta}(u,j))$$ where $f_{\\theta}(u,v)$ is the score function\n",
        "\n",
        "* Mini-batch training for the BPR loss"
      ],
      "metadata": {
        "id": "dQ63w9JQPDuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bpr_loss(user_emb_final, user_emb_0, pos_item_emb_final, pos_item_emb_0, neg_item_emb_final, neg_item_emb_0, lambda_val):\n",
        "\n",
        "  pos_scores = torch.mul(user_emb_final, pos_item_emb_final).sum(dim=-1)\n",
        "  neg_scores = torch.mul(user_emb_final, neg_item_emb_final).sum(dim=-1)\n",
        "  # Add L2 regularization\n",
        "  reg_loss = lambda_val * (torch.norm(user_emb_0, p=2) + torch.norm(pos_item_emb_0, p=2) + torch.norm(neg_item_emb_0, p=2))\n",
        "  loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean() + reg_loss\n",
        "  return loss"
      ],
      "metadata": {
        "id": "HDcigJblPEp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.c Evaluation Metrics\n",
        "*   **Recall@K** = # of correctly recommended items in K / Total # of items the user interacted with\n",
        "*   **Precision@K** = # of correctly recommended items in K / K"
      ],
      "metadata": {
        "id": "gGto8VBlPI4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_precision_at_K(P_u, R_u, k):\n",
        "  '''\n",
        "  Args:\n",
        "    P_u: the set of positive items the user will interact in the future\n",
        "    R_u: the set of items recommended by the model\n",
        "  Returns:\n",
        "    recall_at_k (float)\n",
        "    precision_at_k (float)\n",
        "  '''\n",
        "  num_correct_rec = R_u[:, :k].sum(dim=1)\n",
        "  true_pos = torch.Tensor([len(P_u[i]) for i in range(len(P_u))])\n",
        "  recall_at_k = torch.mean(num_correct_rec / true_pos)\n",
        "  precision_at_k = torch.mean(num_correct_rec) / k\n",
        "  return recall_at_k.item(), precision_at_k.item()"
      ],
      "metadata": {
        "id": "8Nak0lNrPIEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(model, edge_index, edge_index_sparse, exclude_edge_indices, k, lambda_val):\n",
        "    '''\n",
        "    Args:\n",
        "        model (LighGCN): lightGCN model\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "        edge_index_sparse (SparseTensor): sparse adjacency matrix\n",
        "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
        "        k (int): determines the top k items to compute metrics on\n",
        "        lambda_val (float): determines lambda for bpr loss\n",
        "\n",
        "    Returns:\n",
        "        tuple: bpr loss, recall @ k, precision @ k\n",
        "    '''\n",
        "    # Get embeddings\n",
        "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(edge_index_sparse)\n",
        "    # edges = structured_negative_sampling(edge_index, contains_neg_self_loops=False)\n",
        "    edges = sample_mini_batch(args['batch_size'], edge_index, device=device)\n",
        "\n",
        "    user_indices, pos_item_indices, neg_item_indices = edges[0], edges[1], edges[2]\n",
        "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[pos_item_indices], items_emb_0[pos_item_indices]\n",
        "    neg_items_emb_final, neg_items_emb_0 = items_emb_final[neg_item_indices], items_emb_0[neg_item_indices]\n",
        "\n",
        "    # Compute loss\n",
        "    loss = bpr_loss(users_emb_final, users_emb_0,\n",
        "                    pos_items_emb_final, pos_items_emb_0,\n",
        "                    neg_items_emb_final, neg_items_emb_0,\n",
        "                    lambda_val).item()\n",
        "\n",
        "    user_embedding = model.user_emb.weight\n",
        "    item_embedding = model.item_emb.weight\n",
        "\n",
        "    # Compute score between users and items\n",
        "    score = torch.matmul(user_embedding, item_embedding.T)\n",
        "\n",
        "    # Exclude existing edges\n",
        "    for exclude_edge_index in exclude_edge_indices:\n",
        "        # Get all the positive items for each user from exclude_edge_index\n",
        "        user_pos_items = get_user_positive_items(exclude_edge_index)\n",
        "        exclude_users = []\n",
        "        exclude_items = []\n",
        "        for user, items in user_pos_items.items():\n",
        "            exclude_users.extend([user] * len(items))\n",
        "            exclude_items.extend(items)\n",
        "        score[exclude_users, exclude_items] = float('-inf')\n",
        "\n",
        "    # Get the top k recommended items for each user\n",
        "    _, top_K_items = torch.topk(score, k=k)\n",
        "\n",
        "    # Get all unique users\n",
        "    users = edge_index[0].unique()\n",
        "\n",
        "    # Get P_u, the set of positive items the user will interact in the future\n",
        "    user_pos_items = get_user_positive_items(edge_index) # (dict)\n",
        "    P_u = [user_pos_items[user.item()] for user in users] # (list)\n",
        "\n",
        "    # Get the correctly recommended labels in R_u (intersection of P_u and R_u)\n",
        "    R_u = []\n",
        "    for user in users:\n",
        "        pos_items = user_pos_items[user.item()]\n",
        "        label = list(map(lambda x: x in pos_items, top_K_items[user]))\n",
        "        R_u.append(label)\n",
        "    R_u = torch.Tensor(np.array(R_u).astype('float'))\n",
        "\n",
        "    # Compute recall@k & precision@k\n",
        "    recall, precision = recall_precision_at_K(P_u, R_u, k)\n",
        "\n",
        "    return loss, recall, precision"
      ],
      "metadata": {
        "id": "Ca2NtjFKaLqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training"
      ],
      "metadata": {
        "id": "xb2m85L6UzN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model args\n",
        "args = {\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "    'embedding_dim': 32,\n",
        "    'num_layers': 3,\n",
        "    'topK': 20,\n",
        "    'lr': 0.01,\n",
        "    'weight_decay': 0.1,\n",
        "    'batch_size': 256,\n",
        "    'num_epoch': 1000,\n",
        "    'eval_epoch': 10,\n",
        "    'lambda': 1e-6,\n",
        "}"
      ],
      "metadata": {
        "id": "TOJl3pdZiMu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is not used for now. We can use different optimizers if needed.\n",
        "def build_optimizer(args, params):\n",
        "    weight_decay = args.weight_decay\n",
        "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
        "    if args.opt == 'adam':\n",
        "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'sgd':\n",
        "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
        "    elif args.opt == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'adagrad':\n",
        "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    if args.opt_scheduler == 'none':\n",
        "        return None, optimizer\n",
        "    elif args.opt_scheduler == 'step':\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
        "    elif args.opt_scheduler == 'cos':\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
        "    return scheduler, optimizer"
      ],
      "metadata": {
        "id": "8vnbYmVsXO4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device {device}.\")\n",
        "\n",
        "model = LightGCNConv(num_users, num_businesses, args['embedding_dim'], args['num_layers']) # LightGCN model\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=args['weight_decay'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3Y_5eGLUuzy",
        "outputId": "62c97dc7-cc57-49cf-f6ed-7097e99639a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda.\n",
            "Sanity check  29596 57492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "edge_index, edge_index_sparse = generate_edge_index(train_data, train_user_id_map, train_business_id_map)\n",
        "\n",
        "num_interactions = edge_index.shape[1]\n",
        "all_indices = [i for i in range(num_interactions)]\n",
        "\n",
        "train_indices, test_indices = train_test_split(\n",
        "    all_indices, test_size=0.2, random_state=1)\n",
        "val_indices, test_indices = train_test_split(\n",
        "    test_indices, test_size=0.5, random_state=1)\n",
        "\n",
        "train_edge_index = edge_index[:, train_indices]\n",
        "val_edge_index = edge_index[:, val_indices]\n",
        "test_edge_index = edge_index[:, test_indices]"
      ],
      "metadata": {
        "id": "uDejKymLqWHX",
        "outputId": "23e8ee7a-a7e5-4219-af4f-2c522081a0e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-2b12e6acca7e>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  row=torch.tensor(edge_index[0]),\n",
            "<ipython-input-9-2b12e6acca7e>:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  col=torch.tensor(edge_index[1]),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sparse_edge_index = SparseTensor(row=train_edge_index[0], col=train_edge_index[1], sparse_sizes=(\n",
        "    num_nodes, num_nodes))\n",
        "val_sparse_edge_index = SparseTensor(row=val_edge_index[0], col=val_edge_index[1], sparse_sizes=(\n",
        "    num_nodes, num_nodes))\n",
        "test_sparse_edge_index = SparseTensor(row=test_edge_index[0], col=test_edge_index[1], sparse_sizes=(\n",
        "    num_nodes, num_nodes))"
      ],
      "metadata": {
        "id": "kllyLGdaqczX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_edge_index = train_edge_index.to(device)\n",
        "train_edge_index_sparse = train_sparse_edge_index.to(device)\n",
        "\n",
        "val_edge_index = val_edge_index.to(device)\n",
        "val_edge_index_sparse = val_sparse_edge_index.to(device)\n",
        "\n",
        "test_edge_index = test_edge_index.to(device)\n",
        "test_edge_index_sparse = test_sparse_edge_index.to(device)"
      ],
      "metadata": {
        "id": "HMwE27ZbUj4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(args['num_epoch']):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Forward propagation\n",
        "  users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model(train_edge_index_sparse)\n",
        "\n",
        "  # print(users_emb_final.shape)\n",
        "  # print(users_emb_0.shape)\n",
        "  # print(items_emb_final.shape)\n",
        "  # print(items_emb_0.shape)\n",
        "\n",
        "  # Mini batching\n",
        "  user_indices, pos_item_indices, neg_item_indices = sample_mini_batch(args['batch_size'], train_edge_index, device=device)\n",
        "  user_indices = user_indices.to(device)\n",
        "  pos_item_indices = pos_item_indices.to(device)\n",
        "  neg_item_indices = neg_item_indices.to(device)\n",
        "\n",
        "  users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "  pos_items_emb_final, pos_items_emb_0 = items_emb_final[pos_item_indices], items_emb_0[pos_item_indices]\n",
        "  neg_items_emb_final, neg_items_emb_0 = items_emb_final[neg_item_indices], items_emb_0[neg_item_indices]\n",
        "\n",
        "  # Compute loss\n",
        "  train_loss = bpr_loss(users_emb_final, users_emb_0,\n",
        "                        pos_items_emb_final, pos_items_emb_0,\n",
        "                        neg_items_emb_final, neg_items_emb_0,\n",
        "                        args['lambda'])\n",
        "\n",
        "  # print(f'Epoch: {epoch}',\n",
        "  #       f'loss: {train_loss}')\n",
        "\n",
        "  train_losses.append(train_loss.item())\n",
        "\n",
        "  train_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # if scheduler:\n",
        "  #   scheduler.step()\n",
        "\n",
        "\n",
        "    # start validation loss evaluation\n",
        "  if epoch % args['eval_epoch'] == 0:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      val_loss, recall, precision = evaluation(model, val_edge_index, val_edge_index_sparse, [train_edge_index], args['topK'], args['lambda'])\n",
        "      val_losses.append(val_loss)\n",
        "      print('Epoch {:d}: train_loss: {:.4f}, val_loss: {:.4f}, recall: {:.4f}, precision: {:.4f}'\\\n",
        "            .format(epoch, train_loss, val_loss, recall, precision))\n",
        "      # scheduler.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "Ri2Cf5CcV8Gr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "outputId": "936c58f2-3f96-49d9-d344-bce438fc80ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train_loss: 0.6930, val_loss: 0.6932, recall: 0.0008, precision: 0.0000\n",
            "Epoch 10: train_loss: 0.6921, val_loss: 0.6929, recall: 0.0009, precision: 0.0001\n",
            "Epoch 20: train_loss: 0.6915, val_loss: 0.6928, recall: 0.0013, precision: 0.0001\n",
            "Epoch 30: train_loss: 0.6902, val_loss: 0.6922, recall: 0.0021, precision: 0.0001\n",
            "Epoch 40: train_loss: 0.6887, val_loss: 0.6925, recall: 0.0026, precision: 0.0002\n",
            "Epoch 50: train_loss: 0.6869, val_loss: 0.6921, recall: 0.0029, precision: 0.0002\n",
            "Epoch 60: train_loss: 0.6824, val_loss: 0.6920, recall: 0.0037, precision: 0.0002\n",
            "Epoch 70: train_loss: 0.6839, val_loss: 0.6918, recall: 0.0058, precision: 0.0003\n",
            "Epoch 80: train_loss: 0.6760, val_loss: 0.6920, recall: 0.0074, precision: 0.0004\n",
            "Epoch 90: train_loss: 0.6764, val_loss: 0.6914, recall: 0.0076, precision: 0.0005\n",
            "Epoch 100: train_loss: 0.6729, val_loss: 0.6901, recall: 0.0088, precision: 0.0005\n",
            "Epoch 110: train_loss: 0.6726, val_loss: 0.6906, recall: 0.0090, precision: 0.0005\n",
            "Epoch 120: train_loss: 0.6625, val_loss: 0.6879, recall: 0.0099, precision: 0.0006\n",
            "Epoch 130: train_loss: 0.6454, val_loss: 0.6884, recall: 0.0114, precision: 0.0007\n",
            "Epoch 140: train_loss: 0.6557, val_loss: 0.6880, recall: 0.0125, precision: 0.0007\n",
            "Epoch 150: train_loss: 0.6449, val_loss: 0.6870, recall: 0.0129, precision: 0.0008\n",
            "Epoch 160: train_loss: 0.6396, val_loss: 0.6827, recall: 0.0133, precision: 0.0008\n",
            "Epoch 170: train_loss: 0.6322, val_loss: 0.6822, recall: 0.0139, precision: 0.0008\n",
            "Epoch 180: train_loss: 0.6166, val_loss: 0.6821, recall: 0.0140, precision: 0.0008\n",
            "Epoch 190: train_loss: 0.6114, val_loss: 0.6809, recall: 0.0143, precision: 0.0008\n",
            "Epoch 200: train_loss: 0.5986, val_loss: 0.6770, recall: 0.0147, precision: 0.0008\n",
            "Epoch 210: train_loss: 0.6045, val_loss: 0.6751, recall: 0.0155, precision: 0.0009\n",
            "Epoch 220: train_loss: 0.5600, val_loss: 0.6791, recall: 0.0154, precision: 0.0009\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6e6d292acffc>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m# Mini batching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0muser_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_item_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_item_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0muser_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mpos_item_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_item_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-5404f754d7ca>\u001b[0m in \u001b[0;36msample_mini_batch\u001b[0;34m(batch_size, edge_index, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# edges = structured_negative_sampling(edge_index, contains_neg_self_loops=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_negative_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_businesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# customized sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;31m# edges = negative_sampling(edge_index, num_nodes=(len(torch.unique(edge_index[0])), len(torch.unique(edge_index[1]))))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# indices = torch.randperm(edges.shape[1])[:batch_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-3dbf6376e2e3>\u001b[0m in \u001b[0;36msample_negative_edges\u001b[0;34m(edge_index, num_users, num_businesses, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0muser_neg_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0muser_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0msampled_user_neg_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_neg_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_neg_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0msampled_neg_edges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_user_neg_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Append the sampled negative item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0msampled_neg_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_neg_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w4yacM6k6xiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8HmRrxm6yeF",
        "outputId": "e6530c29-9667-4e2e-a0f4-62a1925f88bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87013"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "items_emb_final.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMx9MiQ8utoU",
        "outputId": "43dd5d09-43cd-4dc7-d430-1edff27ee011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([30831, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(args['num_epoch'])\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "ax.plot(epochs, train_losses, color='r', label='Train', alpha=1)\n",
        "ax.grid(color='g', ls='-.', lw=0.5)\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Losses')\n",
        "plt.title('Training Losses')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9xwPayiYoMPJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "outputId": "6e56ba2e-e076-404b-d604-cb61c6bb3875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAJJCAYAAADr6NUGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC9KklEQVR4nOzdd3gU5doG8HtTIYQETCgBQwsYRIqIBjFSjlIESwRRROnoQRQLqKASKRr0AxU5igoqHUUEUVBAQRRQRKMQFRAiPbRQAiSEQOp+f7xsZmZ3tmUmmZ3N/buuXDt933CeM/LwvMVitVqtICIiIiIiIp8XYHQDiIiIiIiIyDNM4IiIiIiIiEyCCRwREREREZFJMIEjIiIiIiIyCSZwREREREREJsEEjoiIiIiIyCSYwBEREREREZkEEzgiIiIiIiKTYAJHRERERERkEkzgiIjItIYMGYJGjRqV6d5JkybBYrHo2yAiIqJyxgSOiIh0Z7FYPPrZuHGj0U01xJAhQxAeHm50M4iIyIQsVqvVanQjiIjIvyxevFixv3DhQqxfvx6LFi1SHO/WrRvq1KlT5u8pLCxESUkJQkNDvb63qKgIRUVFqFKlSpm/v6yGDBmC5cuXIzc3t8K/m4iIzC3I6AYQEZH/GTBggGL/119/xfr16x2O28vLy0NYWJjH3xMcHFym9gFAUFAQgoL4n0EiIjIXdqEkIiJDdOnSBS1btsS2bdvQqVMnhIWF4aWXXgIArFy5EnfeeSfq1auH0NBQxMXF4dVXX0VxcbHiGfZj4A4dOgSLxYI333wTH374IeLi4hAaGoqbbroJv//+u+JetTFwFosFo0aNwldffYWWLVsiNDQU1113Hb799luH9m/cuBE33ngjqlSpgri4OMyePVv3cXXLli1Du3btULVqVURHR2PAgAE4duyY4prMzEwMHToUV199NUJDQxETE4OkpCQcOnSo9Jo//vgDPXr0QHR0NKpWrYrGjRtj2LBhiueUlJRgxowZuO6661ClShXUqVMHI0aMwLlz5xTXefIsIiIqP/ynRyIiMkxWVhZ69uyJBx98EAMGDCjtTjl//nyEh4djzJgxCA8Pxw8//IAJEyYgJycHb7zxhtvnfvrpp7hw4QJGjBgBi8WCadOmoU+fPjhw4IDbqt3PP/+MFStW4PHHH0f16tXxzjvv4L777kNGRgaioqIAAGlpabjjjjsQExODyZMno7i4GK+88gpq1aql/Q/livnz52Po0KG46aab8Prrr+PkyZP43//+hy1btiAtLQ01atQAANx3333YtWsXnnzySTRq1AinTp3C+vXrkZGRUbrfvXt31KpVCy+88AJq1KiBQ4cOYcWKFYrvGzFiROl3PvXUUzh48CBmzpyJtLQ0bNmyBcHBwR4/i4iIypGViIionD3xxBNW+//kdO7c2QrAOmvWLIfr8/LyHI6NGDHCGhYWZr18+XLpscGDB1sbNmxYun/w4EErAGtUVJT17NmzpcdXrlxpBWD9+uuvS49NnDjRoU0ArCEhIdZ9+/aVHvvrr7+sAKzvvvtu6bG7777bGhYWZj127Fjpsb1791qDgoIcnqlm8ODB1mrVqjk9X1BQYK1du7a1ZcuW1kuXLpUe/+abb6wArBMmTLBarVbruXPnrACsb7zxhtNnffnll1YA1t9//93pNT/99JMVgPWTTz5RHP/2228Vxz15FhERlS92oSQiIsOEhoZi6NChDserVq1aun3hwgWcOXMGHTt2RF5eHvbs2eP2uf369UPNmjVL9zt27AgAOHDggNt7u3btiri4uNL91q1bIyIiovTe4uJifP/997j33ntRr1690uuaNm2Knj17un2+J/744w+cOnUKjz/+uGKSlTvvvBPNmzfH6tWrAYg/p5CQEGzcuNGhq6ONrVL3zTffoLCwUPWaZcuWITIyEt26dcOZM2dKf9q1a4fw8HD8+OOPHj+LiIjKFxM4IiIyTP369RESEuJwfNeuXejduzciIyMRERGBWrVqlU6Akp2d7fa5DRo0UOzbkjlnSY6re2332+49deoULl26hKZNmzpcp3asLA4fPgwAiI+PdzjXvHnz0vOhoaGYOnUq1q5dizp16qBTp06YNm0aMjMzS6/v3Lkz7rvvPkyePBnR0dFISkrCvHnzkJ+fX3rN3r17kZ2djdq1a6NWrVqKn9zcXJw6dcrjZxERUfniGDgiIjKMvNJmc/78eXTu3BkRERF45ZVXEBcXhypVqmD79u0YN24cSkpK3D43MDBQ9bjVg5VztNxrhGeeeQZ33303vvrqK3z33Xd4+eWX8frrr+OHH35A27ZtYbFYsHz5cvz666/4+uuv8d1332HYsGF466238OuvvyI8PBwlJSWoXbs2PvnkE9XvsI3t8+RZRERUvliBIyIin7Jx40ZkZWVh/vz5ePrpp3HXXXeha9euii6RRqpduzaqVKmCffv2OZxTO1YWDRs2BACkp6c7nEtPTy89bxMXF4dnn30W69atw86dO1FQUIC33npLcc3NN9+MKVOm4I8//sAnn3yCXbt24bPPPiu9PysrC4mJiejatavDT5s2bTx+FhERlS8mcERE5FNsFTB5xaugoADvv/++UU1SCAwMRNeuXfHVV1/h+PHjpcf37duHtWvX6vIdN954I2rXro1Zs2YpuieuXbsWu3fvxp133glArJt3+fJlxb1xcXGoXr166X3nzp1zqB5ef/31AFB6zQMPPIDi4mK8+uqrDm0pKirC+fPnPX4WERGVL3ahJCIin3LLLbegZs2aGDx4MJ566ilYLBYsWrTIp7owTpo0CevWrUNiYiJGjhyJ4uJizJw5Ey1btsSff/7p0TMKCwuRkpLicPyqq67C448/jqlTp2Lo0KHo3Lkz+vfvX7qMQKNGjTB69GgAwL///ovbb78dDzzwAFq0aIGgoCB8+eWXOHnyJB588EEAwIIFC/D++++jd+/eiIuLw4ULF/DRRx8hIiICvXr1AiDGto0YMQKvv/46/vzzT3Tv3h3BwcHYu3cvli1bhv/973/o27evR88iIqLyxQSOiIh8SlRUFL755hs8++yzSE5ORs2aNTFgwADcfvvt6NGjh9HNAwC0a9cOa9euxXPPPYeXX34ZsbGxeOWVV7B7926PZskERFXx5ZdfdjgeFxeHxx9/HEOGDEFYWBj+7//+D+PGjUO1atXQu3dvTJ06tXQ2yNjYWPTv3x8bNmzAokWLEBQUhObNm+Pzzz/HfffdB0AkZ6mpqfjss89w8uRJREZGIiEhAZ988gkaN25c+r2zZs1Cu3btMHv2bLz00ksICgpCo0aNMGDAACQmJnr1LCIiKj8Wqy/9kyYREZGJ3Xvvvdi1axf27t1rdFOIiMhPcQwcERFRGVy6dEmxv3fvXqxZswZdunQxpkFERFQpsAJHRERUBjExMRgyZAiaNGmCw4cP44MPPkB+fj7S0tLQrFkzo5tHRER+imPgiIiIyuCOO+7AkiVLkJmZidDQUHTo0AGvvfYakzciIipXrMARERERERGZBMfAERERERERmQQTOCIiIiIiIpPgGDgVJSUlOH78OKpXrw6LxWJ0c4iIiIiIyI9ZrVZcuHAB9erVQ0CA6xobEzgVx48fR2xsrNHNICIiIiKiSuTIkSO4+uqrXV7DBE5F9erVAYg/wIiICINbI0zbMg1jE8ca3QwyOcYR6YFxRFoxhkgPjCPSypdiKCcnB7GxsaV5iCuchVJFTk4OIiMjkZ2d7TMJ3PELx1Gvej2jm0EmxzgiPTCOSCvGEOmBcURa+VIMeZN/cBITk3jsm8eMbgL5AcYR6YFxRFoxhkgPjCPSyqwxxASOiIiIiIjIJHwigXvvvffQqFEjVKlSBe3bt0dqaqrTa7t06QKLxeLwc+edd5ZeY7VaMWHCBMTExKBq1aro2rUr9u7dWxG/ChERERERUbkxfBKTpUuXYsyYMZg1axbat2+PGTNmoEePHkhPT0ft2rUdrl+xYgUKCgpK97OystCmTRvcf//9pcemTZuGd955BwsWLEDjxo3x8ssvo0ePHvjnn39QpUqVCvm9iIiIiIj8RXFxMQoLC41uhq6igqNw+fLlCvmuwMBABAUF6bJEmeGTmLRv3x433XQTZs6cCUCswRYbG4snn3wSL7zwgtv7Z8yYgQkTJuDEiROoVq0arFYr6tWrh2effRbPPfccACA7Oxt16tTB/Pnz8eCDDzo8Iz8/H/n5+aX7tllgfGkSk9+O/ob2V7c3uhlkcowj0gPjiLRiDJEeGEcVJzc3F0ePHoW/zX2YX5SP0KDQCvu+sLAwxMTEICQkxOGcN5OYGFqBKygowLZt2/Diiy+WHgsICEDXrl2xdetWj54xZ84cPPjgg6hWrRoA4ODBg8jMzETXrl1Lr4mMjET79u2xdetW1QTu9ddfx+TJkx2O91vWD8FhwQCAqsFVsbTvUqRsTsGwtsMcBj2O7zgeaZlp6NuiL6b+PBXpWeml5xJjE5HUPAk7Tu5AWHAYZm+brbj3iwe+wNQtUzGmwxg8uFzZvlEJo5CVl4V9Z/dh/YH1SD0mdS9tWbslRiWMwtq9axEfHY9pW6Yp7p2bNBez/piF5E7J6LO0D4pKikrPDWw9EGHBYYipHoMtGVuw/sD60nOxEbF4o/sb+OD3D9A9rjvG/zBe8dy3e7yNJTuXILlTMoauHIqsvKzSc0nxSYiPjkdRSREysjOw/J/lpeciQiOwuM9ipGxOwaA2gzBqzSjFcyd1mYQtGVswqM0gTNw4EQfOHSg917lhZ3SP64795/aL3y1truLeVf1XIWVzCp5MeBIDvxyoODemwxhkZGegS6MumLN9DtIy00rPta3bFsNvGI6NhzaiQWQDTN86XXHvot6L8G7qu0julIx7ltyjODes7TAAQFzNOKzbvw6bDm8qPdekZhNM7jIZC/9aiMQGiZi0cZLi3pm9ZmLhXwuR3CkZA1YMQE5+Tum5vi36okFkAwQFBCH9TDpWpq8sPRcVFoV5SfOQsjkF/Vv2x+jvRiueO+W2KVi3fx1G3jQSz697HkdyjpSe69akG07knsCZvDPIK8zDor8XlZ4LCgjCin4rkLI5BY/d+BiGrRymeO7YxLFIP5OOns16YmbqTOw8tbP0XEL9BPRv2R+px1IRFRaFmakzFfd+1vczTN86HeMSx+G+z+9TnBvRbgTyCvPQqk4rrNyzEluObCk9Fx8Vj3G3jsPyf5ajbd22mPLTFMW9s+6ahblpc5HcKRn9lvfDpcJLpef6t+yPqLAoRIZGIi0zDWv2rik9FxMeg5m9ZmL61ulIap6EseuV0wdP6zYNK/esxJgOYzBqzSicyD1Req5Xs15oW7ctsvOzkZWXhSU7l5SeM/odkVA/AUt2LqmQd0R2fjbWPryW7wj43zsisUEiTlw4Ue7viEPnD6FRjUYA+I6w8ad3REX9PeLRrx8tjSOA7wgbvd8RwZZgPNLwETSu3Ri1a9VGsbUYJdYSnL90vvQ+i8WChjUa4tTFU6hZpSaO5RxTPLdWtVrIL8pHeGg4zl46i8uFUtUrLCQMkaGRyCvMQ1BAkCIeACA2MhZZl7IQHRaNjPMZinNXhV2FEmsJqgRWQU5BDvIK8krPhQaFIjosGjn5OagSVAWnL55W3Fsvoh6O5xxHo5qNkHE+AyXWktJzkVUiERQQhABLAC4XXcaF/Aul54IDgxFTPQZn8s4gIiQCmbmZiufWDa+LnIIcRIdF48SFEygsFlVLa7EV2ReycSb9DLKqZiHrkvIdEVToeVpmaAXu+PHjqF+/Pn755Rd06NCh9PjYsWOxadMm/Pbbby7vT01NRfv27fHbb78hISEBAPDLL78gMTERx48fR0xMTOm1DzzwACwWC5YuXerwHDNU4M7knUF0WLTRzSCTYxyRHhhHpBVjiPTAOKoYly9fxsGDB9GoUSNUrVrV6OboqrC4EMGBwRX2fXl5eTh8+DAaN27sMKyr0iwjMGfOHLRq1ao0eSur0NBQREREKH58zdSfpxrdBPIDjCPSA+OItGIMkR4YRxVLj7Fbvsa+elbeAgL0Sb0MTeCio6MRGBiIkydPKo6fPHkSdevWdXnvxYsX8dlnn2H48OGK47b7yvJMXybvSkFUVowj0gPjiLRiDJEeGEek1eWiipnARG+GJnAhISFo164dNmzYUHqspKQEGzZsUHSpVLNs2TLk5+djwIABiuONGzdG3bp1Fc/MycnBb7/95vaZREREREREvszwLpRjxozBRx99hAULFmD37t0YOXIkLl68iKFDhwIABg0apJjkxGbOnDm49957ERUVpThusVjwzDPPICUlBatWrcKOHTswaNAg1KtXD/fee29F/EpERERERORnGjVqhBkzZhjdDOPXgevXrx9Onz6NCRMmIDMzE9dffz2+/fZb1KlTBwCQkZHh0F80PT0dP//8M9atW6f6zLFjx+LixYv473//i/Pnz+PWW2/Ft99+a+o14BJjE41uAvkBxhHpgXFEWjGGSA+MI3LG3Xi9iRMnYtKkSQgPCffqub///nvpzPdGMnwdOF/kzSwwFWXPmT1oHt3c6GaQyTGOSA+MI9KKMUR6YBxVDNsslGozJ/qqzExpcpKlS5diwoQJSE+XxkyGh4cjPDwclwovoUpQFRQXFyMoqPzrWq7+LCvNLJSVyY6TO4xuAvkBxhHpgXFEWjGGSA+MI4NYrcDFi8b8eFh3qlu3bulPZGQkLBZL6f6ePXtQvXp1rF27Fh0SOiA0NBQ///wz9u/fj6SkJNSpUwfh4eG46aab8P333yuea9+F0mKx4OOPP0bv3r0RFhaGZs2aYdWqVXr+aatiAmcSYcFhRjeB/ADjiPTAOCKtGEOkB8aRQfLygPBwY37y8ty3z0MvvPACJqZMxO7du9G6dWvk5uaiV69e2LBhA9LS0nDHHXfg7rvvRkZGhsvnTJ48GQ888AD+/vtv9OrVCw8//DDOnj2rWzvVMIEzidnbZhvdBPIDjCPSA+OItGIMkR4YR6TFK6+8gpY3t0RcXByuuuoqtGnTBiNGjEDLli3RrFkzvPrqq4iLi3NbURsyZAj69++Ppk2b4rXXXkNubi5SU1PLte2GT2JCREREREQmERYG5OYa9906ufHGG5EHqaKXm5uLSZMmYfXq1Thx4gSKiopw6dIltxW41q1bl25Xq1YNEREROHXqlG7tVMMEjoiIiIiIPGOxAD4wE6NW1apVQ16xlMA999xzWL9+Pd588000bdoUVatWRd++fVFQUODyOcHBwYp9i8WCkpKScmmzDRM4M1iyBI/N/Ruo/g3QsSNQWAgcOgQsWwbcdRdw663i/0xEREREROS1LVu2YMiQIejduzcAUZE7dOiQsY1yggmcGXzyCXp9fxj4/m7Hc9OmAfXqAXXrAhcuAAEBYoaedu2AoCDgyBEx4LN6deDSJaBhQ+Cpp4AaNcS/nly8KO674Qbg4EFxrHp1MVCU/M4XD3xhdBPIDzCOSCvGEOmBcURaxV0VV7rdrFkzrFixAnfffTcsFgtefvnlcq+klRUTODN46in8EXwaN/51WiRZgEjOYmOB48elH7l//1V/1i+/AEuWuP/Odu2AG28Ezp4F9u8X+6GhwBdfAC+8ALRuDVxzDRAcDERFiQqgWhWwpMT5OapwU7dMRXKnZKObQSbHOCKtGEOkB8YRaZWZm4l61esBAKZPn45hw4bhlltuQXR0NMaNG4ecnByDW6iOC3mr8MWFvPMK8xAWVBU4fRqoWlVU2qpVA86dA/btA06eFJW34mKR3G3bBhw4ADRpAvzwA3D99SKZ+uIL4MQJ/RtYty7Qo4dIEGvVEsndjh2iHTfdBPTpA6xZAyQmAuPHA5cvA9HR0v1Wq5Tk2f61I4CTpOotrzCP0y6TZowj0ooxRHpgHFUMMy7k7anikmIEBgRW2PfptZA3EzgVvpjA3bPkHqzqr9PCgKdPi3F01aoB+fnAVVcBv/0GVKkiErHnnwd27RLdLatXF8lWWJiowM2Zo08bAKBNGyApSXTzXLUKGDlSJKQffCC6eA4ZAtSsCTRtKip+ubmiO+hNN0mDZ8+fF4lfZKR+7fJjusYRVVqMI9KKMUR6YBxVDH9O4PZm7UWzqGYV9n16JXDsQlkZ1arleCwxUdr+9FPn9777LvDPPyLB+vhjkWA1bAj8/Tfw4YfAmTNAz54iyVq/Hti6VdxnsYgqm9xff4kfm5QUafvsWWD6dPU2BAcDzz0nunZ+/jkQEiKqeddeCyxeLL4nM1MkiNu3i4peQoLofhoaKhLBZs3YrZOIiIiITIcJHHmnalUxHg4AXn9dOt6vHzBlivLayZOlpO3SJaCgAHjtNamqFxAA7NwJ7NkjZtUsLhaVQUAkmQEBomuovcJC5XcXFEjjAK+/3vGe4GDg+++BO+4Q7QBEd89ly8R4wJAQoFEj0dbLl0WbiouBsWPZjZOIiIiIfAoTOCpftipXWJj4mTbN+bU5OSKZCgwUSZfVKqplX3whEqpHHhHXde0qEqt168R+UpKouP32m/OE7667pOQNAL77DnDXPfbaa8Wzi4uBw4eBxo1Fl9OtW4HOnZncEREREVGFYwJnEqMSRhndhPJnn1BZLKJaN2SI2K9fX4zXS0gQ+6tXAytXAm+9Ja7LyADmzhXJ1ssvA3/+KbqDfvGFWCoBEOP8WrUCfv/deTuqVxfXjxolxuNt2ya6hiYmAlu2iGuuuw54+GGgQQOReA4eLBJUm4ICkYT6WDfNShFHVO4YR6QVY4j0wDiqWP44bUbtarUr9Pv0+jNkCcEksvKyjG6C8e64Q0reAODOO8W4u+rVxX6DBsCkScC8eWL2zT59gOXLRUI3bx6wcKGYBGXNGjF2DwA6dRJdJW0++gjYvVskikePikrdmTPinC15A8QkLy+9BAwYADz+uPgERHXu5pvFWLtbbwU2bBAJpC3xXLRIesbrr4slGHbt8vzPIC8P+OYbURUsA8YR6YFxRFoxhkgPjKOKERgoZmksKCgwuCX6KyopqtDvy8vLAwAEBwdreg5noVThi7NQ7j+7X7HYIGl09KiY8GTwYCAmRkx2kpYGDBsmqmY7doiZLwH1CVjU7NkD/Pe/wObNrq87c0Y8MypK7N9/v5gI5n//E2PxJkyQEkKrVSSptnX5hg4F5s8HXn0VSPZ+7RvGEemBcURaMYZID4yjimG1WpGRkYHCwkLUq1cPAX40hCS/KB+hQaHl/j1WqxV5eXk4deoUatSogZiYGIdruIyARr6YwKVsTuFilRVtxAhRudu8Gfj1V1GNmz1bJFtavP++qNrZtGwpJk6R+/lnoFcv4IYbgI0bxbHjx4F69aRrLl8WlT4vMI5ID4wj0ooxRHpgHFWcgoICHDx4ECW2tXr9RPblbERWqbilqGrUqIG6devCojLEhssI+KHUY6lGN6Hyef994M03RRfNm28GnnlGHE9JEQlX+/aiGnbbbcDbb7t/3kMPiTF58uQNcEzeANH9EpCSN0CZvAHAjz+KbqXulJSI8YCtWzOOSBeMI9KKMUR6YBxVnJCQEDRr1szvulGO/GYkPrjrgwr5ruDg4NLuqFoxgSNyJjBQGl8nN368tP3MM6Kb43/+A9xzjzjWqZNI0h58EBg9WiyJcOutIpGSr7E3aJCYAOWrr8rWvp49xXi+Nm1Ed8z8fDE2LjISePRRsQ5f/fqiivf++6LLZQvZ/X/8ASxYALzyijQmkIiIiEhFQECA3y3knVWYZcrfiQkckVYWC3D33aKr5apVYnbKNm1E18hrrhGzUdosXSrWzANEktWhg1iM/KOPREXNE61bi4XTAdHNEhDr2u3aBZw4AQwfLmbjBIAjR0T3TwBIScGNzyUAnY6JxO6mm8TxDz8UE7S0bSuSVhurFTh/Xiyo3rKlmISlceMy/RERERERkT6YwJlEy9otjW4CudOxo/ixue46x2seeAAIDwf27xfLElgsQP/+Ym272nZT2T73nOiaaT/j5OOPA489pjz23XfS9ocfOm3ihDdTgS33i8lbbAoKRDIXGQl8+63oLgqIpRjsF2dPSxOLpVOlxvcRacUYIj0wjkgrs8YQJzFR4YuTmBy/cBz1qtdzfyGZ19q1UkUtMlJUv6xW4OJFMfvk8uXi3N9/i26ZKjMYKQwZIqpyrta8sxccLL7vv/8V4/vsPf00cO6cWGB94UKgWjUxhm/oUGDqVDEeUG7RIuDQIdF908fWxKOy4/uItGIMkR4YR6SVL8WQN/mH/8wD6ufW7l1rdBOovPXsCcyaJRYbX7ZMHLNYRMXuf/+TrmvSBKhbV/0Z8sXEb7wRuOsu79pQWCjWp1NL3gDRjoULgRUrxCLqANC7txhPd/vtYt/2b0InT4pxfhMmSElkbi6wZIkYr0emxfcRacUYIj0wjkgrs8YQEziTiI+ON7oJVBFGjAAuXAC6dVMer1dPjJNbtEhUvQAxQ+WgQcqlBOTdJ1u0EN0wBw3yrg0TJ3p23fr1omK4b590bMkSMY5u1SpRrbPZvl169kMPASNHAmfPikXT2QnAdPg+Iq0YQ6QHxhFpZdYYYgJnEtO2TDO6CVRRgpwMTX3kEWmBbwDo3FnMIimvuj30kFirbuRIMRtmWJi4pqAAOHoU98/tCQwcqP78d95RPkvOfgkDQFTp1tr9y9VDD4mELClJTNhi88cf4nP6dPE5b56YObNFC6mSJ2e1iqTv4kXgs8+A336Tzr37ruiqmZOj3lYqd3wfkVaMIdID44i0MmsMMYEjMrvcXGnbYhHj195/XzmjZHAwUL8+8qsEiS6Qt9yifMZXXwFPPgnMmKH+HWWduKR1a/H5xx+OyZ7N+vXi8/nnReJZUiIqeO3aAfHxYpKXm28WC5cDwFNPiRk7IyNdTthCRERE5I+YwBGZXVm6IH70kVjiwMa23t2DD0rHZs6Utlu1krbDwsQSCO40bAh8+aXY/usvaYIWe3/+CWRmikXTZ80SY+buvVecO3ZMuu6qq4C9e5X3jhghPi9ccN8eIiIiIj/ABI7I7JYuFdW2efM8v6dFCyA9XVS3brxRLDQOiETu44+BYcPEenI2jRpJ202bAi++KJZMGDlSVMYuX3acWGXsWLFuXK1ayuM9eij3//oL2LZN2rdfusDm0iXxffbWrhXVuLfe4ng6IiIi8ntcRkCFLy4jcCbvDKLDoo1uBvmqy5fF7JVueB1Hjz0G7NkDfPEFEH3lvjfeEJOj2Bs7VpwDRBVt/HggJETMrvntt+L4s8+KRcGHDlXeW6OGWDZBq4QEsSi5bRzhli2iajd4MJcx0BHfR6QVY4j0wDgirXwphriMgB+a9ccso5tAvsyD5A0oQxzNmiVmu4yKAv7v/8R6bmPGqF87YYKo6H3+OTB5skjeAKB9e+maQYOABg2kfVs3TnfJW3KyGMfnTmqq6JJpc8cdIlmcxf//6InvI9KKMUR6YByRVmaNIVbgVPhiBY7ItM6cEZOj3Hcf0LYtkJ0txrPVri0SummyGaD++EMkYI88onzG8uVibTz5zJbOfP21WP/uwgVA/v/f994DHn/c/f1nzwIZGWWfuIWIiIjIS6zA+aE+S/sY3QTyA4bEUXQ0kJIikjdAjFc7dkysAdeunXTd6dNiv0kTx2c0aCAqgE2bim6drvz7r5hlMy5OefzJJ8V4O0AsLG7bttexo2irbekDcsD3EWnFGCI9MI5IK7PGkJMFp8jXFJUUGd0E8gM+E0e2CU/uvltUy268URpjd9NNQM2aQJ06Ys26vXuBG24QE7XYZqEcPx7YsUN9Zss1a4CtW6X99u1FF9NNm4DvvxdLEIweLRZAP3RIOfnK998D//wjtletEu0iBz4TR2RajCHSA+OItDJrDDGBIyLjVK0qujzKhYcDBw+KMW/OFha/+mrx8957wBNPKM9t2KDcb9pUSuDGjhXrzAFAfr5YFNw26+XOnUC3btJ9hYWi+2e0B4Objx8HYmI4UQoRERGVO3ahJCLfExnpPHmTs+8mqaZpUykJsyVvNq+9JpZUyMlRTn4CiC6btWoBK1dKx06cENW/w4elY2vXAvXrA/36OT6fiIiISGdM4ExiYOuBRjeB/IDfxVH37mKGSmfuukvMQhkVpTxeu7a0vXs30LWr6K6pxraoOCDWx3vtNbE0gs3ixeJz2TJgxQqvmm9WfhdHVOEYQ6QHxhFpZdYYYgJnEmHBHlQjiNzwuziyWIBXX5X25UsWzJ4tumc2bKhM4Jo0EePg5H7/Xdq+5x7H76lfH5g/H/juO7G/e7d0Tj7e7qef1NtZUiK6WfoJv4sjqnCMIdID44i0MmsMMYEziZjqMUY3gfyA38bRu+8CjRsDn3wixrnFxwO9e0vn5ePYWrYUXSOdue02x2PHj4tKXvXqyuOHD4vxejbbtqk/8/nnRRL4zTfufxcT8Ns4ogrDGCI9MI5IK7PGEBM4k9iSscXoJpAf8Ns4GjUKOHBAjImbOhXYs0eZpMkrcE2binXonLnhBsculzY5OdJ2VhYweLDYjowUn2lpQHGx2D56VIyjO3sWmD5dHHvmGcdnnjwJPPSQmGTFJPw2jqjCMIZID4wj0sqsMcQEziTWH1hvdBPID1TaOLJP4AIDnV/bpInoKmlfbbN3551S0vXf/wLVqgF5eSJ5vHRJjJN78UWxZIFNVpbjc55+GliyBOjSxeNfx2iVNo5IN4wh0gPjiLQyawwxgSMi/yfvQtmokfPrVq8WXR3btQPOnQPWu3ix//abtH377dJC5S1bioRx506xL18m4fx5x+fs2KHct1qdfycRERFVekzgiMj/1awpbTdu7Pw6+cLggYEiMXvzTef3REaKKlu3biLps7l0Sf27AceJTmxdLgGxkHhUlKjIEREREalgAmcSsRGxRjeB/ECljaPAQGDOHGDaNKB5c8/vs1iAZ58F/v0X6NFDOj5qlJiY5PRpsaxAQICovKkJClLuDxig3C8qkrbvuENU/h56yPM2GqDSxhHphjFEemAckVZmjSGL1cr+OvZycnIQGRmJ7OxsREREGN0cAEBeYZ5ppzol38E4kpk9W4w/y8+Xjrl6HT79NPDOO86vO3VKTIBy7Jj7754zBxgyRCR+TZooZ7L0pC0GYxyRVowh0gPjiLTypRjyJv9gBc4kPvj9A6ObQH6AcSQzYgRw4QLw3nti/+GHXV+fkOD6fO3aYubJtWvdf/fw4cCCBcC336onbwCwaxdw8aL7ZxmAcURaMYZID4wj0sqsMRTk/hLyBd3juhvdBPIDjCM7wcHAY4+J8Wtt2ri+tn9/se5bhw6ur2vaVP14rVqie6Sty+TGjcDChc6f07KlWMtuxQrp2IYNYhKWkhIxdu6aa0Qlr0EDICXFdbt0xDgirRhDpAfGEWll1hhiBc4kxv8w3ugmkB9gHKkICADatweqVHF/3UsvAf/5j+vrGjcGbr5ZPFM+scnp08r15y5ccN+2deuk7b/+Arp2FQniNdcA114L/PgjsGgRMGWKSOoqCOOItGIMkR4YR6SVWWOICRwRkZ4CA4GtW4FffwVWrVKekydZtmUGXLl4UVo77s8/Hc/Ln1GtGpCaqjw/fbpIOD1JFomIiMgUmMAREZWXevWApUvF9rPPAgUF0rm9e8Vnjx7A/PlitsrwcMdn7N4tPtUmNTl0SNq+fFl0p5R79lnRVfPDD8vWfiIiIvI5TOCIiMrTAw8A+/YBU6cqZ7y0ue46YPBg4MgR4J9/HM937Ci6X54+7XjO/vrdu4HvvhPbhYXS8RMnyt5+IiIi8ilM4Ezi7R5vG90E8gOMI4PExYmulfYJXEiISN4AoG5dIDZWrCsXGwuMHCldt2yZegKn1g3zjjvE5/Hj0jGdu1AyjkgrxhDpgXFEWpk1hpjAmcSSnUuMbgL5AcaRwf73P+V+SgrQurXy2IsvAhkZ4tp+/cSxJ54A3njD8XnyJE0uL088w+bDD8V6c2lp0rE9e6TqoJcYR6QVY4j0wDgircwaQ1zIW4UvLuRNRH7AagXS00Xilp8PfPqpWMrAmR07HBM8T+zcKSY9GTBAebxLF+D770VFrl494NIloFMnYNMm77+DiIiIdMOFvP3Q0JVDjW4C+QHGkcEsFqB5c2DxYtEt0lXyBgAtWrhf3kDNgQPA5587Hs/PF4uI16wpkjdAmkzFRt7d8o03gBtvFOvPybiNowMHPJtlkyotvotID4wj0sqsMcQEziSy8rKMbgL5AcaRyQQGAvXrK48NHOj+vsWLHZcwAERytmCB8tiJE+J6AFi9GoiIAN6+MibgvfeAbdvE+nO5uaW3uI2juDigVStpCQQiO3wXkR4YR6SVWWOICRwRkS+zr9JNnQo884zre778UnxWr65MAJ1VxQYOBD75BLjrLrE/Zoz4zMyUrvn2W8/aW1QkbR844Nk9RERE5DEmcEREvuzaa6Xt1FQgJkZUyVyxLSFw553ASy959j324+U6dlTOmqlW0VNj65opbwcRERHphgmcSSTFJxndBPIDjCMTmjFDTD7y5ZfATTeJY9WrO78+LEzajo4WyxGozWDpzs8/K/dls1W6jCN5Avfjj8rFy4mu4LuI9MA4Iq3MGkNM4EwiPjre6CaQH2AcmVCDBiIRuvde6ZizCtynnwIPPijtR0eLiVOGD1e/PiTE83bI1qFzGUfyBC45WeqOSSTDdxHpgXFEWpk1hpjAmURRSZH7i4jcYBz5CWcJXP/+yjFv0dHis0YN6ViTJtL2o4+KWTE9sW8f8PXXANzE0eXLyv333vPs+VSp8F1EemAckVZmjSEmcCaRkZ3h/iIiNxhHfqJRI+fn1BI4iwXo21fMDimfxbJaNbGkgKffdc89wM6druNIXoEjcoLvItID44i0MmsMMYEzieX/LDe6CeQHGEd+on174L//Fdu9e4vPPn3Ep1oCB4h14f79F6hTRzoWHq6szqkpKVHu79njGEd//gkMGSJmrWQCRx7gu4j0wDgircwaQ0zgiIjMxmIBZs8Gjh8HVqwQ3RuXLhXnrr5aui4qSnlPQICoutmEh7sfBydfSgAQXSStVjGpyv794livXmJ9uQcfZAJHRERUzpjAERGZVUyM+IyLA4KCxLa8AhcZ6XiPPIGrVk25btuffzpef8MNyv3Dh9Fr/SFR8WvaFHjgAbEYOABs2qSewBW5GGOwahWwbp3z80RERKQQZHQDyDMRoW7WfSLyAOOoEoiOBjp0APLyxAyW9sLDldvy5KpNG6BWLTHj5JNPimeMGwdcc410TXIyHpM/b9ky5fNTUx2/8/RpKdmUy84Gkq5M4ZyXB1St6u63Iz/BdxHpgXFEWpk1hixWq9VqdCN8TU5ODiIjI5GdnY0IdwvmEhH5GqtV/ASodLL4+WexSDcArFwJvPMOsGGDdF9GBrB5s+gOaavqWSza2pOY6Liu3JEjwHPPibF5ALBnDxBvzumciYiItPIm/2AXSpNI2ZxidBPIDzCOKgnbeDc19l0omzVTnm/QABgwQEreAGDjRm3t2bJFVNhsTp4EWraUkjcAOHhQfGZni+/v3dtxAhXyG3wXkR4YR6SVWWOIXShNYlCbQUY3gfwA44gcJjFJSQEuXhSzSDrTubMYp9a9e9m/9/ffgbAwIDYWaNcOyMlRnj90SHy+8ALwySdie98+ZfdN8ht8F5EeGEeklVljiBU4kxi1ZpTRTSA/wDgihwpcVBSwcCFw222u74uLc33+xhvVjzdsKD67dAESEsRYuOPHHa+zVeD++EM6dvq06+8k0+K7iPTAOCKtzBpDTOCIiCoT+SQmwcGe39ekCbB6NbBmjeO5l18Grr3W8fjjjyvXnXPFlsAdOSIdO3XK8/YRERFVEkzgiIgqE3kFTm2ZAVd69QJ69HA8/sorjs968kngvfeA2rU9e3ZaGlBQIMbH2ci3iYiICAATOCKiyiUoCPj0U+Djj4G6db2/PyAAj864HejUSew/8oj4tJ8xyzZzpacVuH37gN9+Ux47dQq4cEHMiPnll+6fkZ4OtGoFfPaZZ99JRERkQkzgTGJSl0lGN4H8AOOIAAD9+wPDh5f59pEPTAO+/Rb45hvgf/8TB+0TONusk/IELiYGWLpUJH/vvef44AULlPsTJ4p2Ll0qFg4fPBi4fNl5w/77X2DnTvH7kU/ju4j0wDgircwaQ0zgTGJLxhajm0B+gHFEetiSsUUsun3nnWJmScCzBC46GnjgAWDTJqBxY8cHf/WV4zH5QuELF4plDn79VeyfPQts3y6d56QnpsF3EemBcURamTWGmMCZhFmnOSXfwjgiPajGkbMErn596Vh0tLRdtarjM7Ky3H/56dPAwIFAYaFIBtu1A777TpyzWt3fTz6B7yLSA+OItDJrDDGBM4mJGyca3QTyA4wj0oNqHNnPaFlYKD7btJGOySc6kSdw9tW4hATH548bByxeLLb37QO++ALYsEHsv/mm+FRb+PvYMfFDPoXvItID44i0MmsMMYEziQPnDhjdBPIDjCPSg2ocyatf1asD//d/YrtJE+n4mTPSdpUq0nbbtkCA7D9HQ4cC77yjfP5VVwEPPywW+gaARYukc7YEzb4Cd+mSeHbz5sDu3cq2qiV7VGH4LiI9MI5IK7PGEBM4IiLS7qqrpO1z54CWLcW2PDGTd5GUV+Bq1gSuuUbaj40VyxB066a8BhDj7gDlenTp6WIdOXlSlp8PpKaKLpe5ucDYsdK53r1FUudqQhQiIiIfxQSOiIi069oVeOopsTxBYKDy3PTpIpGzdXUElAlcaChw003Sfmys+GzUSDpmSxBvvtlxvF1Jiaj07d8vHVu5UrmcwK5d4tNqFef27gV+/tmrX5GIiMgXBBndAPJM54adjW4C+QHGEelBNY4sFmlJAXujRwOPPgqEh0vH5F0og4Olih3gOoELCgJat3affPXrp9w/cgQoLpbG5gGOiSZVGL6LSA+MI9LKrDHECpxJdI/rbnQTyA8wjkgPZYojefIGKCtwFotyspMaNcRnw4bSMVsXSkAkcN4qKgIyM8XC4DZM4AzDdxHpgXFEWpk1hpjAmcT+c/vdX0TkBuOI9KBLHMkrcFYr0L27mGlyzhyR0AHKBE4+xs5WoQOUk6S4k5GhTODy871rM+mG7yLSA+OItDJrDDGBIyKiihdk14PfYhEzVw4bJh2TryEnr8D17Cmd96Qa16CB+Dx8GMjJkY7b1qrzxKuviuUN5AkgERGRAZjAmcTctLlGN4H8AOOI9KB7HDlbgLtRI2DkSODpp8XSBDZt2gB//AH8/rtybTlnbrhBfNpX4LxJ4CZMEN8nnxiFyozvItID44i0MmsMcRITIiIylrMEzmIB3n9f/Vy7dp4/37ZQeFaWegVu924xU+b48cqJU2zk98i7fhIRERmACRwREZlXcbH7a2xVuh9/VFbsLl0Sn7fcApw/Dxw6BCxfLpYpsI3DA4B9+6TtAHZcISIiY/G/REREZCxnFThP2Cdw117reI0tafv9d1Fls7FV4M6fF5/ffy9mwHzhBeX9e/dK2xwDR0REBmMCZxKr+q8yugnkBxhHpAfd4ygsrOz3Pvmkcv/mmx2vcTZOLi9PLAJub9o05b68Apeb6137SBXfRaQHxhFpZdYYYgJnEimbU4xuAvkBxhHpQbc4eust4PrrgbFjy/6MDh3EhCY29eoBQ4cqr4mIUL83L09MbOLOsWPSNhM4XfBdRHpgHJFWZo0hJnAm8WTCk+4vInKDcUR60C2OxowB0tKAqChtz7nuOmk7NBSYO1e5PpyrCtz27e6ff/GitM0EThd8F5EeGEeklVljiAmcSQz8cqDRTSA/wDgiPfhcHIWGStu29eW++gr4z3+An392nsC99x5w333q52xdK7dvBzZtko4zgdOFz8UQmRLjiLQyawwxgSMiInOTzxgZHCw+W7UCfvgBSEx03oXSlfh44PhxsVzB4cPS8QsXREK3aJG2NhMREZUREzgiIvIfLVo4HvNksW97+/Y5zkYJiDXh+vQBBg0CDh5Unjt7FnjkEeCnn7z/PiIiIg8ZnsC99957aNSoEapUqYL27dsjNTXV5fXnz5/HE088gZiYGISGhuKaa67BmjVrSs9PmjQJFotF8dO8efPy/jWIiMhImzYBM2YAPXs6nrNP4Hr08OyZ33zjeGzHDpGoAWLdOLmvvwbmzAGmTPHs+URERGVg6ELeS5cuxZgxYzBr1iy0b98eM2bMQI8ePZCeno7atWs7XF9QUIBu3bqhdu3aWL58OerXr4/Dhw+jRo0aiuuuu+46fP/996X7QUHmX698TIcxRjeB/ADjiPTgk3HUqZP4UVOlirT96qti0pPvvnP/zHPnHI/Ju1OeOKE8Z1tPTj5rJanyyRgi02EckVZmjSFDM5vp06fj0UcfxdArUz7PmjULq1evxty5c/GCSteVuXPn4uzZs/jll18QfGWcQ6NGjRyuCwoKQt26dcu17RUtI9uDqa6J3GAckR5MF0fyMXJBQUDVqsrzNWsCly8Dly5599zMTOW+bYKT48e9b2MlY7oYIp/EOCKtzBpDhnWhLCgowLZt29C1a1epMQEB6Nq1K7Zu3ap6z6pVq9ChQwc88cQTqFOnDlq2bInXXnsNxcXFiuv27t2LevXqoUmTJnj44YeR4Wadn/z8fOTk5Ch+fE2XRl2MbgL5AcYR6cHUcdSypePC4YcPK2ey9NSJE0BhITB+PLBxo5TAnT0L5Odrbqo/M3UMkc9gHJFWZo0hwypwZ86cQXFxMerUqaM4XqdOHezZs0f1ngMHDuCHH37Aww8/jDVr1mDfvn14/PHHUVhYiIkTJwIA2rdvj/nz5yM+Ph4nTpzA5MmT0bFjR+zcuRPVq1dXfe7rr7+OyZMnOxzvt6wfgsNEpa9qcFUs7bsUKZtTMKztMDz2zWOKa8d3HI+0zDT0bdEXU3+eivSs9NJzibGJSGqehB0ndyAsOAyzt81W3PvFA19g6papGNNhDB5c/qDi3KiEUcjKy8IvR35BnfA6SD0mjRFsWbslRiWMwtq9axEfHY9pW6Yp7p2bNBez/piF5E7J6LO0D4pKikrPDWw9EGHBYYipHoMtGVuw/sD60nOxEbF4o/sb+OD3D9A9rjvG/zBe8dy3e7yNJTuXILlTMoauHIqsvKzSc0nxSYiPjkdRSREysjOw/J/lpeciQiOwuM9ipGxOwaA2gzBqzSjFcyd1mYQtGVswqM0gTNw4EQfOHSg917lhZ3SP64795/aL3y1truLeVf1XIWVzCp5MeNJhStgxHcYgIzsDXRp1wZztc5CWmVZ6rm3dthh+w3BsPLQRDSIbYPrW6Yp7F/VehHdT30Vyp2Tcs+QexblhbYcBAOJqxmHd/nXYdFiaarxJzSaY3GUyFv61EIkNEjFp4yTFvTN7zcTCvxYiuVMyBqwYgJx86R8N+rboiwaRDRAUEIT0M+lYmb6y9FxUWBTmJc1DyuYU9G/ZH6O/G6147pTbpmDd/nUYedNIPL/ueRzJOVJ6rluTbthxcgeSmichrzAPi/6WZtELCgjCin4rkLI5BY/d+BiGrRymeO7YxLFIP5OOns16YmbqTOw8tbP0XEL9BPRv2R+px1IRFRaFmakzFfd+1vczTN86HeMSx+G+z5VTto9oNwJ5hXloVacVVu5ZiS1HtpSei4+Kx7hbx2H5P8vRtm5bTPlJOa5o1l2zMDdtLpI7JaPf8n64VChVT/q37I+osChEhkYiLTMNa/ZK42RjwmMws9dMTN86HUnNkzB2vXIR6WndpmHlnpUY02EMRq0ZhRO5Uje5Xs16oW3dtsjOz0ZWXhaW7FxSes7od0RC/QQs2bmkQt4RJy6cwKahm8z1jvj5Z3y77HV0uP1WjH/3XtiiNCc8GNuztiGqTyJazV0Nb2zcugQ3p4SgymuvAa+9hm+6N8JdV849MvtO3NVdtF/tHdG8ytWYfOwafNrkItq27ekz74jEBok4ceFEub8j0s+kIz46HgDfETb+9I6oqL9HdFvYrTSOAP49wsYf3hFy5fmOGPzlYPw45EefeEcEFXqRllkNcuzYMSsA6y+//KI4/vzzz1sTEhJU72nWrJk1NjbWWlRUVHrsrbfestatW9fp95w7d84aERFh/fjjj51ec/nyZWt2dnbpz5EjR6wArNnZ2V7+VuXn7k/vNroJ5AcYR6QH08dRdrbVCoifWrXEsZISq/XLL6XjnvzcfrvV2qGDtD9kiLRt9982B08+Ka6Ljy/3X9cXmT6GyCcwjkgrX4qh7Oxsj/MPwypw0dHRCAwMxMmTJxXHT5486XT8WkxMDIKDgxEYGFh67Nprr0VmZiYKCgoQEhLicE+NGjVwzTXXYN++fU7bEhoaitCydJ8hIiLzka8LZ5t4xGIB7r3Xu+ecOAGcOSPty7fdjYP7+mvxmZ7u+joiIiI7ho2BCwkJQbt27bBhw4bSYyUlJdiwYQM6dOigek9iYiL27duHkpKS0mP//vsvYmJiVJM3AMjNzcX+/fsRExOj7y9ARETmV1jo+bVvvSU+mzQRnydOAKdOSef//Vfa7t8f+PRT58+ymz2ZiIjIU4auAzdmzBh89NFHWLBgAXbv3o2RI0fi4sWLpbNSDho0CC+++GLp9SNHjsTZs2fx9NNP499//8Xq1avx2muv4Yknnii95rnnnsOmTZtw6NAh/PLLL+jduzcCAwPRv3//Cv/99NS2blujm0B+gHFEeqi0cTR6tFgbbvFisW+/zIA8gSssBB57DLCbZKtUWRYX9yOVNoZIV4wj0sqsMWToMgL9+vXD6dOnMWHCBGRmZuL666/Ht99+WzqxSUZGBgICpBwzNjYW3333HUaPHo3WrVujfv36ePrppzFu3LjSa44ePYr+/fsjKysLtWrVwq233opff/0VtWrVqvDfT0/DbxhudBPIDzCOSA+VNo4sFuDOOz1f5+3CBWDnTqBNG8dz8gpccTEgGxpQGVTaGCJdMY5IK7PGkKEVOAAYNWoUDh8+jPz8fPz2229o37596bmNGzdi/vz5ius7dOiAX3/9FZcvX8b+/fvx0ksvKcbEffbZZzh+/Djy8/Nx9OhRfPbZZ4iLi6uoX6fcbDy00egmkB9gHJEe/CKO1qwRC3zb/TfGI/bLELiyZYv6cfmsyFlZ6tccOQJ88glQVKR+3sT8IobIcIwj0sqsMWR4AkeeaRDZwOgmkB9gHJEe/CKOevYUFbLBg5XHf/gBuP561/dWq+b59/zyi/pxeVImn8zr33/FunJZWaLL5oABwNq1nn+fSfhFDJHhGEeklVljiAmcSdivK0JUFowj0oPfxFGQyiiC//wHSEsDnntO7CckiE951S04WNnl0dV4NnkFLj0dqF8fePtt4PJl6XhmprTdvj3w2mvA008D+8V6VTghrSHkL/wmhshQjCPSyqwxxASOiIjI3htviATql1+A1auVE5RYLMqE7p57AGdL0Rw6JJKx334Dnn1WLC8wZowygZNX4GzLGmzeDJw+Lbbz8vT4jYiIyE8wgSMiIlLTpImotPXqJSpncvJulPXrA4mJzp/zzjvAzTdLCRmgXH7g7FnHe4KDpWsuXvS+7URE5LeYwBEREXlLXoGLjgYefFDaHzgQuO464OGHlfccOCBtb98ubT/9tNRl0yYvT1qjjhU4IiKSsVitVqvRjfA1OTk5iIyMRHZ2NiIiIoxuDgAg+3I2IqtU7nWDSDvGEemBcQSgVSuxRAAAzJsHPPSQSOIaNhRj3KxW4IMPANk6pW7JlxMIDQXy88X2008DM2Yor928GYiJAZo1E4lecLDmX6kiMYZID4wj0sqXYsib/IMVOJN4N/Vdo5tAfoBxRHpgHEHZhbJWLSAkBFixQiRvgBgn5242S3vyrpS25A0QFTirVaxBN2gQsG8f0LkzcM01YoKTOnXEouEmwhgiPTCOSCuzxhArcCp8sQJHREQ+5LbbgB9/FNu//w7ceKPjNbm5yvXe3Nm1S3S9tPfQQ8Drr4vqHgB8+qk4BoilBmxJI/9zTkRkWqzA+aF7ltxjdBPIDzCOSA+MIwBVq0rbdeqoXxMeLhYMV0vK1MiXE5DLyxPdK22OHJG2Dx+Wtk204DdjiPTAOCKtzBpDTOCIiIi8VVAgbdeu7fy6nj2BJ5/07Jk9e6ofv3hR2aXyq6+k7UOHpO3oaOU+ERH5JSZwRERE3rpwQdp2tgacTYMG0rarLpXypFAuL0+5btzWrdL2wYPSdna26FJJRER+jQkcERGRt3JzPb82NlbabtrU9bVLlogJUeQuXlQmcHLnzin309I8bxcREZkSEziTGNZ2mNFNID/AOCI9MI6grMC5I6/A1a3r+tpu3URXSDn7Cpwr8jFxPowxRHpgHJFWZo0hJnBERETe8jShAgD5bGL21TV7NWsCjRopjx096l3FTz5ejoiI/A4TOJOIqxlndBPIDzCOSA+MIwALF4pkbNYs7+676y5pOzAQ2LJFeT4gAGjSRHksLw+4+27Pv+PSJe/aZADGEOmBcURamTWGmMCZxLr964xuAvkBxhHpgXEEoEcP0Y1yxAjPrj90CFi7FrhHNmV1cDBwyy2O1zZurK1teXmuz2/dCsydq+07NGIMkR4YR6SVWWOICZxJbDq8yegmkB9gHJEeGEdXuOsOKdewIXDHHcrulM7WbatZU1u71BK4774DnnpKdK+85RZg+HDghx+0fY8GjCHSA+OItDJrDAUZ3QAiIqJKo0oVaTs8XP2amBjn99et63zBbxu1BO6OO8SnvHvmzz8DFgvQubPouklERKbANzYREVFFqlNHfL75pvi0jW/r1Ut89u4tumi+/LLze11x1YUyI0PanjgRuO024MMP3T+TiIh8BhM4k2hSs4n7i4jcYByRHhhHGq1aBSxbBgy7Mn31/PnABx8AixeL/dBQ4NtvgVdeEZOcPPywdK/WBC442PHYnDkeN10vjCHSA+OItDJrDFmsVqvV6Eb4mpycHERGRiI7OxsR8vEKBsq+nI3IKpFGN4NMjnFEemAcVbDly4H77xfbgwaJGTBd+fpr5WyXly8DVauK7eRkICVFeX2rVsDff+vXXg8whkgPjCPSypdiyJv8gxU4k1j4l5v/YBN5gHFEemAcVTD5WLkaNdxfb6vAbdwIDBkC7N0rnbNP3gCgoEBD48qGMUR6YByRVmaNIU5iYhKJDRKNbgL5AcYR6YFxVMGqV5e2bZU0V378EejbF/jPf8T+7t2urzdg4W/GEOmBcURamTWGWIEziUkbJxndBPIDjCPSA+OogskrcKGhYsycfFycvVmzgBkzpP3UVNfPNyCBYwyRHhhHpJVZY4gJHBERkS+TV+CqVAEGDwZmz3Z9zxtveP58AxI4IiIqOyZwREREvkxegbMtHi5fT06NJ10tbQwYA0dERGXHBI6IiMiXyStwxcXiMzBQec1rr4kZKm0OHvT8+a4SOKsVmDcPSEvz/HlERFSumMCZxMxeM41uAvkBxhHpgXFUweTVtsJC9WteeAGIjS3b810lcGvWiPXqbrihbM92gjFEemAckVZmjSEmcCZh1mlOybcwjkgPjKMKZrFI284SOIsFKCoq+3f83/8BZ886Ht++vezPdIExRHpgHJFWZo0hLuStwhcX8iYiokrMlsS99BIwZYryGCC6Oj7xBPD++2X/DvmzbSZNAiZPlr6DiIjKBRfy9kMDVgwwugnkBxhHpAfGkYHUqmy2/9BfuKDt2cePOx6zjbnTGWOI9MA4Iq3MGkNM4EwiJz/H6CaQH2AckR4YRwZ46CEgIAAYOdLxnC2Bq11b23fkqPzvWk4JHGOI9MA4Iq3MGkNM4IiIiHzd4sWiwtaokeM5WwL30ktAq1bS8cGDgUcflfZ37FB/dr164rM8E7itW4HkZK45R0SkAyZwREREvs5iAcLC1M/ZErirrgK++UY63rgx8PLLQFQU8MgjYl/NrFni05bAFRcDAweK8XDyLpsnTgDt20vXe+OWW8Tz3n7b+3uJiEghyOgGkGf6tuhrdBPIDzCOSA+MIx/xxBPAe+8Br78uHZMPfA8PF0sLnDolul86m6XSdo8tgfv6a1Hxs/fWW0Bqqvh57LGytXnXLgCMIdIH44i0MmsMsQJnEg0iGxjdBPIDjCPSA+PIR7z7LnDmDNCli3QsPFzaDg4WnwFX/lMf5OTfbO0TOGddLXNzpW21JQe8wBgiPTCOSCuzxhATOJMICmCxlLRjHJEeGEc+wmIR3SPl5Emas4TNni2By84WnwcOqF93/ry0/fvvnj3b3pVkkjFEemAckVZmjSEmcCaRfibd6CaQH2AckR4YRyZhn9w5Y0vgLl4U49/++Uf9Onli9+efZWvTlbXrGEOkB8YRaWXWGGICZxIr01ca3QTyA4wj0gPjyMdNnw707Qvcd59n18vHzV24AOzfr36dPIGzVevKiDFEemAckVZmjSEmcERERP5k9Ghg2TLPu1CGhoofQIypy8pSv05+/OJFx/PFxcDcucC//0rHUlPFpCg2VypwRERUdubs+ElERET6iYgATp8G9u717Pq8PMdjc+YAI0aIbatVfLZvr0/7iIioFCtwJhEV5uFYBiIXGEekB8aRH7IlZL16eXa9WgXu+++l7TNngMuXHa+5UoFjDJEeGEeklVljyGK12v6ZjGxycnIQGRmJ7OxsRMjHBhAREZmZWhdGq1WsF3f0qOfPSUoCvvpKbO/ZAzz/PLBtm1js22bjRuUSBwAwbJio1BERkYI3+QcrcCaRsjnF6CaQH2AckR4YR35kwADxKV8M3BPyClzfvsA33yiTN0CsU+eEVzG0ezfwn/+IhJBIhu8i0sqsMcQEziT6t+xvdBPIDzCOSA+MIz8QFQWkpwPz5on9AQOUi4A7ExkpPtPSgBYtgE8+EQmWmowMx2MlJQC8jKG+fUXy9p//eH4PVQp8F5FWZo0hJnAmMfq70UY3gfwA44j0wDjyA1YrcM01ypkqPUngWrQQn1lZInEbMMD5bJdqCVxBAQAvY+jYMc+vpUqF7yLSyqwxxASOiIioslEb/u5NAifnLIE7edLxWH6++++wFxjo/T1ERH6MCRwREVFl4y6Bu+cex2OAqNrZU1tSwJmyJHAB/KsKEZEc34pERESVjbsEbvhw4MgRIDtbSuYAoHFjbd/LChwRkWZM4Exiym1TjG4C+QHGEemBcWRi3bqJz0cfdTwnT+AiI4GrrxbVL3kXSZ0SuNIYmjMHeOEF9YTShgkcOcF3EWll1hhiAmcS6/avM7oJ5AcYR6QHxpGJffEFsHo1MEXlLy3yBK5GDWk7N1fabtBA2/dfSeBKY+iRR4CpU4Hff3d+DxM4coLvItLKrDHkZOQx+ZqRN400ugnkBxhHpAfGkYlVrw706qV+zr4CZ5OdrX68LK4kcCNvGgkUFkrH5evK2WMCR07wXURamTWGWIEziefXPW90E8gPMI5ID4wjPxUcLG3LK3Dnz0vbISHePbNlS+X+lQTu+XXPAzk50vEqVZw/gwkcOcF3EWll1hhiAmcSR3KOGN0E8gOMI9ID48hPFRdL29WrS9vyCpzF4vz+Bx9U7s+b59hVs6AAsFqReeaQMjGUf7c9zkJJTvBdRFqZNYb4ViQiIiJlEiWveskTLTl5xa5ePWDJEuX5IUOUiSAgKnBDhmDeqPXAxo3K486wAkdEpMAEjoiIiICiIvXj998vPm+5RXn8rrukbXmXS7mYGOV+fj6wcCEicgvFBCby484wgSMiUuAkJibRrUk3o5tAfoBxRHpgHPkpZ90YZ84EOnYEevcW+3v2APv2AceOAV9+KY45m9wkPl657yxRkx+/fFk5Jo4JHDnBdxFpZdYYYgXOJBIbJBrdBPIDjCPSA+PIT117rfrxiAixblx0tNiPjwfuvBOoVk26xlkFzn7MnLsEbv58MRvmF19I5+Rj4FytF0eVDt9FpJVZY4gJnEmcuHDC6CaQH2AckR4YR37quefEz88/e3Z9WJi07SyBA4AtW0QSCLhP4IYOFZXAvn2lc/IKXEGBZ22jSoHvItLKrDHELpQmkVeYZ3QTyA8wjkgPjCM/FRYGvPGG59fLK3Cu1oe75Rbg0CHgqqucX5OfDwwcKO3Lq27yBO7yZSA01PM2kl/ju4i0MmsMsQJnEov+XmR0E8gPMI5ID4wjAqDehbJmTfGZkKC8NizM9XIAZ88CixdL+7YxcC++CKSmSsddTXZClQ7fRaSVWWOICRwRERF5Ty2B27IFGDECWLZMeW1oKHDvvc6fdeaM4/VWK/B//6c8zgSOiIgJHBEREZWBWhfKa68FZs0CGjRwvH7QIOfPsk/ggoOBixcdr7t82ft2EhH5GSZwJhEUwOGKpB3jiPTAOCIAyklMXI2B8+Sa06eV+zk5jscAVuBIge8i0sqsMcQEziRW9FthdBPIDzCOSA+MIwKgrMDJkzlPrrdnX4G7fBk4fNjxuvffF9003c1GOWMGsMicY1vIc3wXkVZmjSEmcCaRsjnF6CaQH2AckR4YRwRAmZAFB3t3vb0jRxyP/fOP47EPPgBuvRW47jqgsFD9WQcOAKNHiy6bXDfOr/FdRFqZNYaYwJnEYzc+ZnQTyA8wjkgPjCMCoEzabLNGuuKqSnfiylpMt90GxMaKbbUEzmbfPlGJU5OTI21zzJxf47uItDJrDDGBM4lhK4cZ3QTyA4wj0gPjiEo9/DBw441Ax47ur3VVgbOpXl0aK/fee66vzXOyfpN8uYLcXPffWRaHDgFxccC775bP88kjfBeRVmaNISZwREREVDaLFwO//669C6VNeLhn1TzAeXVNPj6uvBK4558XXTWfeqp8nk9E5AITOCIiIip/Vau6v6Z6de0JnPy42lIEerh0qXyeS0TkASZwREREVP4sFvfXeJPAOUui5MfLqwJXUlI+zyUi8gATOJMYmzjW6CaQH2AckR4YR1RugoKcJ3D2CaAnFThbAnfhgva2yTGB8wl8F5FWZo0hJnAmkX4m3egmkB9gHJEeGEdUbi5edJ7A2Y+hmzsXOHbM8bqjR5XP+/prICICmDZNv3YygfMJfBeRVmaNISZwJtGzWU+jm0B+gHFEemAcUblxlcDZL0OwfbuYAVPunXeAxx+X9nNzgcGDxfa4cfq1kwmcT+C7iLQyawwxgTOJmakzjW4C+QHGEemBcUTlpl49zxM4AMjMFJ87dwJvvAE8/bTyfG5u+SRbTOB8At9FpJVZY4gJnEnsPLXT6CaQH2AckR4YR1QuhgwBnntOmcCFhEjbrpYhaNUKGKsyluXixfJJtqxW/Z9JXuO7iLQyawwxgSMiIiLjzZsnxqrJE7jwcGnbWQI3ZozzZ7ICR0R+KMjoBhAREVElNngw0KmTtG+fwJ09K7adrSP39tvOn11eFTgmcERkICZwJpFQP8HoJpAfYByRHhhHpKv585X7zipw8u6UnmIFzq/xXURamTWG2IXSJPq37G90E8gPMI5ID4wjKrO773Z/jRkSOI6B8wl8F5FWZo0hJnAmkXos1egmkB9gHJEeGEdUZp9+CqxciTOtmjq/Rs8Ezr4LZVGR989QwwqcT+C7iLQyawwxgTOJqLAoo5tAfoBxRHpgHFGZhYcD99yDgCpOxrMByrFu8gQuONj5PQ88oH48NxcoLpb2d+0CRo8GDh/2rL3OMIHzCXwXkVZmjSEmcCZh1nUqyLcwjkgPjCPS6kTOMecn5RU4+cyTripwV10FREc7Hs/KUu7feCMwY4a0uHdeHtC2rbT499mzyoTPGSZwPoHvItLKrDHEBI6IiIgqlMXVGDJ5Ale7trTtKoGLjATi4x2PH7NLFG1dKP/4Q3x+8QXw55/ABx8ABw8CdesCAwa4bDsAJnBEZCjOQklEREQVyuLqpDyBGz4c+OcfoEsX4MgR5/dERgL33ANs2aI8fuKE+vVXXw18+SWwZ490bMcOoLAQ+PtvN60HJzEhIkMxgSMiIqIKZXFVwJIncJGRwLp1YvuJJ5zfExkJPPookJ0NvPaa+wakpwN9+iiP2bpb5ue7v58VOCIyELtQmsRnfT8zugnkBxhHpAfGEWnV7CoXs1CGhkrb8mTOlchIMcnJlCllb9Tx4+KzoMD9tUzgfALfRaSVWWOICZxJTN863egmkB9gHJEeGEekVWbOcc8urOpitkq5yMiyN8bGlsCxAmcafBeRVmaNISZwJjEucZzRTSA/wDgiPTCOSKuYWk08u9CbCpzNunXAk0963ygmcKbDdxFpZdYYYgJnEvd9fp/RTSA/wDgiPTCOSKtR9wQCjRoBH3/s+kJna7/VqgXUqCHtV68ubXfrBrz9tveNYhdK0+G7iLQyawxxEhMiIiKqUEfrVxfT9qtJSACuvRZo3Nj5A44cAYKCgDffBPbuBVq3Vp4PDATCwsQ6b56yzVjprAKXnS3O1a6tTOCsVsDicl5NIiJdMYEjIiIi3xEcDOzc6Topsk10Ms5F96dq1cqWwJWUiMW8AwOlc1YrUK+eeF5urjKBKy4WySQRUQVhF0oiIiLyLQEB2qta4eHur5k9G+jZU2zbFvkGHKtwp09LyeDBg44JHBFRBWICZxIj2o0wugnkBxhHpAfGEWlVITEkT+AiItSvqVZNOX7Oxj6BO3RI2r54UbmQNxM4w/BdRFqZNYYMT+Dee+89NGrUCFWqVEH79u2Rmprq8vrz58/jiSeeQExMDEJDQ3HNNddgzZo1mp5pBnmFXnQDIXKCcUR6YByRVhUSQ9WqSdsxMerXhIerJ3AFBSJJO3lS7MvH6509ywqcj+C7iLQyawwZmsAtXboUY8aMwcSJE7F9+3a0adMGPXr0wKlTp1SvLygoQLdu3XDo0CEsX74c6enp+Oijj1C/fv0yP9MsWtVpZXQTyA8wjkgPjCPSqkJiSF6Bq1vX+TVqXS3z84Hx48V9S5YwgfNRfBeRVmaNIUMTuOnTp+PRRx/F0KFD0aJFC8yaNQthYWGYO3eu6vVz587F2bNn8dVXXyExMRGNGjVC586d0aZNmzI/0yxW7llpdBPIDzCOSA+MI9KqQmJInpiVpQL3+utie9QoJnA+iu8i0sqsMWRYAldQUIBt27aha9euUmMCAtC1a1ds3bpV9Z5Vq1ahQ4cOeOKJJ1CnTh20bNkSr732GoqvvDzL8kwAyM/PR05OjuLH12w5ssXoJpAfYByRHhhHpFWZYki+WLcn5OvEeZvAycfAFRWJZQtssrKUSRsTOMPwXURamTWGDJv39syZMyguLkadOnUUx+vUqYM9e/ao3nPgwAH88MMPePjhh7FmzRrs27cPjz/+OAoLCzFx4sQyPRMAXn/9dUyePNnheL9l/RAcJhYRrRpcFUv7LkXK5hQMazsMj33zmOLa8R3HIy0zDX1b9MXUn6ciPSu99FxibCKSmidhx8kdCAsOw+xtsxX3fvHAF5i6ZSrGdBiDB5c/qDg3KmEUsvKycLHgIlI2pyD1mDSer2XtlhiVMApr965FfHQ8pm2Zprh3btJczPpjFpI7JaPP0j4oKpFm2BrYeiDCgsMQUz0GWzK2YP2B9aXnYiNi8Ub3N/DB7x+ge1x3jP9hvOK5b/d4G0t2LkFyp2QMXTkUWXlZpeeS4pMQHx2PopIiZGRnYPk/y0vPRYRGYHGfxUjZnIJBbQZh1JpRiudO6jIJWzK2YFCbQZi4cSIOnDtQeq5zw87oHtcd+8/tF79bmrKiuqr/KqRsTsGTCU9i4JcDFefGdBiDjOwMdGnUBXO2z0FaZlrpubZ122L4DcOx8dBGNIhsgOlbpyvuXdR7Ed5NfRfJnZJxz5J7FOeGtR0GAIirGYd1+9dh0+FNpeea1GyCyV0mY+FfC5HYIBGTNk5S3Duz10ws/GshkjslY8CKAcjJl/7RoG+LvmgQ2QBBAUFIP5OOlenSvw5FhUVhXtI8pGxOQf+W/TH6u9GK5065bQrW7V+HkTeNxPPrnseRHOkvHd2adMP5y+ex+t/VyCvMw6K/F5WeCwoIwop+K5CyOQWP3fgYhq0cpnju2MSxSD+Tjp7NemJm6kzsPLWz9FxC/QT0b9kfqcdSERUWhZmpMxX3ftb3M0zfOh3jEsc5LJg5ot0I5BXmoVWdVli5Z6XiRRofFY9xt47D8n+Wo23dtpjy0xTFvbPumoW5aXOR3CkZ/Zb3w6XCS6Xn+rfsj6iwKESGRiItMw1r9krjZGPCYzCz10xM3zodSc2TMHb9WMVzp3WbhpV7VmJMhzEYtWYUTuSeKD3Xq1kvtK3bFtn52cjKy8KSnUtKzxn9jkion4AlO5dUyDtix8kdyCvM4zsC/veOSGyQiBMXTpT7OyL1WGrpn5en74hvO4bgtpVR2HxLfaxbco/bd8SiZtfDlvJ9cnoDHoajtAt7ceFsGjrZHS/MuwjbEuKX8y/i4L5fca3tpN3fFwYvH4BzNasA4DvCpqL+HiGPI4DvCBt/eEfIleffI/7N+hcAfOLvEUGFXqRlVoMcO3bMCsD6yy+/KI4///zz1oSEBNV7mjVrZo2NjbUWFRWVHnvrrbesdevWLfMzrVar9fLly9bs7OzSnyNHjlgBWLOzs8v66+nu7k/vNroJ5AcYR6QHxhFpVSEx9P33VquYisRqXbhQ2pb/nDljtS5a5Hh861ZpOyTEar3uOvX7Aav1yBGr9c8/rdbkZKv1woXy/72oFN9FpJUvxVB2drbH+YdhFbjo6GgEBgbipG2GpytOnjyJuk4GG8fExCA4OBiBssU1r732WmRmZqKgoKBMzwSA0NBQhNoWBfVR8VHxRjeB/ADjiPTAOCKtKiSGZOPjFTNSylWr5nwSE5uiIsDV0IriYuD666Xt117zvI0lJcCDDwLXXQdMnOj5fQSA7yLSzqwxZNgYuJCQELRr1w4bNmwoPVZSUoINGzagQ4cOqvckJiZi3759KJENHv73338RExODkJCQMj3TLMbdOs7oJpAfYByRHhhHpFWFxFB0NBB05d+pb7pJ/ZrQUPUxcMnJ0nZJifsEzubvv71r46ZNwLJlwKRJ3t1HAPguIu3MGkOGzkI5ZswYfPTRR1iwYAF2796NkSNH4uLFixg6dCgAYNCgQXjxxRdLrx85ciTOnj2Lp59+Gv/++y9Wr16N1157DU888YTHzzQreR9worJiHJEeGEekVYXF0PHjQHo6EBsLTJ0KyP5OgZAQwGJRT+B+/lm572kC52zBcGcuX1Z/DnmE7yLSyqwxZFgXSgDo168fTp8+jQkTJiAzMxPXX389vv3229JJSDIyMhAQIOWYsbGx+O677zB69Gi0bt0a9evXx9NPP41x48Z5/Eyzalu3rdFNID/AOCI9MI5IqwqLoVq1xA8AjL0y0YBteYC4OPGplsDZs1qdnysokLbtn2W1AiNHii6STz7peK9sSAhyc72fabOS47uItDJrDFmsVldvpcopJycHkZGRyM7ORoS3/5pWTu5Zcg9W9V9ldDPI5BhHpAfGEWllaAxZLOLzrruAr78WSwQ0aFD2523YANx+u9gePRp4803A9o/PP/4I3Hab2Fb769aaNcCdd4rtI0eAq68uezsqIb6LSCtfiiFv8g9Du1ASERERGaLTlcUDPKnAuXLsmLT9xReiirb8SrcsedfLlSuB+fOV9+blSdsXLmhrBxFVGoZ2oSQiIiKqUN98A6xbBzzzjNhXm4VSTbVqwMWLjsflCVxGhvi8/37Hitu994rPrCxg+HCx0DgTOCIqA1bgiIiIqPK4807gf/8Dgq8s1R0UBFSt6v6+mBj148ePe/f9zz0H9OsntuUJoauJUoiIZJjAmcSsu2YZ3QTyA4wj0gPjiLTyuRjypBtl7drK/UaNxOeJE95/37p14pMVOE18Lo7IdMwaQ0zgTGJu2lyjm0B+gHFEemAckVY+F0OedKO0nyHSNoPk6dNl/155BY4JnNd8Lo7IdMwaQ7okcMXFxfjzzz9x7tw5PR5HKpI7Jbu/iMgNxhHpgXFEWvlcDKlV4EJDlfv2s8J5ksC5m+ibCZwmPhdHZDpmjaEyJXDPPPMM5syZA0Akb507d8YNN9yA2NhYbNy4Uc/20RX9lvczugnkBxhHpAfGEWnlczGklsCdPy9N8Q84T+BOnVJ/ptXqfnFudqHUxOfiiEzHrDFUpgRu+fLlaNOmDQDg66+/xsGDB7Fnzx6MHj0a48eP17WBJFwqvGR0E8gPMI5ID4wj0srnYig+XnzedZf4fOUVoEoVMfOkjbME7swZ9Wfm5SkX+VbDCpwmPhdHZDpmjaEyJXBnzpxB3bp1AQBr1qzB/fffj2uuuQbDhg3Djh07dG0gERERUbl6913g77+BVauAQ4eA5CvdqjxJ4Jw5e9Z9AievwHEWSiLyUJkSuDp16uCff/5BcXExvv32W3Tr1g0AkJeXh0B3LzQiIiIiX1K1KtCqFWCxAA0bik8ACAuTrvE2gcvKAgoLXV/DChwRlUGZFvIeOnQoHnjgAcTExMBisaBr164AgN9++w3NmzfXtYEk9G/Z3+gmkB9gHJEeGEeklWliSEsF7swZ7ypwaouEk0umiSPyWWaNoTIlcJMmTULLli1x5MgR3H///Qi9MlNTYGAgXnjhBV0bSEJUWJTRTSA/wDgiPTCOSCvTxJCWBO6++4Dbb3d9jTxps014YrUCw4aJBcY/+sjztto7e1Z0Cb3vPs/WuTMh08QR+SyzxlCZEjgA6Nu3LwDg8uXLpccGDx6svUWkKjI00v1FRG4wjkgPjCPSyjQxpCWBy8kBvvxS/VzAlREs8gSuqEh8Hj0KzJ8vtt96y/F7PXXffcDGjcCPPwILFpTtGT7ONHFEPsusMVSmMXDFxcV49dVXUb9+fYSHh+PAgQMAgJdffrl0eQHSV1pmmtFNID/AOCI9MI5IK9PEkDcJXHCw588NuvLv5/IulLYKXGamdEzLxCa2ZZ0WLy77M3ycaeKIfJZZY6hMCdyUKVMwf/58TJs2DSEhIaXHW7ZsiY8//li3xpFkzd41RjeB/ADjiPTAOCKtTBND9pOY2HoaPfig8rqPPgKuzM7tEVsCd0k2hbm8AmfjLoHbtElU6VwtGC77e5q/MU0ckc8yawyVKYFbuHAhPvzwQzz88MOKWSfbtGmDPXv26NY4IiIiIsPYV+Def1+MK/v4YyA3Vzo3eDAQE+P5c20JnGwYCoqLgf37gT59pGPZ2a6f06UL8NxzwFdfOb/myjwFROQ/ypTAHTt2DE2bNnU4XlJSgkJ3U+YSERERmYG8m2RkpKjI3X23SOzkCVxwsDKBc1eNy8kBevYEzp2TjhUVAaNGOV7nCVdr8PpxBY6osipTAteiRQv89NNPDseXL1+Otm3bam4UOYoJ9+Jf9oicYByRHhhHpJVpYsg2Lg1QVuMAx3Xb5Enbnj3Ayy+7fva33zp+19mzymPuKnA28rF09vw4gTNNHJHPMmsMlWkWygkTJmDw4ME4duwYSkpKsGLFCqSnp2PhwoX45ptv9G4jAZjZa6bRTSA/wDgiPTCOSCvTxJA8gQuw+zdveQUOAAYMAGbPBmJjRbWuTh3vvuvnnx2PeVqBsyVw//4LPP+8Mnn04wTONHFEPsusMVSmClxSUhK+/vprfP/996hWrRomTJiA3bt34+uvv0a3bt30biMBmL51utFNID/AOCI9MI5IK9PE0B13AFFRQI8ejufsE7hbbwV++w3Ytk3s21fsysLTCpxtOYL4eDFGT94V048TONPEEfkss8ZQmdeB69ixI9avX69nW8iFpOZJRjeB/ADjiPTAOCKtTBNDERHAsWPqSZDazI8JCdJ2eHjZv7dRI+DQIVGBW7IEqFcP6NzZ+fV5ecCpU9K+fHIUP57ExDRxRD7LrDFUpgrckSNHcFQ2zW1qaiqeeeYZfPjhh7o1jJTGrh9rdBPIDzCOSA+MI9LKVDEUGgpYLN7fp1aBsx/35kzDhuLz11+Bhx4Ss026kpcHpKdL+/XqSdvuFhw3MVPFEfkks8ZQmRK4hx56CD/++CMAIDMzE127dkVqairGjx+PV155RdcGEhEREZmOWgLn6TATWwL311/Or5FXAPPylBOZyGe3LCjw7DuJyDTKlMDt3LkTCVe6CXz++edo1aoVfvnlF3zyySeYP3++nu0jIiIi8l3OKlxqCVxAgFhywJ1GjcSnPBGzZ1v4GxDJm3xRcPl98u6UROQXypTAFRYWIvRKn+rvv/8e99xzDwCgefPmOHHihH6tIyIiIvJFNWuKz//8R/28s0lMPJlUxJbAyatn8hkxAUC+7u4vvwCffSbty5cjkCd2rrBSR2QaZUrgrrvuOsyaNQs//fQT1q9fjzvuuAMAcPz4cURFRenaQBKmdZtmdBPIDzCOSA+MI9LKL2JoyxZg9Ghg8WL1884SOE8mFYmOdjwmT9jU9pculbZPn5a2PanAvfkmULUqoLLGry/zizgiQ5k1hsqUwE2dOhWzZ89Gly5d0L9/f7Rp0wYAsGrVqtKulaSvlXtWGt0E8gOMI9ID44i08osYuvZaYPp05+u9aUngIiMdj9lXyOwTOGc8SeCefx4oKQEeecSzZ/oIv4gjMpRZY8hitarNg+tecXExcnJyUNPWhQDAoUOHEBYWhtq1a+vWQCPk5OQgMjIS2dnZiIiIMLo5AIDC4kIEB3rQb57IBcYR6YFxRFpVihjKzweqVFEes1pF98jDh53fN3AgMGYM0Lat8viZM2JNuu3bgXXrgIcfBho0cN8Oi0V0v3Q1k6bt3DXXKGez9HGVIo6oXPlSDHmTf5SpAnfp0iXk5+eXJm+HDx/GjBkzkJ6ebvrkzVeNWjPK/UVEbjCOSA+MI9KqUsSQs7Fu7ipwCxa4rsC1awe8+CIwY4Zn7bBaPa/WmUyliCMqV2aNoTIlcElJSVi4cCEA4Pz582jfvj3eeust3Hvvvfjggw90bSAJJ3I5OQxpxzgiPTCOSKtKEUPOKl6uJjGpVk3cp/av7/ZJ2KZNnrdlwwbPrivLencGqhRxROXKrDFUpgRu+/bt6NixIwBg+fLlqFOnDg4fPoyFCxfinXfe0bWBRERERKYWGChNEGLfrVLOtsSAWgJnPwbuwgXPv79XL8+uM1kCR1RZlSmBy8vLQ/Xq1QEA69atQ58+fRAQEICbb74Zh1316yYiIiKqLGbPBgYPFuPhbr1VHLvpJufXBwWJz+BgMSukXGGhmGjEJjfXu7Z4upyA3PTpYpkEb7+LiMpVmRK4pk2b4quvvsKRI0fw3XffoXv37gCAU6dO+cykH/6mVzMP//WMyAXGEemBcURaVZoY+u9/gfnzlYt9v/CC8+ttCRzgOA6uoAA4f17a9zapyspyf419Be7ZZ4GNG4FFi7z7rgpSaeKIyo1ZY6hMCdyECRPw3HPPoVGjRkhISECHDh0AiGpcW/tZk0gXbevyz5W0YxyRHhhHpFWljqEGDYADB9TPyRM4+38QLyxULtCdkyM+1SY8AYAAu7/inTnjeM2ffwLz5kn78gRO3mWzbBOWl7tKHUekC7PGUJD7Sxz17dsXt956K06cOFG6BhwA3H777ejdu7dujSNJdn620U0gP8A4Ij0wjkirSh9DjRurH3dXgVOrotWoAWSr/HkGBSmTMLUEztU/umdkSNuuJl4xUKWPI9LMrDFUpgocANStWxdt27bF8ePHcfToUQBAQkICmjdvrlvjSJKV50HXByI3GEekB8YRacUYckJeNbOvwBUUKCtwNmFhysTPJjBQLNBt420XykOHpG1btc+ZvXuByZOVXTwrAOOItDJrDJUpgSspKcErr7yCyMhINGzYEA0bNkSNGjXw6quvokQ+wJZ0s2TnEqObQH6AcUR6YByRVowhD9hX4AoL1ZOw4GCRxNkLDASmTQP69BH7ahU4V+QJnFqFT+7664FJk4BnnvHuOzRiHJFWZo2hMiVw48ePx8yZM/F///d/SEtLQ1paGl577TW8++67ePnll/VuIxEREZF/eest8blggXRMXgGzr8AdPgz8+qvjc4KDxfpx9mxVueho8elJBU7OmwQuL098bt3q3XcQUZmUaQzcggUL8PHHH+Oee+4pPda6dWvUr18fjz/+OKZMmaJbA4mIiIj8zpgxYpbKwECx1ACgTOBq11Ze/+ij6s9xlsDZZr6MihKfziZOkZN/v7y7prsEzsbVGndEpJsyVeDOnj2rOtatefPmOKvWP5s0qxpc1f1FRG4wjkgPjCPSijF0RXi4cvyaPIGKjXV+X5Mm0rarLpSAVIFbsEBaTNwZ+ffbqmqA5wmc/dp15YxxRFqZNYbKlMC1adMGM2fOdDg+c+ZMtG7dWnOjyNHSvkuNbgL5AcYR6YFxRFoxhmTka8TJuUrgZDOAqy76DUiJYceO0rGNGz1vl3zhb08TuNBQz5+vA8YRaWXWGCpTAjdt2jTMnTsXLVq0wPDhwzF8+HC0aNEC8+fPx5tvvql3GwlAyuYUo5tAfoBxRHpgHJFWjCEZ+cyT8gpYgwbO72nWTNoOCgK2b3e8xpYY3nSTNBvl8uVAXBywcqX62m5aK3AV3IWScURamTWGypTAde7cGf/++y969+6N8+fP4/z58+jTpw927dqFRYsW6d1GAjCs7TCjm0B+gHFEemAckVaMISc87UIpT+CCg9WvkVf2rrlGfP79txgLd++9QHGx4z2FhdJ2WSpwFdyFknFEWpk1hsq8Dly9evUwZcoUfPHFF/jiiy+QkpKCc+fOYc6cOXq2j6547JvHjG4C+QHGEemBcURaMYackCdwtslH1NgncMuWAbfcAixcKB2Xj627+mrHZ2zY4HhMvvC3pxU4+fJRFVyBYxyRVmaNoTIncERERERUTiwW5wmRPIErLgaSkoAtW4D4eOm4vAKnlsDdcYfjMXkCJ6/AuVrI++JFabuCK3BElRUTOCIiIiJfIK/AAcCRI8CNNzpeV7eutC1PtORJm7sETk1+vvpz8/PVx8wBwIUL0nZQmVanIiIvMYEjIiIi8gX2CVx0NHDDDY7XySc+kSda8uPyZCoyUn2pAXvOulAC6mPmAGUCV1Tk/juISDOv/qmkT58+Ls+fP39eS1vIhfEdxxvdBPIDjCPSA+OItGIMOWGfwAHKSUpGjAAeekh53pMKnMUCREQ4JmX2nHWhBMQEJ2oVttxc5TUViHFEWpk1hryqwEVGRrr8adiwIQYNGlReba3U0jLTjG4C+QHGEemBcURaMYa8IE/EXnkF6NRJed6TBA7wrAIn70Jpn+w5S84MrMAxjkgrs8aQVxW4efPmlVc7yI2+Lfoa3QTyA4wj0gPjiLRiDDmhVoGTJ2iRka7Py5M2+2pZtWruv7+4WPwEBKhX4KxWkeTJJ1eRJ3AVXIFjHJFWZo0hjoEziak/TzW6CeQHGEekB8YRacUYckItgZNXwkJDHc/rWYEDRBKmNmnJ6dNA06YiEZw7Vzoun6GyghM4xhFpZdYYYgJnEulZ6UY3gfwA44j0wDgirRhDXnA3bk3vBC4/37H6BgCrV4tFwEtKgE2bpON//SVtV3AXSsYRaWXWGGICR0REROQL1Cpw8nXW1Fy+LG07m4US8KwLJSAmMrElcIGBUuInX/hbPtnJxo3StrwC52zWSiLSjAkcERERkS9w14VSbs4cICQE+PJL6ZgeFbiCAuk7q1aVZsFUS+AuXAC2bZOOFxWJrpddugDh4cD06Z59JxF5hQmcSSTGJhrdBPIDjCPSA+OItGIMOaGWwMlnhpQbNkxM4d+rl3RM7y6U8gROXnWzbf/5p+hSaVNYKH42bRKVwRkzPPvOMmIckVZmjSEmcCaR1DzJ6CaQH2AckR4YR6QVY8gL770nqllvv+14Tr5GHKB9FkpAWYELC3P8DkBU3i5fFgmc3JYtyjFx9hOh6IxxRFqZNYaYwJnEjpM7jG4C+QHGEemBcURaMYacUKvA3XQTcP488Mwz7u+XJ3ABdn/F86YLpVoFTm7TJiAmBvjtN7F/443SuYQEaVtenSsHjCPSyqwxxATOJMKCPXzxErnAOCI9MI5IK8aQE2oJHODYHdIZV9d504XSNnGKswQOEEnlp5+KbXkCJyefYKUcMI5IK7PGEBM4k5i9bbbRTSA/wDgiPTCOSCvGkBPOEjhPyatu9s8KCfHsGQkJwIIFYrtWLecJHCB1kWzSRP282nIEalJTgYceAjIyPLv+CsYRaWXWGGICR0REROQP5BU4+wROfu7uu10/Z9ky8RkT4zqBs4mOVj9+6ZJn4+DatweWLAEeftj9tUTEBI6IiIjIJ2itwLnqQil/tnzpAVfq1vUsgatVy/k5Z7Noqtm92/NriSoxJnBERERE/sBVAifvXunpmDqtFTjA826UQLlPekLkL5jAmcQXD3xhdBPIDzCOSA+MI9KKMeSEnhU4V10oPeVJBa5qVddLFOza5fn3eZnAMY5IK7PGEBM4k5i6ZarRTSA/wDgiPTCOSCvGkJ1hw8Tn5MnanmO/dICn55zxpAJXo4brazp2BLZt8+z7vFw3jnFEWpk1hpjAmcSYDmOMbgL5AcYR6YFxRFoxhux8/DFw6hTQs6e258irbvYVuKuv9v559glcaKjjNZGRjouG25szx7PvKy72vG1gHJF2Zo0hJnAm8eDyB41uAvkBxhHpgXFEWjGG7FgsricCKesz5Xr3BsaOBb7wostYvXrKBC4y0vGaoiL3Vbrz5z37Pi+7UDKOSCuzxpCbfzIhIiIiItMLCACmetldrFo1ZXIWESEqhXI5Oe4rcJ4mcFYrkJsrxs0lJGgfE0jkp1iBIyIiIiKl+vXFp7sKXE6O+wrcuXOefWdxMdClC3DzzcDixZ7dQ1QJMYEjIiIi8jfuqlcNGyr3q1RR7tvGzNlX4Oxdvuw+gTtxwvV5m5ISacKTefM8u4eoEmICZxKjEkYZ3QTyA4wj0gPjiLRiDPmAH39UTmwSHq48P3Kk+HRXgXvkEfddKI8e9axNXs5CyTgircwaQ0zgTCIrL8voJpAfYByRHhhHpBVjqAK4q8A1bgxMny7tyxO4J54ABg0S2/IErnp1abtdO+Dzz4EZM9xX4IqLPZth0stJTBhHpJVZY4gJnEkk1E8wugnkBxhHpAfGEWnFGKoAnkwAIl/cW56c9e4t3S9PzqpWlbYjIoD77xcTnbirwAFAYaH7a7zkNo7sJ1whsmPWdxETOJNYsnOJ0U0gP8A4Ij0wjkgrxpCPkCde1apJ2/LxcPIETn5cniDKE0FnvE3gPEhAXcbRe+8BdeoAKSnefS9VKmZ9FzGBM4nUY6lGN4H8AOOI9MA4Iq0YQz5CnsDJu1DKF+x2VoFztWi4mqIi79vnhss4GnVlbNPLL+v+veQ/zPouYgJHRERE5G+0dKF0lsDJK3ABXv4Vshy6UBJVVkzgiIiIiCojZ10oPUngvE0QmcAR6YYJnEm0rN3S6CaQH2AckR4YR6QVY6gCeJtgeduF0l0Fbt484OJFKelr0gT480/3bfIC44i0MmsMMYEzCbOuU0G+hXFEemAckVaMoQrgSQLnbAycs0lMGjSQtu0TuHHjgNtuA44dA1avFssQhIZK9xcUAP36Obbhvffct9MJxhFpZdYYYgJnEmv3rjW6CeQHGEekB8YRacUY8hHyBC4kRNp2VoFr2lTatk8Q/+//gA0bgHr1gF69pARPfn9OjvKeo0elyUbsebCoN+OItDJrDDGBM4n46Hijm0B+gHFEemAckVaMIR8h70IpT7TkCdyFC9J2kybStqeTmMifa7UCeXnAX3+Jbfmzy4BxRFqZNYaYwJnEtC3TjG4C+QHGEemBcURaMYYqgLdj4JwlcIcPS9vyiU48eT6grPJZrUC3bsD11wPffOP6GSdPAq++CmRmOr3EZRx52j6q1Mz6LmICR0RERFTZyRMteXVNPh4OAK6+Wnzed59nz7WvwP3yi9hevNj12nD//ANMmAA0a+ZRd0oH3i5zQGQiQe4vISIiIiK/U1wsbQc5+SvhhAmiCvfYY2I/LU383H67Z98hT+BOn5a269QBLl92f39uLrB9O9CunWffZ8MKHPkxJnBERERE/saTBKakRNqWT2IiFxMDrFkj7UdHi26QnpIncHKLFwONG3v2jGPHvE/giPwY68smMTdprtFNID/AOCI9MI5IK8ZQBYiOdn+NPIGTLyOgJ2cJ3LlzwJgxnj3DSaXOZRyVtQvlX3+JJRGys8t2P5mKWd9FTOBMYtYfs4xuAvkBxhHpgXFEWjGGytGSJUDPnsDEie6vlSdX/fsDiYnAyy/r2x5nXTO94SSBcxlHZe1Cef31wLRpwHPPle1+MhWzvossVmtZRob6t5ycHERGRiI7OxsRERFGN4eIiIhIf1YrcP/9QMOGwFtvlc93dOgA/PqrtmfMng3897/e3VO1qpT4efNXXVvid/31YqwfUQXxJv9gBc4k+iztY3QTyA8wjkgPjCPSijHkIywWYPny8kveAOddKL3hpALnMo60zkLJ+kalYNZ3ERM4kygqcTHVLpGHGEekB8YRacUYqkT0SOAuXVI97DKOtM5CyQSuUjDru4gJHBERERGVD28SOGfj5S5fBubPB554QjnxiitM4MiPcRkBIiIiIiof3kxiEhysvrj35cvA0KFiu1MnoF8/989iAkd+jBU4kxjYeqDRTSA/wDgiPTCOSCvGUCXiTQXO2bXyMXDp6aWbLuOoIhO4EyeAN98Ezp7V9p1U4cz6LmIFziTCgsOMbgL5AcYR6YFxRFoxhioRLQlcQIDoMilP4E6fLt10GUcVOYlJ9+7Azp3ATz8BK1dq+16qUGZ9F7ECZxIx1WOMbgL5AcYR6YFxRFoxhiqxGjWcn7NP4G6+WXzm5krHZAmcyziqyArczp3ic80abd9JFc6s7yKfSODee+89NGrUCFWqVEH79u2Rmprq9Nr58+fDYrEofqpUqaK4ZsiQIQ7X3HHHHeX9a5SrLRlbjG4C+QHGEemBcURaMYYqEftJR1wlcPLFx9u0Ae69V2yfOycdP3WqdNNlHBmxjEBoqLbvpApn1neR4Qnc0qVLMWbMGEycOBHbt29HmzZt0KNHD5yS/R/UXkREBE6cOFH6c/jwYYdr7rjjDsU1S5YsKc9fo9ytP7De6CaQH2AckR4YR6QVY6gSsU+EXCVwI0YAu3YBhYViEe3ISHFcPrbsxx+B4cOBnBzXcaS1AufpbJdyISHavpMqnFnfRYaPgZs+fToeffRRDL0yu9CsWbOwevVqzJ07Fy+88ILqPRaLBXXr1nX53NDQULfXEBEREVE58iaBs1iAFi2kfVsPK/vJQebOFcndTbq0UB0rcOTDDK3AFRQUYNu2bejatWvpsYCAAHTt2hVbt251el9ubi4aNmyI2NhYJCUlYdeuXQ7XbNy4EbVr10Z8fDxGjhyJrKwsp8/Lz89HTk6O4oeIiIiIdFa9uufX2hI4eRdKm7ffRv1jF6T9Tz4BnnoKWL8eeOgh4MwZbe1kAkc+zNAK3JkzZ1BcXIw6deoojtepUwd79uxRvSc+Ph5z585F69atkZ2djTfffBO33HILdu3ahauvvhqA6D7Zp08fNG7cGPv378dLL72Enj17YuvWrQgMDHR45uuvv47Jkyc7HO+3rB+Cw8SA2qrBVbG071KkbE7BsLbD8Ng3jymuHd9xPNIy09C3RV9M/Xkq0rOkaW4TYxOR1DwJO07uQFhwGGZvm62494sHvsDULVMxpsMYPLj8QcW5UQmjkJWXhYiQCKRsTkHqMWl8YMvaLTEqYRTW7l2L+Oh4TNsyTXHv3KS5mPXHLCR3SkafpX0Uq80PbD0QYcFhiKkegy0ZWxQl5NiIWLzR/Q188PsH6B7XHeN/GK947ts93saSnUuQ3CkZQ1cORVaelBwnxSchPjoeRSVFyMjOwPJ/lpeeiwiNwOI+i5GyOQWD2gzCqDWjFM+d1GUStmRswaA2gzBx40QcOHeg9Fznhp3RPa479p/bL363tLmKe1f1X4WUzSl4MuFJDPxSOSXsmA5jkJGdgS6NumDO9jlIy0wrPde2blsMv2E4Nh7aiAaRDTB963TFvYt6L8K7qe8iuVMy7llyj+LcsLbDAABxNeOwbv86bDq8qfRck5pNMLnLZCz8ayESGyRi0sZJintn9pqJhX8tRHKnZAxYMQA5+dI/GvRt0RcNIhsgKCAI6WfSsTJdmtEqKiwK85LmIWVzCvq37I/R341WPHfKbVOwbv86jLxpJJ5f9zyO5BwpPdetSTdUCayC1f+uRl5hHhb9vaj0XFBAEFb0W4GUzSl47MbHMGzlMMVzxyaORfqZdPRs1hMzU2di56mdpecS6iegf8v+SD2WiqiwKMxMnam497O+n2H61ukYlzgO931+n+LciHYjkFeYh1Z1WmHlnpXYckTqix4fFY9xt47D8n+Wo23dtpjy0xTFvbPumoW5aXOR3CkZ/Zb3w6XCS6Xn+rfsj6iwKESGRiItMw1r9koDy2PCYzCz10xM3zodSc2TMHb9WMVzp3WbhpV7VmJMhzEYtWYUTuSeKD3Xq1kvtK3bFtn52cjKy8KSnVLXbKPfEQn1E7Bk55IKeUccPHcQeYV5fEfA/94RiQ0SceLCiXJ/R+w4uaP0z4vvCMGf3hHyv0c8cjkbkbJn/nR6GzpC3T1L7lG8I0oyt+I2AIVZp6E2l+WTb2/BPfVFHK0a8LU4+O67qs/19B2RfGX/xIXjGHElRt29I2x/g8wLKMGDdu8BviMEX/17xMnckwDgE++IoELP0zKL1WrcSoXHjx9H/fr18csvv6BDhw6lx8eOHYtNmzbht99+c/uMwsJCXHvttejfvz9effVV1WsOHDiAuLg4fP/997j99tsdzufn5yM/P790PycnB7GxscjOzkZEREQZfjP95RXmmXaqU/IdjCPSA+OItGIMVSJJScCqVdL+4MHAggWO1/3vf6KCJrd+vZiiXy4sDMjLk/atVvHjatIS+V9133sPeOUV4NtvgbZtpYXDbQuO28bONWkC7N/v+nezsd3TqhXw99+e3UM+wZfeRTk5OYiMjPQo/zC0C2V0dDQCAwNx8uRJxfGTJ096PH4tODgYbdu2xb59+5xe06RJE0RHRzu9JjQ0FBEREYofX/PB7x8Y3QTyA4wj0gPjiLRiDFUi9nUCu5nDAQBz5jgmb2rXtm4NXLwI3Hij8vmXLsFjo0aJmSwTE4HiYuC664Drry/bpCX2OImJ6Zj1XWRoAhcSEoJ27dphw4YNpcdKSkqwYcMGRUXOleLiYuzYsQMxMc7XcTh69CiysrJcXuPrusd1d38RkRuMI9ID44i0YgxVIvYJXNWqjteoJXVqx233bt4sHcvJUR8j56oNgEj6hg8H/v1XzHx54YL7e9zhGDjTMeu7yPBlBMaMGYOPPvoICxYswO7duzFy5EhcvHixdFbKQYMG4cUXXyy9/pVXXsG6detw4MABbN++HQMGDMDhw4fxyCOPABATnDz//PP49ddfcejQIWzYsAFJSUlo2rQpevToYcjvqAf7MSZEZcE4Ij0wjkgrxlAl4kkC5yzxsU/gwsKkZ9iek5XlOEulveJiabtVK2lb3pWzsFB5HRO4SsGs7yLDlxHo168fTp8+jQkTJiAzMxPXX389vv3229KJTTIyMhAg69d87tw5PProo8jMzETNmjXRrl07/PLLL2hxZdrZwMBA/P3331iwYAHOnz+PevXqoXv37nj11VcRyv9jEREREVUc+0SoSRPHazxN4OTJX1QUcPSoSODkY+LUdOoE/Pe/wJAhzq+9fFkkcc7a7UyRNLELEziqKIYncAAwatQojBo1SvXcxo0bFftvv/023n77bafPqlq1Kr777js9m0dEREREZSEfW/bqq46TkgDOu1DaV+vk+9HRIoE7c0YkX65s3Sp+hgwBcnPVr8nPL1sCJx9/xwSOKojhXSiJiIiIyE/JE6HkZPUkp1o19Xvtj4fJZguMihKfWVnux8DZHDnieQLn6aQm8opekE/URagSYAJnEm/3cF51JPIU44j0wDgirRhDlYh9JUstgQsPV7+3Zk2gQQNp374LJeDZGDibjRvFLJZqLl9WdocsSwKnx0yWVKHM+i5iAmcS8oX+iMqKcUR6YByRVoyhSsSTBM5ZBQ4Abr1V2nZWgfM0gdu50/m56dPFmnU28mqcK/IETj4JCpmCWd9FTOBMIrlTstFNID/AOCI9MI5IK8ZQJdKrl/isUUN8qq2V5qwCBygTOPlYuXr1xOfBg8oulF26OH+Wq66Wn3wC/PqrtF9Q4PxaOSZwpmbWdxETOJMYunKo0U0gP8A4Ij0wjkgrxlAlMmoU8NlnUvUrMFA5lT/gugLXt6+0ffKktN22rfjctk2qwL3zDjBihPNneVqpA5jAVRJmfRcxgTOJrLwso5tAfoBxRHpgHJFWjKFKJCgI6NcPqF9fOpaWBjRrJu2rrQ1nU6sW8NZbQHAwMHCgdLxdO/G5Zw9w+LDYrlnT+YyWQPkncN9+C7iYKZ18j1nfRUzgiIiIiKjiBAYqJ/wIcPPX0TFjxOQj//mPdKxuXZy+qooYY/fbb+LYVVe5TuBsXSg9mS2yuNizipp8GQFbW4nKGRM4IiIiIqpYnla4bIKDHQ6dibKr3LlL4GwVuIgIz76zenXg889dX6O2Bp184pY//lCOrSPSARM4k0iKT3J/EZEbjCPSA+OItGIMkdcJnIrIyDrKA+66UNoqcJGRnn3BpUui+6cragmcbTmCggLgppuADh2ACxc8+06qUGZ9FzGBM4n46Hijm0B+gHFEemAckVaMIdIjgasSXkN5wF0FzpZEeZrAeUItgcvPF5/y8XHZ2fp9J+nGrO8iJnAmUVRS5P4iIjcYR6QHxhFpxRii0iRHg+JQu26V7ipwNp52oVSTmQl8+qmUgLpK4FyN89u9Wyx5sHFj2dtCmpn1XcQEziQysjOMbgL5AcYR6YFxRFoxhkiPCtyFANlfviMixOQkniRw8gXBvXXrrcDDDwOvvSb21RK45GRR7XO1GHjv3sCmTcqJWajCmfVdxATOJJb/s9zoJpAfYByRHhhHpBVjiErHiWmw/9JRaadmTfHpSQLnyTVOv3S/+FyxQnyqJXCzZgHPPKNM4OxntMwwZ+Lgb8z6LmICR0RERESmUxAcKO1cdZX4dLWmnI0n13hKLYEDgNWrlVVG+wRO3r2SyEtM4IiIiIjIdAqDZX+NrVFDfHpSXauIBO7cOWUCZ19xZAJHGjCBM4mIUA0DbomuYByRHhhHpBVjiJCQID5bty77M+SJmG1ikpAQ7+7zhKsJV5wlcAUFyi6UTOB8klnfRUzgTGJxn8VGN4H8AOOI9MA4Iq0YQ4QVK8RkH2vWlPkR917/oLQTHi4+LRbVRb8VvE3g1JYAsFjEp7MEDmAXShMw67uICZxJpGxOMboJ5AcYR6QHxhFpxRgi1K8PvPqq+Cyj74//LO3YEjjAfQLn7SQmtgROnnR5ksC5qsBZrd61gcqFWd9FTOBMYlCbQUY3gfwA44j0wDgirRhDpIcbGt8i7VSvLm2760ZZlgTusceA2FjHc64SuNxcaVuHWTdJf2Z9FzGBM4lRa0YZ3QTyA4wj0gPjiLRiDJEePtm7QtpxVoFr0MDxxtBQqYLmifPngdmzgePHHc+5SuDkSwXYd6Ekn2DWdxETOCIiIiIynQL5LJTyCpw8gdu9G9i8WXljSAgQGAinmjdX7quNgbNxlcCdPi1tswJHOmICR0RERESmUxAi+2usvAIn70IZHAyEhSlvdJfAde4M/PADcN11Yl8tgbONYXOVwJ09K21XZAK3d69j0kp+hQkcEREREZlOoXwhb2ddKAMCHCc1CQkRx50JCQH+8x+gVSuxf+6c4zV5eeLT0wSuIrtQtm0rktDffqu476QKxQTOJCZ1mWR0E8gPMI5ID4wj0ooxRHrof+NQacfZJCbOEjh5BW7nTqBlS2k/NFR82hYHP3HC8cs9SeAOHpS2K7ICd/Gi+Pz444r7TpMy67uICZxJbMnYYnQTyA8wjkgPjCPSijFEevgzO13acdaF0mJxnJXSPoGLiADuvVfatyVwkZHiU23yEluS5CqB27BB2jZiEpONGyv+O03GrO8iJnAmYdZpTsm3MI5ID4wj0ooxRHq4o+W90o6rdeDcVeACA5XX2BI+VwmcJxU4OSMmMdm3r+K/02TM+i5iAmcSEzdONLoJ5AcYR6QHxhFpxRgiPcz8W9ZF0NU6cO4SuKAg8WPjSQWusFD85Od71tj8fPWxdM788QfQrBnw1Vee30NeM+u7iAmcSRw4d8DoJpAfYByRHhhHpBVjiPRw4JIssXJVgVPrQikXGOh9AgeIKpynFbi+fYGrrhLj4mwzWNrY7wNA796igta7t2fPd/Uscsqs7yImcERERERkOtYA2WLc3nahlCc6QUGuu1DaxrvZy80FCgq8a/SyZaJyJyfvXrl1K9CvH3D0qHfPtbF/NvmlIPeXEBERERH5lothsqTL2SQmgPuKnLsKnDPr1nnWULkqVRyrdgUFUhtvucX7Z8oxgasUmMCZROeGnY1uAvkBxhHpgXFEWjGGSA83tugKbHxBJGTyBMyTLpT2FbiyJHApKd43umpVxwQuPx9ITwd+/93759mTJ3AWi/PrCIB530VM4Eyie1x3o5tAfoBxRHpgHJFWjCHSQ/e47kCdVo4n1CpsFouUtKmdd9WF0uGLu4vq24EyjJ9yVoFr1877Z6lhAucVs76LOAbOJPaf2290E8gPMI5ID4wj0ooxRHpwGkf2FTj7Y/YVOG+7UDZo4F1D5SwW9QROL/IErqREv+f6KbO+i5jAEREREZH/UEvg5FU3+wQuIEA9gbOv1Nk0bKjcDwvzvG2FhcClS8pj5ZXAAUzi/BQTOJOYmzbX6CaQH2AckR4YR6QVY4j04DSO1BIvVxU4QJnA2e4PcjLSqHZt5XdUqSJtx8UBs2Y5b/S6dUD79spjniZw5887nxHTxtUMl+TArO8iJnBERERE5D/UKnBqCZqze2wVOPli33IREUDNmurPu/9+4JFHgEcfVb/3s88cF/92l8DNnQukporvrFNH/ZqPPwbuugvYu1d5vLjY9bPJlJjAEREREZH/cNb10SY42LMKXICTvya7SuACAkTi56oKBwDz50tdMe+/3/W1w4dLVbuLF9WTsgkTgNWrgV69lMeZwPklJnBERERE5D/UEjj5hCTuulDaKnAWi3oSV726MoGzXQ9I1ztL/gDgoYeAwYNFl0gA2LfP+bVq7MfQAcCJE+rXMoHzS0zgTGJV/1VGN4H8AOOI9MA4Iq0YQ6QHp3Gk1oWycWNpWy2BU+tCCah3o3RVgfNk6v6qVcVndrb7a9WoJXDVqqlfywTOJbO+i5jAmUTK5jIsFklkh3FEemAckVaMIdKD0zi64w7xKU++mjSRtgMDPetCaf8MG1cVOE8SOPmkJ2WRl+d4zNlkJZzExCWzvouYwJnEkwlPGt0E8gOMI9ID44i0YgyRHpzG0S23AFu3AseOScfkFTg1al0o7Y/buBsD546tAldWahU4Z5U2VuBcMuu7iAmcSQz8cqDRTSA/wDgiPTCOSCvGEOnBZRzdfLNyxsbrr1eet6/AyRMvd10o7StwZe1CWVb2FTirVXsCZ7UCQ4YAjz3m+Gfjx8z6LmICR0RERET+rXt34NlngQ8+EPv2SYo80XHXhTI0FAgPV+7byBPBzp3V22JL4L7/3n271dhX4IqKnCdd9gncn3+KJQ6OH1ceP3gQWLAAmD0bSE8vW7uowjCBIyIiIiL/ZrEAb74pKkyAY8JTUiJtu6rAPfOM+JRPGuKsArd+PdCvn2NbbGPgbr8deOopj5qvkJcH7N8v1n37+Wfg8mXn19qPgWvbVqwZN3So8vj+/dL2mjXet4kqFBM4IiIiIqrc5AmcfEZKeQL30UfA22+LbWcJnLwCFxwMxMU5fpe8C+VVV3nf1kuXxFIEq1cDHTuqj4mzcdaFctcu5f6BA9L2Dz943yaqUEzgTGJMhzFGN4H8AOOI9MA4Iq0YQ6QHTXHkqgulvIomT+DkE5rIEzh5xa56deVz1SZBkSdwUVHu22ovL0+ZcLmqwDlL4OyXWpBX4Mq6vIEJmfVdxATOJDKyM4xuAvkBxhHpgXFEWjGGSA+6xlHDhurH5QmcfDssTNoOCQGmTwe6dQOGD1fer5bAyZcRKGsFTk6PBE6eEBYUADNmAN98o7xm5Upg7VrHZ2Vni+pkVpbzdvgos76LVKKKfFGXRl2MbgL5AcYR6YFxRFoxhkgPmuLIvgIXHw8sWwbExCiPyxMweQJn34Vy9GjxY09tEhR5Ba52bc/bbGM/C6XeFbjUVPEDSH9OWVnAvfeK7fx8ZbfRwYNFcvf552Lcn4mY9V3ECpxJzNk+x+gmkB9gHJEeGEekFWOI9KApjtRmbezbF0hMVB7ztgulPXddKOPjHc+7W4rgo4+AM2ekfW8mMbGRJ3BWq7ICp0berbKwUHlu5UrxWdZZNQ1k1ncREziTSMtMM7oJ5AcYR6QHxhFpxRgiPWiKI0/XOnPWhdLZJCb23HWhvPpqx/PyZ6v56y/lvtYKXFYWkJPj+jvlk7P40eLgZn0XMYEjIiIiIlLjSQLnqgLnrgulWrXNXQJnryyzUMoTOFfVN9v98nY6q+pVFKsVGDYMePVVY9thICZwRERERFS5lKUCJ6+myScxUUvSXJ2TJ3Bq5M/2hNYKnG38W82ajtfl5zses+9CaeNt4llW27YB8+YBEyZUzPf5ICZwJtG2blujm0B+gHFEemAckVaMIdJDhcSRJxU4V10K1RJF+wRuyxblvreJkCdj4KxWYMQI6bhaBe7aax3vtyVw8t/RWQUuPNx9W/VQUCBty9fvKwOzvouYwJnE8BuGu7+IyA3GEemBcURaMYZID5riyNMKnLNZKOVJmLOKFKBewZKPgQOAW24BvvpK2vc2gVu61Pk5W+K1axfw4YfScXkCd+6c+KxXz/F+tQTO2e8bHq45ofKIvO1qf75eMOu7iAmcSWw8tNHoJpAfYByRHhhHpBVjiPSgKY60dqGUjwlzlcDJq0U2al0o5ROheNuF8ssvnZ+zJV5HjyqPy5MgW/vVKmi26p4nFbiTJ4FatYBJk1w2VzP5/w6uxv95wKzvIiZwJtEgsoHRTSA/wDgiPTCOSCvGEOlBUxxpnYVSztWkHp4mcPKJUPQcS2ZLvOwnKpEncLb2qyVw7ipw8t8vNxc4exaYPLns7fWEPHl21X3UA2Z9FzGBM4npW6cb3QTyA4wj0gPjiLRiDJEeKiSOyiOBU5u10l0F7qOPgLffdv49ztgSr337lMflv4stIVNLHN2NgZOvD1dR5N+vMYEz67tIZXEKIiIiIiI/prULpZy3CZza0gHyiphaIhUeDtSt6/x7nLG1zT6BU6uouepCKf8d5RW48+e9b5NW8rZo7EJpVkzgiIiIiIjUeFKBkydf9uQJ3NatQGSk++9US+CCg90vP6DGlqjt2aM8rpaQqVX+3FXgjEjg5G3RWIEzK3ahJCIiIiJSI6+62Vfg3n4baN7c9Xpk8qrWzTerT9VvTy2RCgnxfnITQCQ7ubmOFTi1BC44WNmVE3A/Bi43130bNm4E6tcHVq70uNkusQLHBM4sFvVeZHQTyA8wjkgPjCPSijFEeqiQOHJVgXvmGWD3bvXp922efx647Tbg449df4+8S6daouauAjdgADBwoOPx4mJgxw7HLqNqCVxQkOMyAO5moVTrIgoov69nT+D4ceDee5233525c4HEROD0aV3HwJn1XcQEziTeTX3X6CaQH2AckR4YR6QVY4j0UCFx5EkXSldq1gQ2bACGu1lvTL42nLNJTlxV4OrXV+/KWVQE/Pmn+nEbeQXOfjyfuwqcsyUU5JUxPbo5Dh8O/PKLWKLAmwTOagVmzgQ2bVI9bdZ3EcfAmURyp2Sjm0B+gHFEemAckVaMIdJDhcSRJ5OY6KFNG2DIEOeJmLsKXLVq6uPRiovVE7gffgCeegp45x0pIVL7XlsCZ5/wFRSI651V4C5elBLOgAD9FvjOyvKuC+X69cCTT4ptlYlrzPouYgXOJO5Zco/RTSA/wDgiPTCOSCvGEOlBUxzddJP4rFPH9XVaK3CesliAefOAlBT1RNFdBS4sTJmA2cayOUvgAODdd0VSI6/A2VPrQnn+vOg2evvtzitwFy9K23r+uZWUeFeB273b5WmzvouYwBERERFR5bJihahA/fST6+sqKoGTC1D567m7Cpx9AlejhvgsKBBj4ADgyy8d7zt92nUCp9aF8rvvRCXsxx9dV+Bs9PxzKy5WtsVdBc5ZgmlyTOCIiIiIqHK5+mrgf/8DmjVzfV1FdaGUU1snLiREOU7OXrVqygSsZk3xuXu3SHKqVQPuvhv4+mvlfRkZ3idw8u6QFV2BKy72rgLHBI6IiIiIqBKRJ20VVYFTExysXpmzsa/A2RK4bdvEZ+vWov32E6SUJYGTb+flqbfHV7pQOqsQmhwTOJMY1naY0U0gP8A4Ij0wjkgrxhDpoULiyIgulM4qcK7Yj4+zdaG0JXDXXy8+7auIGzYolxGwpzYGTl6Bu3BBvT3yBM5V4ukt+wqcuy6UbhI4s76LmMAREREREamRJ1NGd6F0pVo1qVoGSAmc7ZizBO7994G//hLbns5CKU/gcnLU2zNnjjS+0D7x/esvYMkS9fvUyGeP9LYLpZ9W4LiMgEnE1YwzugnkBxhHpAfGEWnFGCI9VHgcGd2F0pWwMGU1yr6rpLMEzt13qHWhlCdNzhK4r74SP1ar45+brS21agFduzpvj408CSsp8W4SEzcJnFnfRazAmcS6/euMbgL5AcYR6YFxRFoxhkgPFR5HRiZwtoqa3IMPStthYcrESp6o1a0rxsDZH7fn6TIC8u6RzrpQ2qglcDa//ur6Xvs2AN6PgXMziYlZ30VM4Exi02H1FeSJvME4Ij0wjkgrxhDpoULiSN59z6gulCEhjl0or7kGGD1a2q9WTZnMyJOm//s/aQZLPSpw8olLnFXgbC5dUrZF/ueZm+v6Xhv571VYqOsYOLO+i5jAERERERG5Y9QkJosWOV5TXAxcd520X62aMpmRt7VePWlb7wqcuwTuwgVlW+Tj9NxV7+zbYPvuslbg5GP3TI4JHBERERGRGnkyZUQXyr//Bh54wPF4SYlI2j79FJg1S4wnc1aBk3e/dJfAvfSS2O7SRXyeOSM+y1qBs0/g5ElmWRK4vLyyT2Ii/x1MjgmcSTSp2cToJpAfYByRHhhHpBVjiPRQIXEk7/Kn53T4rsiTxlat1K+xJSP9+wMjRohtW5fKe+9VJmqRkdK2qwQuKAhISQEyM4EXXxTHjh0Tn/KkyZsxcBcuKCtf8uTP/t7iYuCDD8Ti43KuKnDukjI3CZxZ30VM4ExicpfJRjeB/ADjiPTAOCKtGEOkhwqPI7Xp/SuaLbF65x3Hc127AkePAsuXK9vqrAJnX1EMDhb31akD1K8vjtkSuLJW4HJzld0mXVXgZs8GHn8caNFCedw+gZO3pahIJGnyRFtO3oVSnvhdYdZ3ERM4k1j410Kjm0B+gHFEemAckVaMIdJDhcSRs8SgPLlKFF97DTh7FkhKUj9fv75IzORVMmcVuKuuUt4rHwN39dXi89w5kbBpGQMnr4K5SuB+/ln9Ga66UJ48KX6/Pn3U73VTgTPru4gJnEkkNkg0ugnkBxhHpAfGEWnFGCI9+G0cVa3q+nzNmu6fIZ/hUb4mnKcJXESEGGMHiCqcfdXLRl6NU3PhgrICJ2/XuXPKa51N+S9P4C5dUiZlf/4pzn/1lfq9agncrFliYpeBA00bQ0zgTGLSxklGN4H8AOOI9MA4Iq0YQ6QHv42jBx4Qk4hMnFj2Zzibol+ewNkngvIEzmKRqnBHj5Z9AhD7Cpy8Yrd3L/D229K+syn/7ScqcVf1k1NL4CZMAE6cABYvxqQfNfwZG4gJHBERERGRrwgNBX78EZg0qezP8CSBk3etBByXEbAlcEeOlD2Be/RRZRKVna08P1k2Bs1ZBc5+rTd3VT85efJn+x1kk6oEFhvQRVYHTOCIiIiIiPyJs9khvUng4uPF548/ul8w21P//qvcv/56kdTNmiWqYmrsK3DyMXjuyO+1df2U/RmEFJpzbbgKWlKeiIiIiIgqhB4VuNatxef8+d5997JlwOrV6vdt3arcDwsTVbply5w/zz6Bc/a7qZEnnrYKnGz2zaBCc64NxwqcSczsNdPoJpAfYByRHhhHpBVjiPRQIXFkxCyUenDW5VGewEVEKM/Zr3PXpk3ZvrtKFefdHH/9VblfWOg6eQO0VeDUEjhZF8ppHV/x/Fk+hAmcSZh1mlPyLYwj0gPjiLRiDJEeGEcuLFgA1KrlWAWTr/1mX4Gz17Jl2b47KAg4f1793Nmzyn2Vtdkc6J3AycbaffXXUs+f5UOYwJlEcqdko5tAfoBxRHpgHJFWjCHSA+PIhZtvFmukDR6sPC6vslWv7voZ4eFA//7ef3dQEPDqq8pjVauqf58nCZx8GQLAuwRObRZK2Xc+0Xq458/yIUzgTGLAigFGN4H8AOOI9MA4Iq0YQ6QHxpEbrhYEB5TdKZ154QXvvzcoCEhIAEaMkI6FhgJNmjhe60kCZ7+8gDezUMqTP9t3yb4zee3znj/LhzCBM4mcfC/WvCBygnFEemAckVaMIdID40ijwED3i4bbT2ziCds98gXEQ0L0S+CcVeDsxysWFyvHAqpU4AoumjOGmMAREREREVU2gYFAnTqurylLAmer7MnvDQkB4uIcr3WWwMnXi/M0gSuxWxLAfl05lQQu2KTLCDCBIyIiIiJSY9ZZKD0REFBxCZyzLpTOFu+uUQPIyBDb9gmcszXp7GfetB87V1ws/veUXRfMZQSoPPVt0dfoJpAfYByRHhhHpBVjiPTAONIoMBCoW9f1NXpW4GJiHK911YVy6ZUZIu0TOGfsEzj7+4qLHb6vc932nj3bxzCBM4kGkQ2MbgL5AcYR6YFxRFoxhkgPjKMyatZMfN5xh/cJ3F13qV8nn93Sdo99BU5tvJ0nY+CcVencPcuDBK5OUA3Pnu1jmMCZRFCABzMFEbnBOCI9MI5IK8YQ6aFC4siTmRrNZudOICtLVMRsyww0bap+rX0CV6uWtN29u7QdFiZt2/7M5H92ISHeJ3C27qtlrcDZd6EsKnL4vqACDxJIH8QEziTSz6Qb3QTyA4wj0gPjiLRiDJEeKiSOJk8GYmOBKVPK/7sqSkgIcNVVYrtDB+Cvv4A//lC/1lUCJ18jTr4ouLMxcN4mcLZJScqxC+Wps0c9e7aP8YkE7r333kOjRo1QpUoVtG/fHqmpqU6vnT9/PiwWi+KnSpUqimusVismTJiAmJgYVK1aFV27dsXevXvL+9coVyvTVxrdBPIDjCPSA+OItGIMkR4qJI6uvho4fBh46aXy/y6jtG6tTMDk7BO46GhpOzwc+Owz4P33pW6ZgPMxcPK/r9u2Peke6WkCZ58Mqk1iYnfN7iPbPXu2jzE8gVu6dCnGjBmDiRMnYvv27WjTpg169OiBU6dOOb0nIiICJ06cKP05fPiw4vy0adPwzjvvYNasWfjtt99QrVo19OjRA5cvXy7vX4eIiIiI/Im7BbH9mX0CFxIibefmAv36ASNHAg0bOt7jqgJn63LpTRfKatVct9WTCpxdwhhcxGUEymT69Ol49NFHMXToULRo0QKzZs1CWFgY5s6d6/Qei8WCunXrlv7UkU2BarVaMWPGDCQnJyMpKQmtW7fGwoULcfz4cXz11VcV8BsREREREfkBi0XMVmkTGChV27p1k443kE0o46wCJ0/gbNvedKGsXt11W+0TOPv14lQqcFwHrgwKCgqwbds2dO3atfRYQEAAunbtiq1btzq9Lzc3Fw0bNkRsbCySkpKwa9eu0nMHDx5EZmam4pmRkZFo376902fm5+cjJydH8eNrosKijG4C+QHGEemBcURaMYZID4yjCiJPxAIDgR07gFOngPr1peOxsdK2J2PgbNd4U4Fzl8AVFQHHjokq2/btwH/+43je7vsirSEwI0On1jlz5gyKi4sVFTQAqFOnDvbs2aN6T3x8PObOnYvWrVsjOzsbb775Jm655Rbs2rULV199NTIzM0ufYf9M2zl7r7/+OiZPnuxwvN+yfggOE8FXNbgqlvZdipTNKRjWdhge++YxxbXjO45HWmYa+rboi6k/T0V6ljSwNjE2EUnNk7Dj5A6EBYdh9rbZinu/eOALTN0yFWM6jMGDyx9UnBuVMApZeVlI7piMlM0pSD0mjQ9sWbslRiWMwtq9axEfHY9pW6Yp7p2bNBez/piF5E7J6LO0D4pKpKAd2HogwoLDEFM9BlsytmD9gfWl52IjYvFG9zfwwe8foHtcd4z/YbziuW/3eBtLdi5BcqdkDF05FFl5WaXnkuKTEB8dj6KSImRkZ2D5P8tLz0WERmBxn8VI2ZyCQW0GYdSaUYrnTuoyCVsytmBQm0GYuHEiDpw7UHquc8PO6B7XHfvP7Re/W5qyQruq/yqkbE7BkwlPYuCXAxXnxnQYg4zsDHRp1AVzts9BWmZa6bm2ddti+A3DsfHQRjSIbIDpW6cr7l3UexHeTX0XyZ2Scc+SexTnhrUdBgCIqxmHdfvXYdPhTaXnmtRsgsldJmPhXwuR2CARkzZOUtw7s9dMLPxrIZI7JWPAigHIyZf+0aBvi75oENkAQQFBSD+TrujjHxUWhXlJ85CyOQX9W/bH6O9GK5475bYpWLd/HUbeNBLPr3seR3KOlJ7r1qQbnkx4Eqv/XY28wjws+ntR6bmggCCs6LcCKZtT8NiNj2HYymGK545NHIv0M+no2awnZqbOxM5TO0vPJdRPQP+W/ZF6LBVRYVGYmTpTce9nfT/D9K3TMS5xHO77/D7FuRHtRiCvMA+t6rTCyj0rseXIltJz8VHxGHfrOCz/Zzna1m2LKT8pB5DPumsW5qbNRXKnZPRb3g+XCqWFPfu37I+osChEhkYiLTMNa/auKT0XEx6Dmb1mYvrW6UhqnoSx68cqnjut2zSs3LMSYzqMwag1o3Ai90TpuV7NeqFt3bbIzs9GVl4WluxcUnrO6HdEQv0ELNm5pMLeEXmFeXxHwP/eEYkNEnHiwokKeUfY/rz4jhD87R1REX+PyMrLUvz/ju8IQe93xGeWItjmmDyceww7D38v3hHfS++IhP1nkHxlO6sgG0OXDEbn34/i2f9v796joyrvNY4/k9tkQsgFAglBUBQOKKBgYjCCtS05QmRpsXhdqQ1apShYqBUvKGoFCqd2ia2nUnGJriVKKi5Ry0FsGm8VuYncwiXiDRBIkGIugEBC3vNHyGQmGXJhttmzJ9/PWlnMvPvN5LfxEfitd+93nxo7UF2u5Tv+rvH1748cUHdJ1SeO6XRPmlu84SUtXrxWS48fU6Qk0zlezV3M+o+FD+rqWX/XmoxUxR+u1sBGx+e8P0u7vuisv/mMpUTWNYWh8GdEVHUb2jJjo7179xpJ5uOPP/YbnzZtmsnKymrVZ5w4ccKcd9555uGHHzbGGLNy5Uojyezbt89v3vXXX29uuOGGgJ9x7NgxU1FR4f3as2ePkWQqKirO4Kx+GDM/mGl3CQgD5AhWIEcIFhmCFchRO+nSxZi69TBjnn8+8JzNmxvmHDlSN/b3vzeM3X67MbW1De979677NSKiYazx17RpdZ9z4YV176+44vRzJWMGDGh4nZPT9Pirrxqzdavf2Lqxres32kNFRUWr+w9bV+BSUlIUGRmpsrIyv/GysjKltfRgwVOio6M1dOhQff7555Lk/b6ysjL18Hnie1lZmYYMGRLwM9xut9xu9xmcQfu5edDNLU8CWkCOYAVyhGCRIViBHLWTxpdQBjJwoJST43+vW+N74Hw3g6l/XdvMPWj1mw+29hLKr75qeN1oh3pJAe+B+6/4s5vOcwBb74GLiYlRRkaGioqKvGO1tbUqKipSdnZ2qz7j5MmT2rJli7dZ69Onj9LS0vw+s7KyUmvWrGn1Z4aixkvcwJkgR7ACOUKwyBCsQI7aSWsauIgIqbBQ+r//a2jOGjdwbdXWBs73sQGna+Aa7UL56Vcft72uEGD74+Xvuece5efnKzMzU1lZWXrqqad05MgR3XrrrZKkX/7yl+rZs6fmzJkjSXr88cd16aWXqm/fviovL9cTTzyhXbt26fbbb5dUt0Pl1KlTNWvWLPXr1099+vTRjBkzlJ6errFjx9p1mgAAAIDztKaBa+n7zuRKt+9P3ZNW38D5boKyc6f/s+caC9TABdjExKm7UNrewN1444369ttv9cgjj6i0tFRDhgzRihUrvJuQ7N69WxERDQuF3333ne644w6VlpYqOTlZGRkZ+vjjj3XBBRd459x33306cuSIJkyYoPLyco0YMUIrVqxo8sBvAAAAAM3wXT070wau8Qpc/Q6TzWm8Aue7wtbSrVaBGsYweoyA7Q2cJE2ePFmTJ08OeOz999/3ez9v3jzNmzev2c9zuVx6/PHH9fjjj1tVIgAAANDx2LUC17iB832uW3OXZCYnt+4euDFjtNPzubLaXpntQqKBQ8tm/3R2y5OAFpAjWIEcIVhkCFYgR+3EtxGLakPrYPUKXGJiw7HmGkm3u+UGbtAgadkyXVi2peU6QpCtm5ig9f75xT/tLgFhgBzBCuQIwSJDsAI5aidnugLn2+w1XoGrqWm5GWx8D9y0aVJurvTSS3WbppxOdbV/zfV8G7hTP9upGWIFziHuvOROu0tAGCBHsAI5QrDIEKxAjtrJme4m2dz31dTUHW90T5qfY8fqvurndOsmLW94oLYiI+uassZqak4/Xr8L5ananJohVuAcYto/p9ldAsIAOYIVyBGCRYZgBXLUTnwbMd/LGNvyfY1X4KqrW16BW7/ef+fJxk3g6VYDq6sDN4YBVuCcmiEaOIfYU7nH7hIQBsgRrECOECwyBCuQo3ZiRQMXaAWuLffTBfqM0zVwAR4XIClgA+fUDNHAAQAAAAjMt9GyagWupqZt99NJP8gKnFPRwAEAAAAIrP6+MSn4Fbjp0+t+ffrpwPepnY7L1bRhO10TZkzDxie+aODQ3v773P+2uwSEAXIEK5AjBIsMwQrkqJ0cPdrw2veetJYEauBmzZL275duv73hMQGtERNT18T5am4Fr34HS181NQ0PAz/VwDk1QzRwDjG893C7S0AYIEewAjlCsMgQrECO2olvA9e4iWqObwNXv+2/yyWlpdW9DtRknU6g3S+ba+BeeaXp2Ndf1zWOkreBc2qGaOAcYn/VfrtLQBggR7ACOUKwyBCsQI7aiW8D1xaBGrgzFei5bq29h+6mm+p+ffHFJp/n1AzRwDnE0eoz/J8H8EGOYAVyhGCRIViBHLUTKxq4tqzcBdLWFbh6Tz8t9ejRdPzUCpxTM0QD5xAvbX7J7hIQBsgRrECOECwyBCuQo3bSlksdfZ3JCtzIkdKzzzbdtTJQA9eajUiiogI3eqfGnJohGjgAAAAAgZ3pCpxv09baFbjUVGnCBCk52X880Cpaa1bgoqMDN3pt2UAlBNHAAQAAAAjsTFfgfHXq1Lp59U1Z46brkktOP7c5p1uBq6xsXT0hytkPQehAoiL4T4XgkSNYgRwhWGQIViBH7aRnT2nv3jP73pkz63Z/zMw8/ZzoaCkpSfr2W2ns2Lqxxg1cRkbT77OggXNqhlzGGGN3EaGmsrJSiYmJqqioUEJCgt3lAAAAAPbYtEm6915p9mwpK8u6z62/rLJzZ+mLL6StW6Urrqgb79u3bkyShgyRVq6U4uL8v3/wYKm4uPmfUVAglZRIjz7qP96/v7RjhyWnYZW29B9cQukQsz6cZXcJCAPkCFYgRwgWGYIVyFE7uegiqbDQ2ubNl8cjdesm/fjHDU2d7wrchg1Nm7fGc043FhUVeF5FhSTnZogGziEmZk60uwSEAXIEK5AjBIsMwQrkKEx4PE3HWrPDZKBLIwM1cIGeIXfqEkqnZogGziFue/M2u0tAGCBHsAI5QrDIEKxAjsJEbGzTsTNt4Gprm35OoM8/tbOmUzNEAwcAAADAHkOGNB1bsKCu8fqf/zn99wVq4E6e9H8fFdX0mXKStGRJm0oMNc7cegUAAACAc338sbRwoTRnTtNjWVl1lzkGuvyxXmtX4Bo3cAsWSNdd1/Z6QwgNHAAAAID2lZ1d93U6zTVvkhRoI/3GY4EauC5dWldfCOMSSoe4b/h9dpeAMECOYAVyhGCRIViBHHVwBw+2PCdQA+ezo6VTM0QD5xAlB0vsLgFhgBzBCuQIwSJDsAI56uDKylqeE6iB89n10qkZooFziNx+uXaXgDBAjmAFcoRgkSFYgRx1cOXlLc9pYQXOqRmigXOI/137v3aXgDBAjmAFcoRgkSFYgRyhRS00cE7NEA2cQxQfKLa7BIQBcgQrkCMEiwzBCuQILWrhEkqnZogGDgAAAED4aWEFzqlo4AAAAAA4y6JFUt++zc9pYQXOqWjgHCKrZ5bdJSAMkCNYgRwhWGQIViBHHVxenrRzpzRvXt37X/+66ZxADVx8vPelUzPkMibQU/A6tsrKSiUmJqqiokIJCQl2lyNJ+uLQFzqvy3l2lwGHI0ewAjlCsMgQrECOIKnu4d2ffSb16ydFRvof++YbKSJCSk+ve+/xSEePeg+HUoba0n+wAucQa/eutbsEhAFyBCuQIwSLDMEK5AiSJJdL6t+/rlFrrPEKXKPLJ52aIRo4h+ga19XuEhAGyBGsQI4QLDIEK5AjtCgqSoqNbXgfHe132KkZooFzCKc+pwKhhRzBCuQIwSJDsAI5Qouio/1X4GJi/A47NUM0cAAAAADCT1SU/31xjRo4p6KBAwAAAOB8q1fX3RNXLyrK/zgNHAAAAACEiGHDpKKihveNG7hG98A5FY8RCCAUHyNwtPqo4qKd/+R42IscwQrkCMEiQ7ACOUJAH3wg/fjHda/r25z6VbnMTGndOu/UUMoQjxEIQ0+uetLuEhAGyBGsQI4QLDIEK5AjBBTocQL1Gl1C6dQMsQIXQCiuwFWfrFZ0ZHgs+8I+5AhWIEcIFhmCFcgRAqqslBITpfh4qaqqbqx+Be6KK6T33/dODaUMsQIXhsa9Os7uEhAGyBGsQI4QLDIEK5AjBJSQIB06JJWVNT3W6B44p2YoquUpAAAAAOAQycmBx9mFEgAAAAAcwuOxuwJL0MABAAAACF+zZkkpKdLcuXZXYgkaOIf4dcav7S4BYYAcwQrkCMEiQ7ACOUKrPfSQdOCA1Lev37BTM0QD5xBHq4/aXQLCADmCFcgRgkWGYAVyhDap34nSh1MzRAPnEINTB9tdAsIAOYIVyBGCRYZgBXKEYDk1QzRwDvHmjjftLgFhgBzBCuQIwSJDsAI5QrCcmiEaOIdYuWel3SUgDJAjWIEcIVhkCFYgRwiWUzNEAwcAAAAADkEDBwAAAAAOQQPnEP279re7BIQBcgQrkCMEiwzBCuQIwXJqhlzGGGN3EaGmsrJSiYmJqqioUEJCgt3lSJIOHj2olLgUu8uAw5EjWIEcIVhkCFYgRwhWKGWoLf0HK3AO8dq21+wuAWGAHMEK5AjBIkOwAjlCsJyaIRo4hxiaNtTuEhAGyBGsQI4QLDIEK5AjBMupGaKBc4jZ/55tdwkIA+QIViBHCBYZghXIEYLl1AzRwAEAAACAQ9DAAQAAAIBD0MABAAAAgEPwGIEAQvExAvuq9im9c7rdZcDhyBGsQI4QLDIEK5AjBCuUMsRjBMLQwg0L7S4BYYAcwQrkCMEiQ7ACOUKwnJohVuACCMUVOAAAAADhiRW4MHTjazfaXQLCADmCFcgRgkWGYAVyhGA5NUM0cA7xffX3dpeAMECOYAVyhGCRIViBHCFYTs0QDRwAAAAAOESU3QWEovrbAisrK22upEH10eqQqgfORI5gBXKEYJEhWIEcIVihlKH6OlqzPQmbmATwzTffqFevXnaXAQAAAKAD2bNnj84666xm59DABVBbW6t9+/apc+fOcrlcdpejyspK9erVS3v27GFXTJwxcgQrkCMEiwzBCuQIwQq1DBljVFVVpfT0dEVENH+XG5dQBhAREdFi52uHhISEkAgYnI0cwQrkCMEiQ7ACOUKwQilDiYmJrZrHJiYAAAAA4BA0cAAAAADgEDRwDuB2u/Xoo4/K7XbbXQocjBzBCuQIwSJDsAI5QrCcnCE2MQEAAAAAh2AFDgAAAAAcggYOAAAAAByCBg4AAAAAHIIGDgAAAAAcggbOAf7617/qnHPOUWxsrIYNG6a1a9faXRJCxJw5c3TJJZeoc+fO6t69u8aOHauSkhK/OceOHdOkSZPUtWtXxcfHa9y4cSorK/Obs3v3bo0ZM0ZxcXHq3r27pk2bppqamvY8FYSIuXPnyuVyaerUqd4xMoTW2Lt3r37xi1+oa9eu8ng8Gjx4sD755BPvcWOMHnnkEfXo0UMej0c5OTnauXOn32ccOnRIeXl5SkhIUFJSkn71q1/p8OHD7X0qsMnJkyc1Y8YM9enTRx6PR+edd55mzpwp3/32yBF8ffjhh7r66quVnp4ul8ulN954w++4VXnZvHmzLr/8csXGxqpXr1764x//+EOfWvMMQlpBQYGJiYkxCxcuNFu3bjV33HGHSUpKMmVlZXaXhhAwatQo88ILL5ji4mKzceNGc9VVV5nevXubw4cPe+dMnDjR9OrVyxQVFZlPPvnEXHrppeayyy7zHq+pqTGDBg0yOTk5ZsOGDWb58uUmJSXFPPjgg3acEmy0du1ac84555gLL7zQTJkyxTtOhtCSQ4cOmbPPPtuMHz/erFmzxnz55ZfmnXfeMZ9//rl3zty5c01iYqJ54403zKZNm8w111xj+vTpY77//nvvnNGjR5uLLrrIrF692vz73/82ffv2NTfffLMdpwQbzJ4923Tt2tUsW7bMfPXVV2bJkiUmPj7e/PnPf/bOIUfwtXz5cvPQQw+Z119/3UgyS5cu9TtuRV4qKipMamqqycvLM8XFxWbx4sXG4/GYZ599tr1OswkauBCXlZVlJk2a5H1/8uRJk56ebubMmWNjVQhVBw4cMJLMBx98YIwxpry83ERHR5slS5Z452zfvt1IMqtWrTLG1P3hFxERYUpLS71z5s+fbxISEszx48fb9wRgm6qqKtOvXz9TWFhorrjiCm8DR4bQGvfff78ZMWLEaY/X1taatLQ088QTT3jHysvLjdvtNosXLzbGGLNt2zYjyaxbt8475+233zYul8vs3bv3hyseIWPMmDHmtttu8xv7+c9/bvLy8owx5AjNa9zAWZWXZ555xiQnJ/v9fXb//feb/v37/8BndHpcQhnCTpw4ofXr1ysnJ8c7FhERoZycHK1atcrGyhCqKioqJEldunSRJK1fv17V1dV+GRowYIB69+7tzdCqVas0ePBgpaameueMGjVKlZWV2rp1aztWDztNmjRJY8aM8cuKRIbQOm+99ZYyMzN1/fXXq3v37ho6dKiee+457/GvvvpKpaWlfjlKTEzUsGHD/HKUlJSkzMxM75ycnBxFRERozZo17XcysM1ll12moqIiffbZZ5KkTZs26aOPPlJubq4kcoS2sSovq1at0o9+9CPFxMR454waNUolJSX67rvv2uls/EXZ8lPRKgcPHtTJkyf9/lEkSampqdqxY4dNVSFU1dbWaurUqRo+fLgGDRokSSotLVVMTIySkpL85qampqq0tNQ7J1DG6o8h/BUUFOjTTz/VunXrmhwjQ2iNL7/8UvPnz9c999yj6dOna926dfrNb36jmJgY5efne3MQKCe+Oerevbvf8aioKHXp0oUcdRAPPPCAKisrNWDAAEVGRurkyZOaPXu28vLyJIkcoU2syktpaan69OnT5DPqjyUnJ/8g9TeHBg4IE5MmTVJxcbE++ugju0uBg+zZs0dTpkxRYWGhYmNj7S4HDlVbW6vMzEz94Q9/kCQNHTpUxcXF+tvf/qb8/Hybq4NTvPrqq3r55Zf1yiuvaODAgdq4caOmTp2q9PR0cgT44BLKEJaSkqLIyMgmu72VlZUpLS3NpqoQiiZPnqxly5bpvffe01lnneUdT0tL04kTJ1ReXu433zdDaWlpATNWfwzhbf369Tpw4IAuvvhiRUVFKSoqSh988IH+8pe/KCoqSqmpqWQILerRo4cuuOACv7Hzzz9fu3fvltSQg+b+PktLS9OBAwf8jtfU1OjQoUPkqIOYNm2aHnjgAd10000aPHiwbrnlFv32t7/VnDlzJJEjtI1VeQnFv+No4EJYTEyMMjIyVFRU5B2rra1VUVGRsrOzbawMocIYo8mTJ2vp0qV69913myzxZ2RkKDo62i9DJSUl2r17tzdD2dnZ2rJli98fYIWFhUpISGjyDzKEn5EjR2rLli3auHGj9yszM1N5eXne12QILRk+fHiTR5h89tlnOvvssyVJffr0UVpaml+OKisrtWbNGr8clZeXa/369d457777rmprazVs2LB2OAvY7ejRo4qI8P+naWRkpGprayWRI7SNVXnJzs7Whx9+qOrqau+cwsJC9e/f35bLJyXxGIFQV1BQYNxut3nxxRfNtm3bzIQJE0xSUpLfbm/ouO68806TmJho3n//fbN//37v19GjR71zJk6caHr37m3effdd88knn5js7GyTnZ3tPV6/BfyVV15pNm7caFasWGG6devGFvAdmO8ulMaQIbRs7dq1JioqysyePdvs3LnTvPzyyyYuLs4sWrTIO2fu3LkmKSnJvPnmm2bz5s3mZz/7WcDtvIcOHWrWrFljPvroI9OvXz+2f+9A8vPzTc+ePb2PEXj99ddNSkqKue+++7xzyBF8VVVVmQ0bNpgNGzYYSebJJ580GzZsMLt27TLGWJOX8vJyk5qaam655RZTXFxsCgoKTFxcHI8RQPOefvpp07t3bxMTE2OysrLM6tWr7S4JIUJSwK8XXnjBO+f77783d911l0lOTjZxcXHm2muvNfv37/f7nK+//trk5uYaj8djUlJSzO9+9ztTXV3dzmeDUNG4gSNDaI1//OMfZtCgQcbtdpsBAwaYBQsW+B2vra01M2bMMKmpqcbtdpuRI0eakpISvzn/+c9/zM0332zi4+NNQkKCufXWW01VVVV7ngZsVFlZaaZMmWJ69+5tYmNjzbnnnmseeughv+3byRF8vffeewH/HZSfn2+MsS4vmzZtMiNGjDBut9v07NnTzJ07t71OMSCXMT6PtwcAAAAAhCzugQMAAAAAh6CBAwAAAACHoIEDAAAAAIeggQMAAAAAh6CBAwAAAACHoIEDAAAAAIeggQMAAAAAh6CBAwAAAACHoIEDAMAGLpdLb7zxht1lAAAchgYOANDhjB8/Xi6Xq8nX6NGj7S4NAIBmRdldAAAAdhg9erReeOEFvzG3221TNQAAtA4rcACADsntdistLc3vKzk5WVLd5Y3z589Xbm6uPB6Pzj33XL322mt+379lyxb99Kc/lcfjUdeuXTVhwgQdPnzYb87ChQs1cOBAud1u9ejRQ5MnT/Y7fvDgQV177bWKi4tTv3799NZbb3mPfffdd8rLy1O3bt3k8XjUr1+/Jg0nAKDjoYEDACCAGTNmaNy4cdq0aZPy8vJ00003afv27ZKkI0eOaNSoUUpOTta6deu0ZMkS/etf//Jr0ObPn69JkyZpwoQJ2rJli9566y317dvX72f8/ve/1w033KDNmzfrqquuUl5eng4dOuT9+du2bdPbb7+t7du3a/78+UpJSWm/3wAAQEhyGWOM3UUAANCexo8fr0WLFik2NtZvfPr06Zo+fbpcLpcmTpyo+fPne49deumluvjii/XMM8/oueee0/333689e/aoU6dOkqTly5fr6quv1r59+5SamqqePXvq1ltv1axZswLW4HK59PDDD2vmzJmS6prC+Ph4vf322xo9erSuueYapaSkaOHChT/Q7wIAwIm4Bw4A0CH95Cc/8WvQJKlLly7e19nZ2X7HsrOztXHjRknS9u3bddFFF3mbN0kaPny4amtrVVJSIpfLpX379mnkyJHN1nDhhRd6X3fq1EkJCQk6cOCAJOnOO+/UuHHj9Omnn+rKK6/U2LFjddlll53RuQIAwgcNHACgQ+rUqVOTSxqt4vF4WjUvOjra773L5VJtba0kKTc3V7t27dLy5ctVWFiokSNHatKkSfrTn/5keb0AAOfgHjgAAAJYvXp1k/fnn3++JOn888/Xpk2bdOTIEe/xlStXKiIiQv3791fnzp11zjnnqKioKKgaunXrpvz8fC1atEhPPfWUFixYENTnAQCcjxU4AECHdPz4cZWWlvqNRUVFeTcKWbJkiTIzMzVixAi9/PLLWrt2rZ5//nlJUl5enh599FHl5+frscce07fffqu7775bt9xyi1JTUyVJjz32mCZOnKju3bsrNzdXVVVVWrlype6+++5W1ffII48oIyNDAwcO1PHjx7Vs2TJvAwkA6Lho4AAAHdKKFSvUo0cPv7H+/ftrx44dkup2iCwoKNBdd92lHj16aPHixbrgggskSXFxcXrnnXc0ZcoUXXLJJYqLi9O4ceP05JNPej8rPz9fx44d07x583TvvfcqJSVF1113Xavri4mJ0YMPPqivv/5aHo9Hl19+uQoKCiw4cwCAk7ELJQAAjbhcLi1dulRjx461uxQAAPxwDxwAAAAAOAQNHAAAAAA4BPfAAQDQCHcXAABCFStwAAAAAOAQNHAAAAAA4BA0cAAAAADgEDRwAAAAAOAQNHAAAAAA4BA0cAAAAADgEDRwAAAAAOAQNHAAAAAA4BD/D7D//V55EJlqAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the x axes is wrong, but ignore for now\n",
        "iters = [iter * args['eval_epoch'] for iter in range(len(val_losses))]\n",
        "epochs = range(args['num_epoch'])\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "ax.plot(iters, val_losses, color='b', label='Validation', alpha=1)\n",
        "ax.grid(color='g', ls='-.', lw=0.5)\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Losses')\n",
        "plt.title('Validation Losses')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "nIWZ4ccHlV7q",
        "outputId": "5ae349ca-68a0-4183-886e-b4430e63ff74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3kAAAJJCAYAAAAX/f7MAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACjJUlEQVR4nOzde5xN9f7H8ddczJjBDEbMEMb9fkuNxhQql6goHExCdHG/nuNS7oUOlZzoUEpFNamRKHJJ7tS4VZwYd+MutxkMZszs3x/rZ7OjYsya78ze7+fjsR/NrLX22p/tvM+qj+9a36+Xw+FwICIiIiIiIm7B23QBIiIiIiIiknnU5ImIiIiIiLgRNXkiIiIiIiJuRE2eiIiIiIiIG1GTJyIiIiIi4kbU5ImIiIiIiLgRNXkiIiIiIiJuRE2eiIiIiIiIG1GTJyIiIiIi4kbU5ImISI6xf/9+vLy8+Oijj5zbRo0ahZeX1y2938vLi1GjRmVqTQ0aNKBBgwaZek4REZE7oSZPRERs0bx5cwIDAzl37tyfHtO+fXv8/Pw4depUFlZ2+3777TdGjRrF/v37TZfitGLFCry8vIiNjTVdioiIZDNq8kRExBbt27fn4sWLzJ0796b7k5OTmTdvHo8++ighISEZ/pxhw4Zx8eLFDL//Vvz222+MHj36pk3ekiVLWLJkia2fLyIicjvU5ImIiC2aN29Ovnz5+Oyzz266f968eVy4cIH27dvf0ef4+vqSO3fuOzrHnfDz88PPz8/Y54uIiPyRmjwREbFFQEAALVu2ZNmyZZw4ceKG/Z999hn58uWjefPmnD59mn/9619Uq1aNvHnzEhQURNOmTfnll1/+9nNu9kze5cuX6d+/P3fddZfzMw4dOnTDew8cOECPHj2oUKECAQEBhISE8I9//MNlxO6jjz7iH//4BwAPPfQQXl5eeHl5sWLFCuDmz+SdOHGC5557jiJFipA7d25q1KjBxx9/7HLM1ecL33jjDd577z3KlCmDv78/9913Hxs2bPjb732r9u7dyz/+8Q8KFixIYGAg999/PwsWLLjhuMmTJ1OlShUCAwMpUKAA9957r0uDfu7cOfr160d4eDj+/v4ULlyYRo0asXnzZpfz/PTTTzz66KMEBwcTGBhI/fr1Wbt2rcsxt3ouERHJGF/TBYiIiPtq3749H3/8MV988QW9evVybj99+jSLFy8mOjqagIAA/ve///H111/zj3/8g1KlSnH8+HHeffdd6tevz2+//UbRokVv63Off/55PvnkE55++mnq1q3LDz/8wGOPPXbDcRs2bGDdunW0a9eOu+++m/379zN16lQaNGjAb7/9RmBgIPXq1aNPnz68/fbbvPzyy1SqVAnA+c8/unjxIg0aNGD37t306tWLUqVK8eWXX/Lss89y9uxZ+vbt63L8Z599xrlz5+jatSteXl5MmDCBli1bsnfvXnLlynVb3/uPjh8/Tt26dUlOTqZPnz6EhITw8ccf07x5c2JjY3nqqacAmD59On369KF169b07duXS5cu8euvv/LTTz/x9NNPA9CtWzdiY2Pp1asXlStX5tSpU6xZs4bt27dzzz33APDDDz/QtGlTateuzciRI/H29ubDDz/k4YcfZvXq1URERNzyuURE5A44REREbHLlyhVHWFiYIzIy0mX7tGnTHIBj8eLFDofD4bh06ZIjLS3N5Zh9+/Y5/P39Ha+88orLNsDx4YcfOreNHDnScf2/zn7++WcH4OjRo4fL+Z5++mkH4Bg5cqRzW3Jy8g01r1+/3gE4Zs6c6dz25ZdfOgDH8uXLbzi+fv36jvr16zt/nzRpkgNwfPLJJ85tKSkpjsjISEfevHkdSUlJLt8lJCTEcfr0aeex8+bNcwCOb7755obPut7y5csdgOPLL7/802P69evnAByrV692bjt37pyjVKlSjvDwcOefeYsWLRxVqlT5y88LDg529OzZ80/3p6enO8qVK+do0qSJIz093bk9OTnZUapUKUejRo1u+VwiInJndLumiIjYxsfHh3bt2rF+/XqXWyA/++wzihQpwiOPPAKAv78/3t7Wv5LS0tI4deoUefPmpUKFCrd9C9/ChQsB6NOnj8v2fv363XBsQECA8+fU1FROnTpF2bJlyZ8/f4ZvHVy4cCGhoaFER0c7t+XKlYs+ffpw/vx5Vq5c6XJ827ZtKVCggPP3Bx98ELBus7xTCxcuJCIiggceeMC5LW/evLz44ovs37+f3377DYD8+fNz6NChv7xNNH/+/Pz0008cOXLkpvt//vlndu3axdNPP82pU6c4efIkJ0+e5MKFCzzyyCOsWrWK9PT0WzqXiIjcGTV5IiJiq6sTq1x9vuvQoUOsXr2adu3a4ePjA0B6ejpvvfUW5cqVw9/fn0KFCnHXXXfx66+/kpiYeFufd+DAAby9vSlTpozL9goVKtxw7MWLFxkxYgTFixd3+dyzZ8/e9ude//nlypVzNq1XXb2988CBAy7bS5Qo4fL71YbvzJkzGfr8P9Zys+/9x1oGDx5M3rx5iYiIoFy5cvTs2fOG5+gmTJjAtm3bKF68OBEREYwaNcqlEd21axcAnTp14q677nJ5vf/++1y+fNn5Z/p35xIRkTujJk9ERGxVu3ZtKlasSExMDAAxMTE4HA6XWTXHjRvHgAEDqFevHp988gmLFy9m6dKlVKlSxTn6Y4fevXszduxY2rRpwxdffMGSJUtYunQpISEhtn7u9a42un/kcDiy5PPBavri4+P5/PPPeeCBB5gzZw4PPPAAI0eOdB7Tpk0b9u7dy+TJkylatCivv/46VapU4bvvvgNw/nm9/vrrLF269KavvHnz3tK5RETkzmjiFRERsV379u0ZPnw4v/76K5999hnlypXjvvvuc+6PjY3loYce4oMPPnB539mzZylUqNBtfVbJkiVJT09nz549LqNY8fHxNxwbGxtLp06dePPNN53bLl26xNmzZ12O++PsnX/3+b/++ivp6ekuo3k7duxw7s8qJUuWvOn3vlktefLkoW3btrRt25aUlBRatmzJ2LFjeemll5xLVISFhdGjRw969OjBiRMnuOeeexg7dixNmzZ1jpwGBQXRsGHDv63tr84lIiJ3RiN5IiJiu6ujdiNGjODnn3++YW08Hx+fG0auvvzySw4fPnzbn3W1SXj77bddtk+aNOmGY2/2uZMnTyYtLc1lW548eQBuaP5uplmzZhw7dozZs2c7t125coXJkyeTN29e6tevfytfI1M0a9aMuLg41q9f79x24cIF3nvvPcLDw6lcuTIAp06dcnmfn58flStXxuFwkJqaSlpa2g23rxYuXJiiRYty+fJlwBqxLVOmDG+88Qbnz5+/oZbff/8d4JbOJSIid0YjeSIiYrtSpUpRt25d5s2bB3BDk/f444/zyiuv0LlzZ+rWrcvWrVv59NNPKV269G1/Vs2aNYmOjua///0viYmJ1K1bl2XLlrF79+4bjn388ceZNWsWwcHBVK5cmfXr1/P9998TEhJywzl9fHwYP348iYmJ+Pv78/DDD1O4cOEbzvniiy/y7rvv8uyzz7Jp0ybCw8OJjY1l7dq1TJo0iXz58t32d/orc+bMcY7MXa9Tp04MGTKEmJgYmjZtSp8+fShYsCAff/wx+/btY86cOc6RxsaNGxMaGkpUVBRFihRh+/btTJkyhccee4x8+fJx9uxZ7r77blq3bk2NGjXImzcv33//PRs2bHCOgnp7e/P+++/TtGlTqlSpQufOnSlWrBiHDx9m+fLlBAUF8c0333Du3Lm/PZeIiNwho3N7ioiIx3jnnXccgCMiIuKGfZcuXXL885//dISFhTkCAgIcUVFRjvXr19+wPMGtLKHgcDgcFy9edPTp08cREhLiyJMnj+OJJ55wHDx48IYlFM6cOePo3Lmzo1ChQo68efM6mjRp4tixY4ejZMmSjk6dOrmcc/r06Y7SpUs7fHx8XJZT+GONDofDcfz4ced5/fz8HNWqVXOp+frv8vrrr9/w5/HHOm/m6hIKf/a6umzCnj17HK1bt3bkz5/fkTt3bkdERITj22+/dTnXu+++66hXr54jJCTE4e/v7yhTpoxj4MCBjsTERIfD4XBcvnzZMXDgQEeNGjUc+fLlc+TJk8dRo0YNx3//+98b6tqyZYujZcuWznOVLFnS0aZNG8eyZctu+1wiIpIxXg5HFj7ZLSIiIiIiIrbSM3kiIiIiIiJuRE2eiIiIiIiIG1GTJyIiIiIi4kbU5ImIiIiIiLgRNXkiIiIiIiJuRE2eiIiIiIiIG9Fi6BmUnp7OkSNHyJcvH15eXqbLERERERERN+ZwODh37hxFixbF2/uvx+rU5GXQkSNHKF68uOkyRERERETEgxw8eJC77777L49Rk5dB+fLlA6w/5KCgIMPVWCasncCgqEGmyxA3p5yJ3ZQxsZsyJnZTxsQOSUlJFC9e3NmH/BUvh8PhyIKa3E5SUhLBwcEkJiZmmybvyLkjFM1X1HQZ4uaUM7GbMiZ2U8bEbsqY2OF2+g9NvOJGun3bzXQJ4gGUM7GbMiZ2U8bEbsqYmKYmT0RERERExI2oyRMREREREXEjmnhFRERERCQHcjgcXLlyhbS0NNOlSCbw8fHB19c3U5ZnU5PnRoY+ONR0CeIBlDOxmzImdlPGxG5ZkbGUlBSOHj1KcnKy7Z8lWScwMJCwsDD8/Pzu6Dxq8tzIlmNbqHN3HdNliJtTzsRuypjYTRkTu9mdsfT0dPbt24ePjw9FixbFz88vU0Z/xByHw0FKSgq///47+/bto1y5cn+74Plf0RIKGZQdl1A4mXySQoGFTJchbk45E7spY2I3ZUzsZnfGLl26xL59+yhZsiSBgYG2fY5kveTkZA4cOECpUqXInTu3yz4toeChxq8Zb7oE8QDKmdhNGRO7KWNit6zK2J2M9Ej2lFn/myoZbiT+VLzpEsQDKGdiN2VM7KaMid2UMTFNTZ6IiIiIiIgbUZMnIiIiIiI5QoMGDejXr5/z9/DwcCZNmvSX7/Hy8uLrr7++48/OrPNkBTV5biSqeJTpEsQDKGdiN2VM7KaMid2UsZt74oknePTRR2+6b/Xq1Xh5efHrr7/e1jk3bNjAiy++mBnlOY0aNYqaNWvesP3o0aM0bdo0Uz/LLmry3EiLii1MlyAeQDkTuyljYjdlTOymjN3cc889x9KlSzl06NAN+z788EPuvfdeqlevflvnvOuuu7JshtHQ0FD8/f2z5LPulJo8N7L1+FbTJYgHUM7EbsqY2E0ZE7uZyJjDARcumHnd6oJsjz/+OHfddRcfffSRy/bz58/z5Zdf8uSTTxIdHU2xYsUIDAykWrVqxMTE/OU5/3i75q5du6hXrx65c+emcuXKLF269Ib3DB48mPLlyxMYGEjp0qUZPnw4qampAHz00UeMHj2aX375BS8vL7y8vJz1/vF2za1bt/Lwww8TEBBASEgIL774IufPn3fuf/bZZ3nyySd54403CAsLIyQkhJ49ezo/y05aDN2NBObSOiliP+VM7KaMid2UMbGbiYwlJ0PevFn+sQCcPw958vz9cb6+vnTs2JGPPvqIoUOHOhdw//LLL0lLS+OZZ57hyy+/ZPDgwQQFBbFgwQI6dOhAmTJliIiI+Nvzp6en07JlS4oUKcJPP/1EYmKiy/N7V+XLl4+PPvqIokWLsnXrVl544QXy5cvHoEGDaNu2Ldu2bWPRokV8//33AAQHB99wjgsXLtCkSRMiIyPZsGEDJ06c4Pnnn6dXr14uTezy5csJCwtj+fLl7N69m7Zt21KzZk1eeOGFv/8DuwMayXMj725613QJ4gGUM7GbMiZ2U8bEbsrYn+vSpQt79uxh5cqVzm0ffvghrVq1omTJkvzrX/+iZs2alC5dmt69e/Poo4/yxRdf3NK5v//+e3bs2MHMmTOpUaMG9erVY9y4cTccN2zYMOrWrUt4eDhPPPEE//rXv5yfERAQQN68efH19SU0NJTQ0FACAgJuOMdnn33GpUuXmDlzJlWrVuXhhx9mypQpzJo1i+PHjzuPK1CgAFOmTKFixYo8/vjjPPbYYyxbtux2/9hum0byRERERERyuMBAa0TN1GffqooVK1K3bl1mzJhBgwYN2L17N6tXr+aVV14hLS2NcePG8cUXX3D48GFSUlK4fPnyLT9zt337dooXL07RokWd2yIjI284bvbs2bz99tvs2bOH8+fPc+XKFYKCgm79S/z/Z9WoUYM81w1hRkVFkZ6eTnx8PEWKFAGgSpUq+Pj4OI8JCwtj61b7b+dVkyciIiIiksN5ed3aLZPZwXPPPUfv3r155513+PDDDylTpgz169dn/Pjx/Oc//2HSpElUq1aNPHny0K9fP1JSUjLts9evX0/79u0ZPXo0TZo0ITg4mM8//5w333wz0z7jerly5XL53cvLi/T0dFs+63q6XdNNzJgBv37QnTffhPnzYft2uHzZdFUiIiIiIq7atGmDt7c3n332GTNnzqRLly54eXmxdu1aWrRowTPPPEONGjUoXbo0O3fuvOXzVqpUiYMHD3L06FHnth9//NHlmHXr1lGyZEmGDh3KvffeS7ly5Thw4IDLMX5+fqSlpf3tZ/3yyy9cuHDBuW3t2rV4e3tToUKFW67ZLhrJcxMLF8KBZU3513W3+Hp5QYkSUK7ctVfZstY/S5cGPz9z9UrONafNHNMliJtTxsRuypjYTRn7a3nz5qVt27a89NJLJCUl8eyzzwJQrlw5YmNjWbduHQUKFGDixIkcP36cypUr39J5GzZsSPny5enUqROvv/46SUlJDB061OWYcuXKkZCQwOeff859993HggULmDt3rssx4eHh7Nu3j59//pm7776bfPny3bB0Qvv27Rk5ciSdOnVi1KhR/P777/Tu3ZsOHTo4b9U0SSN5bqJTJ6jfYQ1t2kCtWtbsSg4HHDgA338PU6fCgAHQvDlUqgQBAVaj17gx9OwJkybBggWwcydk4oi4uKHxa8ebLkHcnDImdlPGxG7K2N977rnnOHPmDE2aNHE+Qzds2DDuuecemjRpQoMGDQgNDeXJJ5+85XN6e3szd+5cLl68SEREBM8//zxjx451OaZ58+b079+fXr16UbNmTdatW8fw4cNdjmnVqhWPPvooDz30EHfddddNl3EIDAxk8eLFnD59mvvuu4/WrVvzyCOPMGXKlNv/w7CBl8NxqytbyPWSkpIIDg4mMTHxth/UtEtyarJzyl6HA06cgF27rr12777283Ujyzfw8YGSJW8c/StXDsLD4Q+3FouHuT5nInZQxsRuypjYze6MXbp0iX379lGqVCly585t2+dI1vur/21vp//Q7ZpupF1sO+ZHzwesWzWLFLFeDzzgepzDAceOuTZ91zeBycmwd6/1WrzY9b0+Plajd7NbQMPDwVeJcnvX50zEDsqY2E0ZE7spY2Ka/pPcA3l5QViY9XrwQdd9DgccPXrz0b/du+HiRdizx3otWuT6Xl9fKFXKdeTvahNYsqQaQBERERGRrKD/7BYXXl5QtKj1ql/fdV96umsDeH0TuHs3XLp0bft337m+N1cuq9krX/7GV5Ei1ueKiIiIiMidU5Mnt8zbG4oVs14NGrjuS0+Hw4dvfgvo7t3Wcg7bt1uvP8qXz2r2KlRwbf7KlYNs8rijiIiIiEiOoSbPjfSK6GXss729oXhx6/XQQ6770tPh4EFr5s7rX/HxsH8/nDsHmzZZrz8KDb15A6glIMwxmTPxDMqY2E0ZE7tlVcY0f6L7yaz/TdXkuZFTyadMl3BT3t7WM3klS0KjRq77Ll2yJnj5YwO4cyccP25NEHPsGKxadeM5S5W61vRd3wQWK2btF3tk15yJ+1DGxG7KmNjN7ozl+v+pzpOTkwkICLD1syRrJScnA9f+N84oNXluJKJYhOkSblvu3FC5svX6o7NnrVs+b9YAnj9/bQKYPz7/FxBg3ep5swawYMEs+VpuLSfmTHIWZUzspoyJ3ezOmI+PD/nz5+fEiROAtWablyY4yNEcDgfJycmcOHGC/Pnz4+Pjc0fnU5PnRmK2xTCs3jDTZWSa/Pnhvvus1/WuLgERH39j87dnjzUD6K+/Wq8/Cglxve2zalWoU8ea/EVujbvlTLIfZUzspoyJ3bIiY6GhoQDORk/cQ/78+Z3/294JNXluJO5wnOkSssT1S0D8cQKYK1es5/yuPvN3fQN46BCcOgXr11uv65UoYTV7ERHWq3ZtyJMnq75RzuIpORNzlDGxmzImdsuKjHl5eREWFkbhwoVJTU21/fPEfrly5brjEbyr1OSJW/H1tZZqKFsWmjVz3XfhgjXT59Wmb8cO2LIFfvsNEhKs15dfWsd6e1ujfBER15q/ypW11p+IiIhkLz4+PpnWGIj70H+yisfIkwdq1LBe1zt3DjZuhLg46/XTT9ZyEFdv+Xz/feu4wEC4995ro3116lizieoWeBERERHJTtTkuZGqhauaLiFHypfPWvbh+qUfDh+GDRushi8uzvr53Dlrls/rZ/osUsR1tO+++6xnCd2ZciZ2U8bEbsqY2E0ZE9O8HFpgI0OSkpIIDg4mMTGRoGyyYveRc0comq+o6TLcUlqa9Yzf1ZG+uDhrlO/KlRuPrVDBdbSvenXw98/6mu2inIndlDGxmzImdlPGxA63039oNTE38t2u7/7+IMkQHx/rmbxnn4WpU62F25OSYO1aeOstiI62FmgHqxmcNQt697YavaAgq9nr0wc++cR6HjAn/9WKciZ2U8bEbsqY2E0ZE9N0u6YbqVCogukSPEpAANSta72uOnny2rN9V1+nTl37+aoCBaxbO6+/1bNw4az/DhmhnIndlDGxmzImdlPGxDTdrplB2fF2zeYxzZkfPd90GXIdhwP27nW9zXPzZrh8+cZjS5a81vDVqWMt4xAQkPU1/x3lTOymjIndlDGxmzImdrid/kMjeSI28vKCMmWsV3S0tS0lBbZudW38duyAAwes1xdfWMf5+lozgUZGwv33W/8sVUqzeYqIiIjIX1OTJ5LF/PysUbrataF7d2tbYqLrMg4//gjHjlnP/m3aBFOmWMfddde1hu/++61bPvPmNfddRERERCT7UZMnkg0EB8Mjj1gvsG7zTEiA9euthu/HH63bPH//Hb75xnqBtWh7tWrXmr7774fy5TXaJyIiIuLJ9ExeBmXHZ/JOJp+kUGAh02WITS5dgi1brjV+69fDoUM3HlewoPVM39XGLyLCaiIzi3ImdlPGxG7KmNhNGRM7aAkFDzVt4zTTJYiNcue2GrcBA6zn9g4etF6xsfDPf0JUlLUe3+nT8N13MGIENG5szeRZtSo8/zx88AH873+Qnp7xOpQzsZsyJnZTxsRuypiYppG8DMqOI3kiKSnwyy/XRvp+/BH27bvxuKtr910d7atTxxoBFBEREZHsSSN5Hqrl7JamSxDD/PysyVh694bPPrOWbzh6FL7+GoYMgfr1ITDQWsh96VJ45RVo1gxCQqBiRWux93fftRrFtLSbf4ZyJnZTxsRuypjYTRkT0zTxihu5kn7FdAmSDYWGQosW1gvgyhVrCYfrR/t27YL4eOv18cfWcXnyWM/zXT+b5113KWdiP2VM7KaMid2UMTFNTZ6Ih/H1hVq1rNfVJRxOnrTW7Lva+MXFwblzsHy59bqqTBlILd6Lzxzw0EMQFmbmO4iIiIjIn1OTJyIUKgSPPWa9wLpVc/t215k8t2+HPXuAPY1pv8I6rlIlePhh61W/vnXbp4iIiIiYpSbPjXSo3sF0CeImfHysGTmrVoUXXrC2nT0L69bBtC/jOfxrBbZssRq/7dvhnXestflq1rzW9D34IOTLZ/JbSE6la5nYTRkTuyljYpqaPDcSmCvQdAnixvLntyZpcZTdzWPlK3D6NKxcCT/8YL1++81ax2/LFnjzTatRjIi41vRFRkJAgOlvITmBrmViN2VM7KaMiWlq8txIWD49ICX2u5qzggXhqaesF8CxY9bze1ebvr17rds816+HsWOtNfzq1r3W9N13H+TKZfCLSLala5nYTRkTuyljYpqWUHAjaxPWmi5BPMCf5Sw0FKKjYfp069m9fftgxgx45hlrgpbLl60mcPhwa+H2AgWskcE33oDNm+9sgXZxL7qWid2UMbGbMiamqclzI0v3LjVdgniAW81ZeDh07gyzZsHhw7BjB/z3v9C6tTVBy4UL8N13MHAg1K5tTf7SqpX1fN/27eBw2Ps9JPvStUzspoyJ3ZQxMU23a4qI7by8oEIF69W9uzVqt3XrtVs7V66EM2fgq6+sF1gjg1dv7Xz4YShVyux3EBEREckp1OSJSJbz9oYaNaxX//7WAu2bNl1r+tassZ7x++wz6wXWyODVhu+hh6BoUaNfQURERCTbUpPnRooHFTddgngAO3Lm6wt16livl16CS5es9fmuNn0//QT791vP+M2YYb2nYsVrTV+DBlqjz53oWiZ2U8bEbsqYmOblcOjJl4xISkoiODiYxMREgoKCTJcDQHJqsqbsFduZyNn589bo3tWmb/Nm12f2vLysyVyefRb+8Q/IJv+XlAzStUzspoyJ3ZQxscPt9B+aeMWNTN0w1XQJ4gFM5CxvXnj0UZgwATZuhFOnYO5c6N0bqlSxGr41a+D5561n+Z55Br7/HtLSsrxUyQS6londlDGxmzImpqnJcyONyzQ2XYJ4gOyQswIF4Mkn4e23Yds2OHgQ/v1v6xbOixfh00+hUSNrspZhw2DXLtMVy+3IDhkT96aMid2UMTFNTZ4bGfrDUNMliAfIjjm7+24YPBh++816lq97d8if32r+xo6F8uWt2zmnT4fERNPVyt/JjhkT96KMid2UMTFNTZ6IuA0vL2vylv/+F44ehdmzoWlTazbPdevgxRet2znbt4elS3U7p4iIiLgnNXki4pZy54Y2bWDhQmtEb8IEqFTJmrnzs8+gcWMoWRJefhni401XKyIiIpJ51OSJiNsrWhQGDoT//Q/i4qBHD+u5vsOH4bXXrGf56taF996Ds2dNVysiIiJyZ7SEQgZlxyUU9pzeQ5mCZUyXIW7OXXJ26RJ88w18/DF89x2kp1vbc+eGp56CTp2gYUPw8TFbpydyl4xJ9qWMid2UMbGDllDwUDHbYkyXIB7AXXKWO7e1pt6338KhQ/D669ZyDJcuQUyMtWRDyZLW4uw7dpiu1rO4S8Yk+1LGxG7KmJimkbwMyo4jeSJyZxwO2LTJGt377DM4ffravvvvt0b32ra1bvUUERERyUo5biTvnXfeITw8nNy5c1OnTh3i4uL+8vizZ8/Ss2dPwsLC8Pf3p3z58ixcuNC5/9y5c/Tr14+SJUsSEBBA3bp12bBhg3N/amoqgwcPplq1auTJk4eiRYvSsWNHjhw5Ytt3zAqd53U2XYJ4AHfOmZcX3HsvTJ4MR45AbCw8/rh1y+bVpRnCwqBdO1i0SLNz2sWdMybZgzImdlPGxDTjTd7s2bMZMGAAI0eOZPPmzdSoUYMmTZpw4sSJmx6fkpJCo0aN2L9/P7GxscTHxzN9+nSKFSvmPOb5559n6dKlzJo1i61bt9K4cWMaNmzI4cOHAUhOTmbz5s0MHz6czZs389VXXxEfH0/z5s2z5Dvb5VTyKdMliAfwlJz5+0OrVtZze4cOwZtvQtWqcPnytaUZSpSAIUNg+3bT1boXT8mYmKOMid2UMTHNeJM3ceJEXnjhBTp37kzlypWZNm0agYGBzJgx46bHz5gxg9OnT/P1118TFRVFeHg49evXp0aNGgBcvHiROXPmMGHCBOrVq0fZsmUZNWoUZcuWZerUqQAEBwezdOlS2rRpQ4UKFbj//vuZMmUKmzZtIiEhIcu+u4jkDKGhMGAA/PqrdTtn795QsKA12jd+PFSubK3PN3UqnDljuloRERHxdEabvJSUFDZt2kTDhg2d27y9vWnYsCHr16+/6Xvmz59PZGQkPXv2pEiRIlStWpVx48aR9v/3TV25coW0tDRy587t8r6AgADWrFnzp7UkJibi5eVF/vz5b7r/8uXLJCUlubxExLN4ecE998Dbb1sN3pw50Ly5dTvn1aUZwsKs5/a++w6uXDFdsYiIiHgiX5MffvLkSdLS0ihSpIjL9iJFirDjT6az27t3Lz/88APt27dn4cKF7N69mx49epCamsrIkSPJly8fkZGRvPrqq1SqVIkiRYoQExPD+vXrKVu27E3PeenSJQYPHkx0dPSfPsT42muvMXr06Bu2t/2yLbkCcwEQkCuA2a1nM2bVGLrU6kK3b7u5HDv0waFsObaF1pVbM37NeOJPXVuBOap4FC0qtmDr8a0E5grk3U3vurx3Tps5jF87ngGRA2gX285lX6+IXpxKPkXk3ZGMWTWGuMPXnmmsWrgqvSJ68d2u76hQqAIT1k5wee+MFjOYtnEaw+oNo+XsllxJv/ZfpR2qdyAwVyBh+cJYm7CWpXuXOvcVDyrO641fZ+qGqTQu05ihPwx1Oe9bTd4iZlsMw+oNo/O8zi63LbSo0IIKhSpwJf0KCYkJxP4W69wX5B/EJy0/YcyqMXSs0ZFeC3u5nHdUg1GsTVhLxxodGbliJHvP7HXuq1+yPo3LNGbPmT3Wd9viOho8P3o+Y1aNoXdEbzrM7eCyb0DkABISE2gQ3oAPNn/AlmNbnPtqhdbiuXueY8X+FZQILsHE9RNd3jvrqVlMjpvMsHrDaB7jestvl1pdAChToAxL9ixh5YGVzn2lC5RmdIPRzPxlJlElohi1YpTLe6c0m8LMX2YyrN4wnvnqGZIuX/uLhdaVW1MiuAS+3r7En4xnXvw8576QwBA+bPEhY1aNIbpqNP0X93c579iHx7JkzxK639edgUsGcjDpoHNfo9KNiCoRxdFzR0lOTWbWr7Oc+3y9ffmq7Vf4ePlwMvkkXeZ1cTnvoKhBxJ+Mp2m5pkyJm8K2E9uc+yKKRRBdNZq4w3GEBIYwJW6Ky3s/b/05E9dPZHDUYFp90cplX9faXUlOTaZakWrM2zGPtQfXOvdVCKnA4AcGE/tbLLVCazF29ViX9057fBoztsxgWL1htI1ty8XUi8590VWjCQkMIdg/mC3HtrBw17XnesPyhjGl2RQmrp9Ii4otGLR0kMt5JzSawLwd8xjQYgCLc/XikaYXObS2HgdXNuTcwXC++AK++ALyF7pIoboLKNXkG3IXOGP8GhFRLIKYbTHZ/hpRMKAgaxLW6BqRQ68RY1aNodu93bL1NSIhMYHmMc3tv0ZEDqDXwl4cPX/Uua9ZuWbUCq1F4uVETiWfcpmFUdeIa3L6f0dczRjoGnFVTrpGXJXdrhG+qbfeuhmdXfPIkSMUK1aMdevWERkZ6dw+aNAgVq5cyU8//XTDe8qXL8+lS5fYt28fPv+/gNXEiRN5/fXXOXrU+gPas2cPXbp0YdWqVfj4+HDPPfdQvnx5Nm3axPY/PDyTmppKq1atOHToECtWrPjTJu/y5ctcvnzZ+XtSUhLFixfPVrNrrklYwwMlHjBdhrg55ezPORzw88/W7JyffgonT1rb/fygY0drQfby5Y2WmCMoY2I3ZUzspoyJHXLM7JqFChXCx8eH48ePu2w/fvw4oaGhN31PWFgY5cuXdzZ4AJUqVeLYsWOkpKQAUKZMGVauXMn58+c5ePAgcXFxpKamUrp0aZdzpaam0qZNGw4cOMDSpUv/8g/L39+foKAgl1d2c/3fnonYRTn7c15eUKsWTJoEhw9bt3NGRUFKCrz/PlSsaE3m8jcTCHs8ZUzspoyJ3ZQxMc1ok+fn50ft2rVZtmyZc1t6ejrLli1zGdm7XlRUFLt37yY9Pd25befOnYSFheHn5+dybJ48eQgLC+PMmTMsXryYFi1aOPddbfB27drF999/T0hISCZ/u6yXkKhJY8R+ytmt8fODli1hzRrr9cQT1kjfV19Zk7Q8/DAsXmxtE1fKmNhNGRO7KWNimvHZNQcMGMD06dP5+OOP2b59O927d+fChQt07mytL9KxY0deeukl5/Hdu3fn9OnT9O3bl507d7JgwQLGjRtHz549nccsXryYRYsWsW/fPpYuXcpDDz1ExYoVnedMTU2ldevWbNy4kU8//ZS0tDSOHTvmMhqYE11/T7qIXZSz2xcVBfPnw7Zt1m2bvr6wfDk8+qg1kUtMjCZpuZ4yJnZTxsRuypiYZrzJa9u2LW+88QYjRoygZs2a/PzzzyxatMg5GUtCQoLzWTuA4sWLs3jxYjZs2ED16tXp06cPffv2ZciQIc5jEhMT6dmzJxUrVqRjx4488MADLF68mFy5rAlSDh8+zPz58zl06BA1a9YkLCzM+Vq3bl3W/gGIiMeoUsV6Xm/PHujXD/LksZ7he/pp61m9d96B5GTTVYqIiEhOZ3R2zat69epFr169brpvxYoVN2yLjIzkxx9//NPztWnThjZt2vzp/vDwcAzONyMiHq5ECXjrLRg+3Grs3n4b9u2DXr1g1Cjo0wd69rTW4hMRERG5XcZH8iTzBPlnv8lgxP0oZ5mnYEGr0TtwAKZMgfBwa0bOESOsRnDAADh48G9P43aUMbGbMiZ2U8bENKNLKORktzOFqYjIrbhyBb78EsaPh19+sbb5+kL79jBoEFSubLY+ERERMSfHLKEgmWvMqjGmSxAPoJzZx9cXoqNhyxb47jt46CGr8fv4Y+t5vubNYe3avz9PTqeMid2UMbGbMiamaSQvg7LjSF5CYgIlgkuYLkPcnHKWteLirJG9uXOvLbcQFQWDB8Njj4G3G/5VnTImdlPGxG7KmNhBI3keqtfCm09eI5KZlLOsFRFhLaq+fTs8/7y1/t7atdaoXvXqMHMmpKaarjJzKWNiN2VM7KaMiWlq8kREcoAKFWD6dGsWzkGDICgI/vc/6NQJypSBSZPg/HnTVYqIiEh2oCZPRCQHKVrUun0zIQH+/W8IDbVm4Ozf35qRc8QI+P1301WKiIiISWryRERyoOBg67m8ffvgvfegbFk4cwZefRVKlrTW3Nu3z3SVIiIiYoImXsmg7Djxyuajm7kn7B7TZYibU86yp7Q0a3KW8eNh40Zrm48PtGljNYM1apit73YoY2I3ZUzspoyJHTTxiodam+ABc6uLccpZ9uTjA61bW7NxLlsGjRtbjV9MDNSsCY8+CsuXX5uhMztTxsRuypjYTRkT0zSSl0HZcSQv8VIiwbmDTZchbk45yzm2bIEJE+CLLyA93dp2330wZAi0aGE1htmRMiZ2U8bEbsqY2EEjeR5q5IqRpksQD6Cc5Ry1alkjebt2QY8ekDs3bNgArVpZi6svX266wptTxsRuypjYTRkT09TkuZG9Z/aaLkE8gHKW85QuDe+8AwcOwLBhUKAAxMfDww/DgAFw8aLpCl0pY2I3ZUzspoyJaWryREQ8ROHC1uyb+/fDiy9a2956C2rXhk2bjJYmIiIimUhNnoiIhwkKgnffhQULrHX2tm+H+++3GsArV0xXJyIiIndKTZ4bqV+yvukSxAMoZ+6jWTPYutWalfPKFWsh9ago61ZOk5QxsZsyJnZTxsQ0NXlupHGZxqZLEA+gnLmXQoWs2Tc/+cRaYD0uzpqwZcqUazNyZjVlTOymjIndlDExTU2eG9lzZo/pEsQDKGfux8sL2re3RvUaNrQmYund21pb79ChrK9HGRO7KWNiN2VMTFOTJyIiABQvDosXw+TJEBAAS5dC1arw6ac5YxF1ERERsajJcyMztswwXYJ4AOXMvXl7Q69e1kLqERGQmAjPPANt28KpU1lTgzImdlPGxG7KmJimJk9ERG5QoQKsXQuvvAK+vvDll9ao3sKFpisTERGRv6MmT0REbsrXF4YPh/XroWJFOHYMHnsMunaF8+dNVyciIiJ/Rk2eiIj8pXvvhc2boV8/6/f33oOaNWHdOpNViYiIyJ/xcjj0OH1GJCUlERwcTGJiIkFBQabLERHJEsuXQ6dOcPCg9fzeoEEwahT4+5uuTERExL3dTv+hkTw3MmbVGNMliAdQzjzbQw9ZSy107Gito/fvf1sTtGzdmnmfoYyJ3ZQxsZsyJqZpJC+DsuNIXuKlRIJzB5suQ9ycciZXffWV9XzeyZPg5wevvgr//Cf4+NzZeZUxsZsyJnZTxsQOGsnzUB3mdjBdgngA5UyuatkStm2DJ56AlBQYPBgaNIC9e+/svMqY2E0ZE7spY2KamjwREcmwIkVg3jz44APImxfWrIEaNeD997WAuoiIiClq8kRE5I54eUGXLvDrr/Dgg9byCi+8AM2bW8suiIiISNZSkyciIpmiVClr9s3XX7ee0fv2W2sB9TlzTFcmIiLiWdTkuZEBkQNMlyAeQDmTv+LjA//6F2zaZN22eeoUtG5tzcZ59uytnUMZE7spY2I3ZUxMU5PnRhISE0yXIB5AOZNbUbUqxMXByy9b6+nNmgXVq8OyZX//XmVM7KaMid2UMTFNTZ4baRDewHQJ4gGUM7lVfn4wdiysXg1lylgLqDdsCH37wsWLf/4+ZUzspoyJ3ZQxMU1Nnhv5YPMHpksQD6Ccye2qWxd+/hm6dbN+f/ttuOce2LDh5scrY2I3ZUzspoyJaWry3MiWY1tMlyAeQDmTjMibF6ZOhYULISwMduyAyEgYNQpSU12PVcbEbsqY2E0ZE9PU5ImISJZp2hS2boU2bSAtDUaPtkb6duwwXZmIiIj7UJMnIiJZKiQEZs+Gzz6D/Plh40aoVcu6jTM93XR1IiIiOZ+aPDdSK7SW6RLEAyhnklmio2HbNmjcGC5dsiZkadwYyvjUN12auDldx8RuypiY5uVwOBymi8iJkpKSCA4OJjExkaCgINPlANZ0vSWCS5guQ9ycciaZzeGwntf717+sWTcLhqQx+3MfGjY0XZm4K13HxG7KmNjhdvoPjeS5kRX7V5guQTyAciaZzcsLevSwZuCsVQtOn/KhcWMYM0a3b4o9dB0TuyljYpqaPDeivzGSrKCciV3Kl4d16+CxtkdwOGD4cHjiCTh92nRl4m50HRO7KWNimpo8NzJx/UTTJYgHUM7ETrlzg3eLbsyYYf28cKG1pt7GjaYrE3ei65jYTRkT09TkiYhIttO5M6xfD6VLw4EDEBUF771nPb8nIiIif01NnoiIZEs1a8KmTdC8OaSkQNeuVvOXnGy6MhERkexNTZ6IiGRb+fPD3Lnw73+Dtzd8/DFERsKuXaYrExERyb60hEIGZcclFBIvJRKcO9h0GeLmlDOx259lbPlyaNcOTpyAoCD46CN46qmsr09yPl3HxG7KmNhBSyh4qMlxk02XIB5AORO7/VnGHnoItmyxns9LSoKWLWHQILhyJYsLlBxP1zGxmzImpmkkL4Oy40ieiIgnSE2FIUNg4v9PXlevHnz+OYSFma1LRETEThrJ81DNY5qbLkE8gHImdvu7jOXKBW++CV9+CfnywapV1jILq1ZlUYGS4+k6JnZTxsQ0NXkiIpIjtW5trZ9XtSocOwYPPwxvvKFlFkRERNTkiYhIjlW+PPz4IzzzDKSlwcCB0KoVJCaarkxERMQcNXkiIpKj5ckDM2fCf/8Lfn7Wkgv33gu//mq6MhERETPU5LmRLrW6mC5BPIByJnbLSMa8vKB7d1izBkqUgN274f77reZP5I90HRO7KWNimpo8ERFxG/fdB5s3w6OPwsWL0KkTdO0Kly6ZrkxERCTrqMlzI2UKlDFdgngA5UzsdqcZCwmBBQtg9GhrhO+99+CBB2D//sypT3I+XcfEbsqYmKYmz40s2bPEdAniAZQzsVtmZMzbG0aMgO++g4IFYdMma5mFhQszoUDJ8XQdE7spY2Kamjw3svLAStMliAdQzsRumZmxJk1gyxaIiIAzZ+Cxx6zmLy0t0z5CciBdx8RuypiYpiZPRETcWokS1kLpPXpYv7/6KjRtCr//brYuERERu6jJExERt+fvD++8A59+CoGBsHSpdfvmjz+arkxERCTzqclzI6ULlDZdgngA5UzsZmfGnn4a4uKsRdQPHYJ69WDKFHA4bPtIyYZ0HRO7KWNimpfDoX+1ZURSUhLBwcEkJiYSFBRkuhwAEi8lEpw72HQZ4uaUM7FbVmQsKQmeew5iY63f27WD6dMhb15bP1ayCV3HxG7KmNjhdvoPjeS5kZm/aNVfsZ9yJnbLiowFBcEXX8Bbb4GvL3z+uTU5y/bttn+0ZAO6jondlDExTU2eG4kqEWW6BPEAypnYLasy5uUF/frBihVQtKjV4N13n9X8iXvTdUzspoyJaWry3MioFaNMlyAeQDkTu2V1xqKiYPNmeOghuHAB2ra1mr+UlCwtQ7KQrmNiN2VMTMsWTd4777xDeHg4uXPnpk6dOsTFxf3l8WfPnqVnz56EhYXh7+9P+fLlWXjdCrfnzp2jX79+lCxZkoCAAOrWrcuGDRtczuFwOBgxYgRhYWEEBATQsGFDdu3aZcv3ExGR7K1IEViyBF56yfr9P/+BBg2syVlERERyGuNN3uzZsxkwYAAjR45k8+bN1KhRgyZNmnDixImbHp+SkkKjRo3Yv38/sbGxxMfHM336dIoVK+Y85vnnn2fp0qXMmjWLrVu30rhxYxo2bMjhw4edx0yYMIG3336badOm8dNPP5EnTx6aNGnCpUuXbP/OIiKS/fj6wrhxMG8eBAfD+vXWMgvLlpmuTERE5PYYb/ImTpzICy+8QOfOnalcuTLTpk0jMDCQGTNm3PT4GTNmcPr0ab7++muioqIIDw+nfv361KhRA4CLFy8yZ84cJkyYQL169ShbtiyjRo2ibNmyTJ06FbBG8SZNmsSwYcNo0aIF1atXZ+bMmRw5coSvv/46q766iIhkQ82bw6ZNULOmtWB648Ywdiykp5uuTERE5NYYbfJSUlLYtGkTDRs2dG7z9vamYcOGrF+//qbvmT9/PpGRkfTs2ZMiRYpQtWpVxo0bR1paGgBXrlwhLS2N3Llzu7wvICCANWvWALBv3z6OHTvm8rnBwcHUqVPnTz/38uXLJCUlubyymynNppguQTyAciZ2yw4ZK1MG1q2zlllIT4dhw6BFCzhzxnRlkhmyQ8bEvSljYpqvyQ8/efIkaWlpFClSxGV7kSJF2LFjx03fs3fvXn744Qfat2/PwoUL2b17Nz169CA1NZWRI0eSL18+IiMjefXVV6lUqRJFihQhJiaG9evXU7ZsWQCOHTvm/Jw/fu7VfX/02muvMXr06Bu2t/2yLbkCcwEQkCuA2a1nM2bVGLrU6kK3b7u5HDv0waFsObaF1pVbM37NeOJPxTv3RRWPokXFFmw9vpXAXIG8u+ldl/fOaTOH8WvHMyByAO1i27ns6xXRi1PJp9hwZAMFAwoSd/jaM41VC1elV0Qvvtv1HRUKVWDC2gku753RYgbTNk5jWL1htJzdkivpV5z7OlTvQGCuQMLyhbE2YS1L9y517iseVJzXG7/O1A1TaVymMUN/GOpy3reavEXMthiG1RtG53mdOZV8yrmvRYUWVChUgSvpV0hITCD2t1jnviD/ID5p+QljVo2hY42O9FrYy+W8oxqMYm3CWjrW6MjIFSPZe2avc1/9kvVpXKYxe87ssb7bFtfR4PnR8xmzagy9I3rTYW4Hl30DIgeQkJhAg/AGfLD5A7Yc2+LcVyu0Fs/d8xwr9q+gRHAJJq6f6PLeWU/NYnLcZIbVG0bzmOYu+7rU6gJAmQJlWLJnCSsPrHTuK12gNKMbjGbmLzOJKhF1w0PaU5pNYeYvMxlWbxjPfPUMSZev/cVC68qtKRFcAl9vX+JPxjMvfp5zX0hgCB+2+JAxq8YQXTWa/ov7u5x37MNjWbJnCd3v687AJQM5mHTQua9R6UZElYji6LmjJKcmM+vXWc59vt6+fNX2K7rM68LnrT+ny7wuLucdFDWI+JPxNC3XlClxU9h2YptzX0SxCKKrRhN3OI6QwBCmxLn+y+/z1p8zcf1EBkcNptUXrVz2da3dleTUZKoVqca8HfNYe3Ctc1+FkAoMfmAwsb/FUiu0FmNXj3V577THpzFjywyG1RtG29i2XEy96NwXXTWakMAQgv2D2XJsCwt3XXuuNyxvGFOaTWHi+om0qNiCQUsHuZx3QqMJzNsxjwGRA+i1sBdHzx917mtWrhm1QmuReDmRU8mniNkW49xn+hoRUSyCmG0x2f4a4ePlwz/r/jN7XCMegRo+Dfnto158+603Ne9J5dGX3+do3u+c79M1wnL1GjFm1Ri63dstW18jdp7aSfmQ8rpG/L+cdo3ICf8dMft/sykfUh7QNeKqnHSNuCq7XSN8U2+9dTO6GPqRI0coVqwY69atIzIy0rl90KBBrFy5kp9++umG95QvX55Lly6xb98+fHx8AOuWz9dff52jR60/oD179tClSxdWrVqFj48P99xzD+XLl2fTpk1s376ddevWERUVxZEjRwgLC3Oeu02bNnh5eTF79uwbPvfy5ctcvnzZ+XtSUhLFixfPVouhi4iIPbZsgVatYN8+yJ0bpk6FZ581XZWIiHiSHLMYeqFChfDx8eH48eMu248fP05oaOhN3xMWFkb58uWdDR5ApUqVOHbsGCn/P991mTJlWLlyJefPn+fgwYPExcWRmppK6dKlAZznvp3P9ff3JygoyOWV3Tzz1TOmSxAPoJyJ3bJjxmrVsp7Ta9YMLl2Czp2ha1frZ8l5smPGxL0oY2Ka0SbPz8+P2rVrs+y6qcvS09NZtmyZy8je9aKioti9ezfp1z0Bv3PnTsLCwvDz83M5Nk+ePISFhXHmzBkWL15MixYtAChVqhShoaEun5uUlMRPP/30p5+bE1w/DC9iF+VM7JZdM1agAHzzDbzyirWQ+nvvwYMPwoEDpiuT25VdMybuQxkT04zPrjlgwACmT5/Oxx9/zPbt2+nevTsXLlygc+fOAHTs2JGXri5cBHTv3p3Tp0/Tt29fdu7cyYIFCxg3bhw9e/Z0HrN48WIWLVrEvn37WLp0KQ899BAVK1Z0ntPLy4t+/foxZswY5s+fz9atW+nYsSNFixblySefzNLvLyIiOYe3NwwfDt99BwULwsaN1jILixebrkxEROQaoxOvALRt25bff/+dESNGcOzYMWrWrMmiRYuck6IkJCTg7X2tFy1evDiLFy+mf//+VK9enWLFitG3b18GDx7sPCYxMZGXXnqJQ4cOUbBgQVq1asXYsWPJlSuX85hBgwZx4cIFXnzxRc6ePcsDDzzAokWLbpiVU0RE5I+aNIHNm6F1a6vRa9oURo2yZuH0Nv7XpyIi4umMTrySk93Og49ZZeYvM+lYo6PpMsTNKWdit5yUscuXoW9fePf/JzJs1gxmzbJG+ST7ykkZk5xJGRM75JiJVyRzlQguYboE8QDKmdgtJ2XM3x+mTYMPP7Rm3Vy4EGrXtmbjlOwrJ2VMciZlTExTk+dGfL2N330rHkA5E7vlxIw9+yysXw+lS8P+/RAZCTNm/N27xJScmDHJWZQxMU1NnhuJPxn/9weJ3CHlTOyWUzNWs6b1fN7jj1u3cT73HLzwgpZZyI5yasYk51DGxDQ1eW5kXvw80yWIB1DOxG45OWMFCsC8eTBmjLXMwvvvwwMPWKN7kn3k5IxJzqCMiWlq8kRERDKRtzcMHWotqxASYi2iXrs2LFpkujIREfEUavJERERs0KiRtczCfffB6dPWzJujR0N6uunKRETE3anJcyMhgSGmSxAPoJyJ3dwpYyVKwOrV0K0bOBzWWnqPPWY1fWKOO2VMsidlTEzTOnkZlB3XyRMRkexr5kzo2tWaiKVkSZgzx7qNU0RE5FZonTwPNWbVGNMliAdQzsRu7pqxjh3hxx+hTBk4cACioqyJWSTruWvGJPtQxsQ0NXluJLpqtOkSxAMoZ2I3d85YjRrWMgtPPGEts/DCC9ZSCxcvmq7Ms7hzxiR7UMbENDV5bqT/4v6mSxAPoJyJ3dw9Y/nzw9dfw9ix1kycM2ZYo3r79pmuzHO4e8bEPGVMTFOTJyIiksW8veHll61lFgoVgi1brOfzFi40XZmIiLgDNXkiIiKGNGxoLbMQEQFnzlgzb44cCWlppisTEZGcTE2eiIiIQcWLw6pV0KOH9fsrr1jN3qlTZusSEZGcS0soZFB2XEJh6/GtVCtSzXQZ4uaUM7GbJ2ds1ixrmYWLF61lFmJj4d57TVflfjw5Y5I1lDGxg5ZQ8FBL9iwxXYJ4AOVM7ObJGevQwVpmoWzZa8ssTJ9uLaQumceTMyZZQxkT0zSSl0HZcSQvOTWZwFyBpssQN6ecid2UMTh7Fp59FubNs37v3BneeQcCAkxW5T6UMbGbMiZ20Eiehxq4ZKDpEsQDKGdiN2XMWmbhq6/gtdesmTg//BDq1oW9e01X5h6UMbGbMiamqclzIweTDpouQTyAciZ2U8Ys3t4wZAgsWQJ33QU//2wts/Dtt6Yry/mUMbGbMiamqckTERHJxh55xFpm4f77rds4n3gChg/XMgsiIvLn1OSJiIhkc3ffDStXQs+e1u9jxkCzZnDypNm6REQke1KT50YalW5kugTxAMqZ2E0Zuzk/P5gyBT75xJqAZckS6/bNDRtMV5bzKGNiN2VMTFOT50aiSkSZLkE8gHImdlPG/lr79vDTT9YyCwkJ8MADsGiR6apyFmVM7KaMiWlq8tzI0XNHTZcgHkA5E7spY3+vWjXYuBEefxxSUuDJJ2HZMtNV5RzKmNhNGRPT1OS5keTUZNMliAdQzsRuytitCQ6GOXOsiVguX7b+uXKl6apyBmVM7KaMiWlq8tzIrF9nmS5BPIByJnZTxm6dnx98+SU8+ihcvAiPPQbr1pmuKvtTxsRuypiYpiZPREQkB/P3txZOf+QRuHABmjbVZCwiIp5OTZ6IiEgOFxAA8+ZBvXqQlASNG8OWLaarEhERU9TkuRFfb1/TJYgHUM7EbspYxuTJA99+C5GR1qLpDRvC1q2mq8qelDGxmzImpnk5HA6H6SJyoqSkJIKDg0lMTCQoKMh0OSIiIgAkJkKjRtYtm3fdBStWQOXKpqsSEZE7dTv9h0by3MiYVWNMlyAeQDkTuyljdyY4GBYvhlq14PffrWf1du40XVX2ooyJ3ZQxMU0jeRmUHUfyTiafpFBgIdNliJtTzsRuyljmOHkSHn7YumWzWDFYtQpKlzZdVfagjIndlDGxg0byPFSXeV1MlyAeQDkTuyljmaNQIfj+e6hUCQ4fthq+AwdMV5U9KGNiN2VMTFOTJyIi4qYKF4Zly6BcOavBe/hhOHTIdFUiImI3NXkiIiJuLCwMfvjBulVz717rGb2jR01XJSIidlKTJyIi4ubuvttq9EqUsCZhadgQTpwwXZWIiNhFTZ4bGRQ1yHQJ4gGUM7GbMmaPkiWtRq9YMfjtN6vRO3XKdFVmKGNiN2VMTFOT50biT8abLkE8gHImdlPG7FOmjPWMXmioNetmo0Zw5ozpqrKeMiZ2U8bENDV5bqRpuaamSxAPoJyJ3ZQxe1WoYDV6d90FW7bAo49CUpLpqrKWMiZ2U8bENDV5bmRK3BTTJYgHUM7EbsqY/SpXtpZXKFgQ4uKgaVM4f950VVlHGRO7KWNimpo8N7LtxDbTJYgHUM7EbspY1qheHZYuhfz5Yd06ePxxSE42XVXWUMbEbsqYmKYmT0RExEPdcw8sXgz58sHKldCiBVy6ZLoqERG5U2ryREREPFhEBHz3HeTJY93C2bIlXL5suioREbkTavLcSESxCNMliAdQzsRuyljWi4qCBQsgIMBq+Nq0gZQU01XZRxkTuyljYpqaPDcSXTXadAniAZQzsZsyZkb9+jB/Pvj7W/98+mm4csV0VfZQxsRuypiYpibPjcQdjjNdgngA5UzspoyZ07AhzJ0Lfn4wZw507AhpaaarynzKmNhNGRPT1OS5kZDAENMliAdQzsRuyphZTZvCl1+Cry/ExMBzz0F6uumqMpcyJnZTxsQ0NXluRGuySFZQzsRuyph5zZvD55+Djw98/DF06+ZejZ4yJnZTxsQ0NXkiIiJyg1atYNYs8PaG6dOhTx9wOExXJSIit0JNnoiIiNxUdDTMmAFeXvDOO/DPf6rRExHJCdTkiYiIyJ/q1Anee8/6+a234KWX1OiJiGR3Xg6HLtUZkZSURHBwMImJiQQFBZkuB4Dk1GQCcwWaLkPcnHImdlPGsqf//hd69rR+HjECRo82W8+dUMbEbsqY2OF2+g+N5LmRiesnmi5BPIByJnZTxrKnHj2skTyAV16BcePM1nMnlDGxmzImpmkkL4Oy40healoquXxymS5D3JxyJnZTxrK38eNhyBDr59dfh3/9y2w9GaGMid2UMbGDRvI8VKsvWpkuQTyAciZ2U8ayt8GDrZE8gIED4e23zdaTEcqY2E0ZE9PU5ImIiMhtGT4chg61fu7bF95912w9IiLiSk2eiIiI3LZXX7VG8sBaLH3GDLP1iIjINWryRERE5LZ5eVnP5/Xta/3+/PPwySdmaxIREYuaPDfStXZX0yWIB1DOxG7KWM7h5WXNuNmtm7V2XqdO8MUXpqv6e8qY2E0ZE9PU5LmR5NRk0yWIB1DOxG7KWM7i5QXvvANdukB6Ojz9NMyda7qqv6aMid2UMTFNTZ4bqVakmukSxAMoZ2I3ZSzn8faG996DDh0gLQ3atoVvvzVd1Z9TxsRuypiYpibPjczbMc90CeIBlDOxmzKWM/n4WJOvtG0LqanQqhUsXGi6qptTxsRuypiYpibPjaw9uNZ0CeIBlDOxmzKWc/n6wqxZ0LIlpKTAk0/CnDmmq7qRMiZ2U8bENDV5IiIikmly5YLPP782otemDcycaboqERHPoiZPREREMlWuXPDpp9cmY+nUCaZONV2ViIjnMN7kvfPOO4SHh5M7d27q1KlDXFzcXx5/9uxZevbsSVhYGP7+/pQvX56F1930n5aWxvDhwylVqhQBAQGUKVOGV199FYfD4Tzm/Pnz9OrVi7vvvpuAgAAqV67MtGnTbPuOWaVCSAXTJYgHUM7EbsqYe/DxgenToU8f6/cePeD1183WdJUyJnZTxsQ0X5MfPnv2bAYMGMC0adOoU6cOkyZNokmTJsTHx1O4cOEbjk9JSaFRo0YULlyY2NhYihUrxoEDB8ifP7/zmPHjxzN16lQ+/vhjqlSpwsaNG+ncuTPBwcH0+f9/0wwYMIAffviBTz75hPDwcJYsWUKPHj0oWrQozZs3z6qvn+kGPzDYdAniAZQzsZsy5j68vWHSJMibF8aNg0GD4Nw5GD3aWnrBFGVM7KaMiWlGR/ImTpzICy+8QOfOnZ2jaYGBgcyYMeOmx8+YMYPTp0/z9ddfExUVRXh4OPXr16dGjRrOY9atW0eLFi147LHHCA8Pp3Xr1jRu3NhlhHDdunV06tSJBg0aEB4ezosvvkiNGjX+dhQxu4v9LdZ0CeIBlDOxmzLmXry8YOxYq8kDePVV+Oc/rcXTTVHGxG7KmJhmrMlLSUlh06ZNNGzY8Fox3t40bNiQ9evX3/Q98+fPJzIykp49e1KkSBGqVq3KuHHjSEtLcx5Tt25dli1bxs6dOwH45ZdfWLNmDU2bNnU5Zv78+Rw+fBiHw8Hy5cvZuXMnjRs3/tN6L1++TFJSkssru6kVWst0CeIBlDOxmzLmnl56CSZPtn5+6y3o2tVaU88EZUzspoyJacZu1zx58iRpaWkUKVLEZXuRIkXYsWPHTd+zd+9efvjhB9q3b8/ChQvZvXs3PXr0IDU1lZEjRwIwZMgQkpKSqFixIj4+PqSlpTF27Fjat2/vPM/kyZN58cUXufvuu/H19cXb25vp06dTr169P633tddeY/To0Tdsb/tlW3IF5gIgIFcAs1vPZsyqMXSp1YVu33ZzOXbog0PZcmwLrSu3Zvya8cSfinfuiyoeRYuKLdh6fCuBuQJ5d9O7Lu+d02YO49eOZ0DkANrFtnPZ1yuiF6eST/H+5vd5qNRDxB2+NiJZtXBVekX04rtd31GhUAUmrJ3g8t4ZLWYwbeM0htUbRsvZLbmSfsW5r0P1DgTmCiQsXxhrE9aydO9S577iQcV5vfHrTN0wlcZlGjP0h6Eu532ryVvEbIthWL1hdJ7XmVPJp5z7WlRoQYVCFbiSfoWExASXv+0K8g/ik5afMGbVGDrW6Eivhb1czjuqwSjWJqylY42OjFwxkr1n9jr31S9Zn8ZlGrPnzB7ru21xHRGeHz2fMavG0DuiNx3mdnDZNyByAAmJCTQIb8AHmz9gy7Etzn21Qmvx3D3PsWL/CkoEl2Di+oku75311Cwmx01mWL1hNI9xvd23S60uAJQpUIYle5aw8sBK577SBUozusFoZv4yk6gSUYxaMcrlvVOaTWHmLzMZVm8Yz3z1DEmXr/3FQuvKrSkRXAJfb1/iT8YzL/7aejwhgSF82OJDxqwaQ3TVaPov7u9y3rEPj2XJniV0v687A5cM5GDSQee+RqUbEVUiiqPnjpKcmsysX2c59/l6+/JV26/o9HUn1nRZQ5d5XVzOOyhqEPEn42larilT4qaw7cQ2576IYhFEV40m7nAcIYEhTImb4vLez1t/zsT1ExkcNZhWX7Ry2de1dleSU5OpVqQa83bMc5mWukJIBQY/MJjY32KpFVqLsavHurx32uPTmLFlBsPqDaNtbFsupl507ouuGk1IYAjB/sFsObaFhbuuPdsbljeMKc2mMHH9RFpUbMGgpYNczjuh0QTm7ZjHgMgB9FrYi6Pnjzr3NSvXjFqhtUi8nMip5FPEbItx7jN9jYgoFkHMtphsf41ISExgSrMpukbk0GvEmFVj6HZvt5teIwLqxjPxvy35Z69gpk/3ZuH/VlCz23+4v2TtLL1GxB2OI6JYhK4R/y+nXSNywn9HvLvpXSKKRQC6Rlx1K9cI/XfEX18jfFNvo3VzGHL48GEH4Fi3bp3L9oEDBzoiIiJu+p5y5co5ihcv7rhy5Ypz25tvvukIDQ11/h4TE+O4++67HTExMY5ff/3VMXPmTEfBggUdH330kfOY119/3VG+fHnH/PnzHb/88otj8uTJjrx58zqWLl36p/VeunTJkZiY6HwdPHjQATgSExMz+keQ6Z747AnTJYgHUM7EbsqY+5s92+Hw9XU4wOFo0cLhuHQpaz9fGRO7KWNih8TExFvuP4yN5BUqVAgfHx+OHz/usv348eOEhobe9D1hYWHkypULHx8f57ZKlSpx7NgxUlJS8PPzY+DAgQwZMoR27ay/papWrRoHDhzgtddeo1OnTly8eJGXX36ZuXPn8thjjwFQvXp1fv75Z9544w2X20ev5+/vj7+/f2Z8dREREY/Wpg0EBkLr1jBvHjRvDnPnWttEROTOGXsmz8/Pj9q1a7Ns2TLntvT0dJYtW0ZkZORN3xMVFcXu3btJT093btu5cydhYWH4+fkBkJycjLe369fy8fFxvic1NZXU1NS/PEZERETs9fjjsGAB5MkDS5bAo49CNnzcXUQkRzI6u+aAAQOYPn06H3/8Mdu3b6d79+5cuHCBzp07A9CxY0deeukl5/Hdu3fn9OnT9O3bl507d7JgwQLGjRtHz549ncc88cQTjB07lgULFrB//37mzp3LxIkTeeqppwAICgqifv36DBw4kBUrVrBv3z4++ugjZs6c6Twmp5r2eM5f60+yP+VM7KaMeY5HHrEavOBgWL3a+v3Uqb9/351SxsRuypiYZrTJa9u2LW+88QYjRoygZs2a/PzzzyxatMg5GUtCQgJHj157ELF48eIsXryYDRs2UL16dfr06UPfvn0ZMmSI85jJkyfTunVrevToQaVKlfjXv/5F165defXVV53HfP7559x33320b9+eypUr8+9//5uxY8fSrZvrA845zR8fEBaxg3ImdlPGPEvdurB8ORQqBBs3QoMGcOyYvZ+pjIndlDExzcvhuPOVatLS0ti6dSslS5akQIECmVFXtpeUlERwcDCJiYkEBQWZLkdERCRH++03aNgQjh6FsmVh2TIoUcJ0VSIi2cft9B8ZGsnr168fH3zwAWA1ePXr1+eee+6hePHirFixIiOnlEzQNrat6RLEAyhnYjdlzDNVrmzdslmyJOzeDQ8+CLt22fNZypjYTRkT0zLU5MXGxlKjRg0AvvnmG/bt28eOHTvo378/Q4cO/Zt3i12uX7tDxC7KmdhNGfNcZcpYjV758pCQAPXqwbZtf/++26WMid2UMTEtQ03eyZMnncscLFy4kH/84x+UL1+eLl26sHXr1kwtUERERDxH8eKwahVUr249m1e/PmzaZLoqEZGcJUNNXpEiRfjtt99IS0tj0aJFNGrUCLCWL7h+DTsRERGR21WkiDUZS0QEnD4NDz8Ma9aYrkpEJOfIUJPXuXNn2rRpQ9WqVfHy8nIuIP7TTz9RsWLFTC1Qbl101WjTJYgHUM7EbsqYABQsCN9/b43kJSVBkyawdGnmnFsZE7spY2Jahpq8UaNG8f777/Piiy+ydu1a/P39AWtB8euXM5CsFRIYYroE8QDKmdhNGZOr8uWDhQuthdKTk60F1OfNu/PzKmNiN2VMTPPN6Btbt24NwKVLl5zbOnXqdOcVSYYF+webLkE8gHImdlPG5HqBgfD11/D00/DVV9CqFcyaBdF3MFCijIndlDExLUMjeWlpabz66qsUK1aMvHnzsnfvXgCGDx/uXFpBst6WY1tMlyAeQDkTuylj8kf+/jB7NnToAGlp0L49vP9+xs+njIndlDExLUNN3tixY/noo4+YMGECfn5+zu1Vq1bl/Tu56sodWbhroekSxAMoZ2I3ZUxuxtcXPvoIunUDhwNeeAEmTcrYuZQxsZsyJqZlqMmbOXMm7733Hu3bt3eZTbNGjRrs2LEj04oTERERucrbG/77Xxg40Pq9f38YO9Zq+kRE5JoMNXmHDx+mbNmyN2xPT08nNTX1josSERERuRkvLxg/Hl55xfp92DB46SU1eiIi18tQk1e5cmVWr159w/bY2Fhq1ap1x0VJxoTlDTNdgngA5UzspozJ3/HyguHD4c03rd/Hj4fevSE9/dber4yJ3ZQxMc3L4bj9v/uaN28enTp14qWXXuKVV15h9OjRxMfHM3PmTL799lvn4ujuLCkpieDgYBITEwkKCjJdDgCpaank8sllugxxc8qZ2E0Zk9vx3nvXntN79lmYPt16fu+vKGNiN2VM7HA7/UeGRvJatGjBN998w/fff0+ePHkYMWIE27dv55tvvvGIBi+7mrh+oukSxAMoZ2I3ZUxux4svWksq+PhYE7NER0NKyl+/RxkTuyljYlqGRvIke47k7Ti5g4qFKpouQ9ycciZ2U8YkI77+Gtq2tRq8Zs0gNhYCAm5+rDImdlPGxA62j+QdPHiQQ4cOOX+Pi4ujX79+vPfeexk5nWSSQUsHmS5BPIByJnZTxiQjnnwSvvnGauwWLoTHHoNz525+rDImdlPGxLQMNXlPP/00y5cvB+DYsWM0bNiQuLg4hg4dyitXp7sSERERyUKNG8PixZAvHyxfbv1+5ozpqkREsl6Gmrxt27YREREBwBdffEG1atVYt24dn376KR999FFm1iciIiJyyx58EJYtg4IF4ccf4eGH4fffTVclIpK1MtTkpaam4u/vD8D3339P8+bNAahYsSJHjx7NvOpEREREbtN998GKFVCkCPz8M9SrB4cPm65KRCTrZKjJq1KlCtOmTWP16tUsXbqURx99FIAjR44QEhKSqQXKrZvQaILpEsQDKGdiN2VMMkO1arB6NRQvDjt2WCN8+/ZZ+5QxsZsyJqZlqMkbP3487777Lg0aNCA6OpoaNWoAMH/+fOdtnJL15u2YZ7oE8QDKmdhNGZPMUq6c1eiVLWs1eA8+aDV8ypjYTRkT0zK8hEJaWhpJSUkUKFDAuW3//v0EBgZSuHDhTCswu8qOSyho4U3JCsqZ2E0Zk8x29Cg0agT/+x/cdRcsXJTKvfcoY2IfXcfEDrYvoXDx4kUuX77sbPAOHDjApEmTiI+P94gGL7vqtbCX6RLEAyhnYjdlTDJbWJj1jN4991iTsDzcNJELF0xXJe5M1zExLUNNXosWLZg5cyYAZ8+epU6dOrz55ps8+eSTTJ06NVMLlFt39LwmvRH7KWdiN2VM7FCokDXrZokScO5EIUaMMF2RuDNdx8S0DDV5mzdv5sEHHwQgNjaWIkWKcODAAWbOnMnbb7+dqQWKiIiIZIb8+eHdd62fJ02CDRtMViMiYp8MNXnJycnky5cPgCVLltCyZUu8vb25//77OXDgQKYWKCIiIpJZHn0Uij2wnPR0eO45SEkxXZGISObLUJNXtmxZvv76aw4ePMjixYtp3LgxACdOnMg2k5B4omblmpkuQTyAciZ2U8bEbv1GHqBQIdi6FSZopnuxga5jYlqGmrwRI0bwr3/9i/DwcCIiIoiMjASsUb1atWplaoFy62qF6s9e7Kecid2UMbHbgxUr8Z//WD+/+ips3262HnE/uo6JaRlq8lq3bk1CQgIbN25k8eLFzu2PPPIIb731VqYVJ7cn8XKi6RLEAyhnYjdlTOyWeDmR6Gho1sy6XfOFFyA93XRV4k50HRPTMtTkAYSGhlKrVi2OHDnCoUOHAIiIiKBixYqZVpzcnlPJp0yXIB5AORO7KWNit1PJp/DygqlTIW9eWLsWpk0zXZW4E13HxLQMNXnp6em88sorBAcHU7JkSUqWLEn+/Pl59dVXSddfhRkTsy3GdAniAZQzsZsyJna7mrESJeC116xtgwfDwYMGixK3ouuYmJahJm/o0KFMmTKFf//732zZsoUtW7Ywbtw4Jk+ezPDhwzO7RhERERFb9OgBdevC+fPQvTs4HKYrEhG5c74ZedPHH3/M+++/T/PmzZ3bqlevTrFixejRowdjx47NtAJFRERE7OLtDe+/DzVrwoIF8PnnEB1tuioRkTuToZG806dP3/TZu4oVK3L69Ok7LkoyJiBXgOkSxAMoZ2I3ZUzs9seMVaoEw4ZZP/fpAydPGihK3IquY2Kal8Nx+zcm1KlThzp16vD222+7bO/duzdxcXH89NNPmVZgdpWUlERwcDCJiYlaG1BERCSHS0mB2rVh2zbo0AFmzjRdkYiIq9vpPzI0kjdhwgRmzJhB5cqVee6553juueeoXLkyH330EW+88UaGipY7N2bVGNMliAdQzsRuypjY7WYZ8/Ozbtv08oJZs2DRIgOFidvQdUxMy1CTV79+fXbu3MlTTz3F2bNnOXv2LC1btuR///sfs2bNyuwa5RZ1qdXFdAniAZQzsZsyJnb7s4zVqQN9+1o/d+1qTcYikhG6jolpGV4nr2jRoowdO5Y5c+YwZ84cxowZw5kzZ/jggw8ysz65Dd2+7Wa6BPEAypnYTRkTu/1VxsaMgfBwSEi49pyeyO3SdUxMy3CTJyIiIuJu8uSBd9+1fn77bfjxR7P1iIhkhJo8ERERkes0bgydOllr5j3/vDUpi4hITqImT0REROQP3nwT7roL/vc/eO0109WIiNye21oMvWXLln+5/+zZs3dSi9yhoQ8ONV2CeADlTOymjIndbiVjISEweTK0awdjx0Lr1lClShYUJ25B1zEx7bZG8oKDg//yVbJkSTp27GhXrfI3thzbYroE8QDKmdhNGRO73WrG2rSBJ56A1FTrts20NJsLE7eh65iYlqHF0CV7LoZ+MvkkhQILmS5D3JxyJnZTxsRut5OxQ4egcmU4d86aiKV3b5uLE7eg65jYwfbF0CV7Gr9mvOkSxAMoZ2I3ZUzsdjsZu/tumDDB+vmll+DAAZuKErei65iYpibPjcSfijddgngA5UzspoyJ3W43Yy++CA8+CBcuQLdu1qybIn9F1zExTU2eiIiIyF/w9obp08HfHxYtgs8+M12RiMhfU5MnIiIi8jcqVIARI6yf+/aF3383W4+IyF9Rk+dGoopHmS5BPIByJnZTxsRuGc3YwIFQowacOgX9+mVuTeJedB0T09TkuZEWFVuYLkE8gHImdlPGxG4ZzViuXPD++9btm599BgsWZHJh4jZ0HRPT1OS5ka3Ht5ouQTyAciZ2U8bEbneSsXvvhf79rZ+7d7eWVhD5I13HxDQ1eW4kMFeg6RLEAyhnYjdlTOx2pxl75RUoXRoOHoSXX86kosSt6DompqnJcyPvbnrXdAniAZQzsZsyJna704wFBsJ771k/v/MOrFuXCUWJW9F1TExTkyciIiJymx55BLp0sdbMe/55uHzZdEUiIteoyRMRERHJgDfegCJFYPt2GDvWdDUiIteoyRMRERHJgAIFYMoU6+fXXoOtmmtDRLIJL4fD4TBdRE6UlJREcHAwiYmJBAUFmS4HgNS0VHL55DJdhrg55UzspoyJ3TIzYw4HtGwJX38NderA2rXg45Mpp5YcTNcxscPt9B8ayXMj49eON12CeADlTOymjIndMjNjXl7W5CvBwfDTTzB5cqadWnIwXcfENI3kZVB2HMlLTk3WlL1iO+VM7KaMid3syNj06fDii9bMm9u2QalSmXp6yWF0HRM7aCTPQ7WLbWe6BPEAypnYTRkTu9mRseeeg/r1ITkZuna1buMUz6XrmJimJk9ERETkDnl7W6N5uXPD0qUwa5bpikTEk6nJExEREckE5crBqFHWz/37w4kTRssREQ+mJk9EREQkk/zzn1CrFpw+DX36mK5GRDyVmjw30iuil+kSxAMoZ2I3ZUzsZmfGfH3h/fetZRRmz4ZvvrHtoyQb03VMTDPe5L3zzjuEh4eTO3du6tSpQ1xc3F8ef/bsWXr27ElYWBj+/v6UL1+ehQsXOvenpaUxfPhwSpUqRUBAAGXKlOHVV1/lj5OIbt++nebNmxMcHEyePHm47777SEhIsOU7ZpVTyadMlyAeQDkTuyljYje7M3bPPdaIHkD37pCYaOvHSTak65iYZrTJmz17NgMGDGDkyJFs3ryZGjVq0KRJE078yU3sKSkpNGrUiP379xMbG0t8fDzTp0+nWLFizmPGjx/P1KlTmTJlCtu3b2f8+PFMmDCBydctXLNnzx4eeOABKlasyIoVK/j1118ZPnw4uXPntv072ymiWITpEsQDKGdiN2VM7JYVGRs1CsqWhcOH4aWXbP84yWZ0HRPTjDZ5EydO5IUXXqBz585UrlyZadOmERgYyIwZM256/IwZMzh9+jRff/01UVFRhIeHU79+fWrUqOE8Zt26dbRo0YLHHnuM8PBwWrduTePGjV1GCIcOHUqzZs2YMGECtWrVokyZMjRv3pzChQvb/p3tFLMtxnQJ4gGUM7GbMiZ2y4qMBQRYs20CTJ0Kq1fb/pGSjeg6JqYZa/JSUlLYtGkTDRs2vFaMtzcNGzZk/fr1N33P/PnziYyMpGfPnhQpUoSqVasybtw40tLSnMfUrVuXZcuWsXPnTgB++eUX1qxZQ9OmTQFIT09nwYIFlC9fniZNmlC4cGHq1KnD119//Zf1Xr58maSkJJdXdhN3+K9vdRXJDMqZ2E0ZE7tlVcYaNIAXXrB+fv55uHQpSz5WsgFdx8Q0X1MffPLkSdLS0ihSpIjL9iJFirBjx46bvmfv3r388MMPtG/fnoULF7J792569OhBamoqI0eOBGDIkCEkJSVRsWJFfHx8SEtLY+zYsbRv3x6AEydOcP78ef79738zZswYxo8fz6JFi2jZsiXLly+nfv36N/3s1157jdGjR9+wve2XbckVmAuAgFwBzG49mzGrxtClVhe6fdvN5dihDw5ly7EttK7cmvFrxhN/Kt65L6p4FC0qtmDr8a0E5grk3U3vurx3Tps5jF87ngGRA25YYLNXRC9OJZ/iQsoFxqwa43JhqVq4Kr0ievHdru+oUKgCE9ZOcHnvjBYzmLZxGsPqDaPl7JZcSb/i3NehegcCcwUSli+MtQlrWbp3qXNf8aDivN74daZumErjMo0Z+sNQl/O+1eQtYrbFMKzeMDrP6+xyb3qLCi2oUKgCV9KvkJCYQOxvsc59Qf5BfNLyE8asGkPHGh3ptdD1weVRDUaxNmEtHWt0ZOSKkew9s9e5r37J+jQu05g9Z/ZY322L64jw/Oj5jFk1ht4Rvekwt4PLvgGRA0hITKBBeAM+2PwBW45tce6rFVqL5+55jhX7V1AiuAQT1090ee+sp2YxOW4yw+oNo3lMc5d9XWp1AaBMgTIs2bOElQdWOveVLlCa0Q1GM/OXmUSViGLUilEu753SbAozf5nJsHrDeOarZ0i6fO0vFlpXbk2J4BL4evsSfzKeefHznPtCAkP4sMWHjFk1huiq0fRf3N/lvGMfHsuSPUvofl93Bi4ZyMGkg859jUo3IqpEFEfPHSU5NZlZv15b6MnX25ev2n7FzlM7OZl8ki7zuricd1DUIOJPxtO0XFOmxE1h24ltzn0RxSKIrhpN3OE4QgJDmBI3xeW9n7f+nInrJzI4ajCtvmjlsq9r7a4kpyZTrUg15u2Yx9qDa537KoRUYPADg4n9LZZaobUYu3qsy3unPT6NGVtmMKzeMNrGtuVi6kXnvuiq0YQEhhDsH8yWY1tYuOvas71hecOY0mwKE9dPpEXFFgxaOsjlvBMaTWDejnkMiBxAr4W9OHr+qHNfs3LNqBVai8TLiZxKPuXyt7mmrxERxSKI2RaT7a8RCYkJrElYo2tEDr1GjFk1hm73dsvW14i4w3E0j2meJdeIlIcG4v/lcHbuDKFauy/457DTukbg/v8dcTVjoGvEVTnpGnFVdvvvCN/U22jdHIYcPnzYATjWrVvnsn3gwIGOiIiIm76nXLlyjuLFizuuXLni3Pbmm286QkNDnb/HxMQ47r77bkdMTIzj119/dcycOdNRsGBBx0cffeTyudHR0S7nfuKJJxzt2rX703ovXbrkSExMdL4OHjzoAByJiYm3/d3t8sRnT5guQTyAciZ2U8bEblmdsa++cjjA4fD1dTh+/jlLP1oM0XVM7JCYmHjL/YexkbxChQrh4+PD8ePHXbYfP36c0NDQm74nLCyMXLly4ePj49xWqVIljh07RkpKCn5+fgwcOJAhQ4bQrp31t1TVqlXjwIEDvPbaa3Tq1IlChQrh6+tL5cqVXc5dqVIl1qxZ86f1+vv74+/vn9GvmyWqFq5qugTxAMqZ2E0ZE7tldcaeegpatYI5c6zbNtevt5ZaEPel65iYZuyZPD8/P2rXrs2yZcuc29LT01m2bBmRkZE3fU9UVBS7d+8mPT3duW3nzp2EhYXh5+cHQHJyMt7erl/Lx8fH+R4/Pz/uu+8+4uPjXY7ZuXMnJUuWzJTvZorWZJGsoJyJ3ZQxsZuJjE2ZAvnzw8aN8J//ZPnHSxbTdUxMMzq75oABA5g+fToff/wx27dvp3v37ly4cIHOnTsD0LFjR166bt7h7t27c/r0afr27cvOnTtZsGAB48aNo2fPns5jnnjiCcaOHcuCBQvYv38/c+fOZeLEiTz11FPOYwYOHMjs2bOZPn06u3fvZsqUKXzzzTf06NEj6768Db7b9Z3pEsQDKGdiN2VM7GYiY6Gh8Oab1s/Dh8OePVlegmQhXcfENKM3C7Rt25bff/+dESNGcOzYMWrWrMmiRYuck7EkJCS4jMoVL16cxYsX079/f6pXr06xYsXo27cvgwcPdh4zefJkhg8fTo8ePThx4gRFixala9eujBgxwnnMU089xbRp03jttdfo06cPFSpUYM6cOTzwwANZ9+VtUKFQBdMliAdQzsRuypjYzVTGOneGTz+FH36AF1+E778HLy8jpYjNdB0T07wcDofDdBE5UVJSEsHBwSQmJhIUFGS6HACaxzRnfvR802WIm1POxG7KmNjNZMb27IFq1eDiRZgxw2r8xP3oOiZ2uJ3+w+jtmiIiIiKepEwZeOUV6+cBA+DYMbP1iIh7UpMnIiIikoX69YPateHsWejd23Q1IuKO1OSJiIiIZCFfX/jgA+ufsbHw9demKxIRd6Nn8jIoOz6TdzL5JIUCC5kuQ9ycciZ2U8bEbtklY0OHwrhxEBYGv/1mLbEg7iG7ZEzci57J81DTNk4zXYJ4AOVM7KaMid2yS8aGD4fy5eHoUbhuonBxA9klY+K5NJKXQdlxJE9ERERyllWroH596+cVK679LCLyRxrJ81AtZ7c0XYJ4AOVM7KaMid2yU8bq1YNu3ayf27aF116D48fN1iR3LjtlTDyTmjw3ciX9iukSxAMoZ2I3ZUzslt0yNn68ddvm8ePw8stQvDi0a2eN7Ol+q5wpu2VMPI+aPBERERGDgoLg55/ho4/g/vshNRVmz4aHHoJKlWDSJDh92nCRIpKjqMkTERERMSwgADp1gvXrYcsW6xbOvHkhPh7694dixeDZZ+HHHzW6JyJ/T02eG+lQvYPpEsQDKGdiN2VM7JbdM1azJkydCkeOwLRpUKMGXLoEH38MkZFQq5a1/dw505XKn8nuGRP3pybPjQTmCjRdgngA5UzspoyJ3XJKxvLlg65drZG9H3+0RvJy54ZffoHu3aFoUWvE7+efTVcqf5RTMibuS02eGwnLF2a6BPEAypnYTRkTu+W0jHl5QZ068OGH1ujepElQsSKcPw/vvmuN7N1/v/VMX3Ky6WoFcl7GxP2oyXMjaxPWmi5BPIByJnZTxsRuOTljBQpA377w22/W7Jvt2kGuXPDTT9C5s/XsXr9+sH276Uo9W07OmLgHNXluZOnepaZLEA+gnIndlDGxmztkzMvLWjg9JgYOHYJ//xtKlYKzZ+E//4HKla3ZOWfPhpQU09V6HnfImORsavJEREREcrDChWHwYNi9GxYtgiefBG/vayN9d98NL70Ee/earlREsoqaPBERERE34O0NTZrA3Llw4ACMHGlNzvL779ZIX9my0LQpzJsHV7RWt4hbU5PnRooHFTddgngA5UzspoyJ3TwhY3ffDaNGWc3e3LlW8+dwXBvpCw+H0aPh8GHDhbopT8iYZG9eDoeW1MyIpKQkgoODSUxMJCgoyHQ5ACSnJmvKXrGdciZ2U8bEbp6asb174b33YMYMa3QPwMcHnnjCWoqhUSNrNFDunKdmTOx1O/2H/q/sRqZumGq6BPEAypnYTRkTu3lqxkqXtm7bPHjQmrClfn1IS4Ovv4ZHH7Vu5xw/Hk6cMF1pzuepGZPsQ02eG2lcprHpEsQDKGdiN2VM7ObpGfP3tyZkWbHCWoqhb1/Inx/27YMhQ6xbPaOjYeVK6xZPuX2enjExT02eGxn6w1DTJYgHUM7EbsqY2E0Zu6ZSJWtx9cOHrcXW69SB1FT4/HNo0MBaimHKFE3UcruUMTFNTZ6IiIiIhwsMhGefhR9/hM2boWtXyJMHduyA3r2hbVu4fNl0lSJyq9TkiYiIiIhTrVowbRocOWKN8vn5wVdfweOPw/nzpqsTkVuhJk9EREREbhAUZD2vt3ChNar3/ffQsCGcPm26MhH5O1pCIYOy4xIKe07voUzBMqbLEDennIndlDGxmzJ2++LirIXUT5+GKlVgyRJroXW5OWVM7KAlFDxUzLYY0yWIB1DOxG7KmNhNGbt9ERGwerXV2P3vfxAVBbt3m64q+1LGxDSN5GVQdhzJExEREbHT/v3Woum7d0ORItaIXvXqpqsS8QwayfNQned1Nl2CeADlTOymjIndlLGMCw+3RvSqV4fjx60F1detM11V9qOMiWlq8tzIqeRTpksQD6Ccid2UMbGbMnZnQkOthdKjouDsWWsylsWLTVeVvShjYpqaPBERERG5LfnzW7dqPvooXLwITzwBX3xhuioRuUpNnoiIiIjctsBAmDfPWig9NRXatYP33jNdlYiAmjy30qJCC9MliAdQzsRuypjYTRnLPH5+8Omn0K0bOBzQtSuMH2+6KvOUMTFNTZ4bqVCogukSxAMoZ2I3ZUzspoxlLh8f+O9/4eWXrd+HDIHBg62mz1MpY2Kamjw3ciX9iukSxAMoZ2I3ZUzspoxlPi8vGDsWXn/d+n3CBHjxRUhLM1uXKcqYmKYmz40kJCaYLkE8gHImdlPGxG7KmH3+9S94/33w9rb+2a4dXL5suqqsp4yJaWry3Ejsb7GmSxAPoJyJ3ZQxsZsyZq/nnrNm2vTzg9hYa+bN8+dNV5W1lDExTU2eiIiIiGSqVq1gwQLIkweWLoVGjeD0adNViXgONXkiIiIikukaNoRly6BAAfjxR6hfH44eNV2ViGdQk+dGgvyDTJcgHkA5E7spY2I3ZSzr1KkDq1ZBWBhs2wZRUbBnj+mq7KeMiWleDocnT3CbcUlJSQQHB5OYmEhQkP6PLCIiIvJn9u2zbtncswdCQ2HJEqhWzXRVIjnL7fQfGslzI2NWjTFdgngA5UzspoyJ3ZSxrFeqFKxebTV2x45BvXqwfr3pquyjjIlpGsnLoOw4kpeQmECJ4BKmyxA3p5yJ3ZQxsZsyZs6ZM/DYY1aDFxgIc+dC48amq8p8ypjYQSN5HqrXwl6mSxAPoJyJ3ZQxsZsyZk6BAtZsm02aQHIyPP64tcyCu1HGxDQ1eSIiIiKSZfLkgfnzoU0bSE2Ftm1h+nTTVYm4FzV5IiIiIpKl/Pzgs8/gxRchPd3654QJpqsScR9q8kREREQky/n4wLRpMGSI9fvgwdbPmi1C5M5p4pUMyo4Tr2w+upl7wu4xXYa4OeVM7KaMid2UsexnwgSryQN44QWYOtVqAnMqZUzsoIlXPNTahLWmSxAPoJyJ3ZQxsZsylv0MGmQ9l+ftbf0zOhpSUkxXlXHKmJimkbwMyo4jeYmXEgnOHWy6DHFzypnYTRkTuylj2VdsLDz9tDUhS+PG8NVX1kQtOY0yJnbQSJ6HGrlipOkSxAMoZ2I3ZUzspoxlX61bw4IF1hp6S5ZAo0bW2no5jTImpqnJcyN7z+w1XYJ4AOVM7KaMid2UseytUSP4/ntrTb3166F+fTh61HRVt0cZE9PU5ImIiIhIthIZCStXQmgobN0KDzwAe9U3idwyNXkiIiIiku1UqwZr10Lp0laD98ADsG2b6apEcgY1eW6kfsn6pksQD6Ccid2UMbGbMpZzlC4Na9ZA1arWLZv16sGPP5qu6u8pY2Kamjw30rhMY9MliAdQzsRuypjYTRnLWcLCrFs377/fmoSlYUNYutR0VX9NGRPT1OS5kT1n9pguQTyAciZ2U8bEbspYzlOwoDUZS6NGcOECPPYYzJljuqo/p4yJaWryRERERCTby5MHvvnGWmYhNRXatIGZM01XJZI9qclzIzO2zDBdgngA5UzspoyJ3ZSxnMvfHz7/HJ5/HtLToXNnawH17EYZE9PU5ImIiIhIjuHjA+++C889ZzV60dGwcKHpqkSyFzV5IiIiIpKjeHtbjV67dnDlCrRqBStWmK5KJPtQkyciIiIiOY6Pj/VM3hNPwKVL8PjjOWN5BZGs4OVwOBymi8iJkpKSCA4OJjExkaCgINPliIiIiHikqw3esmWQPz8sXw41a5quSiTz3U7/oZE8NzJm1RjTJYgHUM7EbsqY2E0Zcy+5c8O8eVC3Lpw9C40bw44dZmtSxsQ0jeRlUHYcyUu8lEhw7mDTZYibU87EbsqY2E0Zc09nz8Ijj8DmzVCsGKxeDaVKmalFGRM75LiRvHfeeYfw8HBy585NnTp1iIuL+8vjz549S8+ePQkLC8Pf35/y5cuz8LppldLS0hg+fDilSpUiICCAMmXK8Oqrr/Jn/Wy3bt3w8vJi0qRJmfm1slyHuR1MlyAeQDkTuyljYjdlzD3lzw+LF0PlynD4sNXwHT5sphZlTEzzNV3A7NmzGTBgANOmTaNOnTpMmjSJJk2aEB8fT+HChW84PiUlhUaNGlG4cGFiY2MpVqwYBw4cIH/+/M5jxo8fz9SpU/n444+pUqUKGzdupHPnzgQHB9OnTx+X882dO5cff/yRokWL2v1VRURERMRGhQrB0qVQrx7s2QMNG8KqVXDXXaYrE8laxpu8iRMn8sILL9C5c2cApk2bxoIFC5gxYwZDhgy54fgZM2Zw+vRp1q1bR65cuQAIDw93OWbdunW0aNGCxx57zLk/JibmhhHCw4cP07t3bxYvXuw89s9cvnyZy5cvO39PSkq67e8qIiIiIvYqWtSahOWBB6xn8xo3tiZjuW48QMTtGW3yUlJS2LRpEy+99JJzm7e3Nw0bNmT9+vU3fc/8+fOJjIykZ8+ezJs3j7vuuounn36awYMH4+PjA0DdunV577332LlzJ+XLl+eXX35hzZo1TJw40Xme9PR0OnTowMCBA6lSpcrf1vraa68xevToG7a3/bItuQKtZjMgVwCzW89mzKoxdKnVhW7fdnM5duiDQ9lybAutK7dm/JrxxJ+Kd+6LKh5Fi4ot2Hp8K4G5Anl307su753TZg7j145nQOQA2sW2c9nXK6IXp5JPcSHlAmNWjSHu8LVmtmrhqvSK6MV3u76jQqEKTFg7weW9M1rMYNrGaQyrN4yWs1tyJf2Kc1+H6h0IzBVIWL4w1iasZenepc59xYOK83rj15m6YSqNyzRm6A9DXc77VpO3iNkWw7B6w+g8rzOnkk8597Wo0IIKhSpwJf0KCYkJxP4W69wX5B/EJy0/YcyqMXSs0ZFeC3u5nHdUg1GsTVhLxxodGbliJHvP7HXuq1+yPo3LNGbPmT3Wd9syw+W986PnM2bVGHpH9L7hNooBkQNISEygQXgDPtj8AVuObXHuqxVai+fueY4V+1dQIrgEE9dPdHnvrKdmMTluMsPqDaN5THOXfV1qdQGgTIEyLNmzhJUHVjr3lS5QmtENRjPzl5lElYhi1IpRLu+d0mwKM3+ZybB6w3jmq2dIunztLxZaV25NieAS+Hr7En8ynnnx85z7QgJD+LDFh4xZNYboqtH0X9zf5bxjHx7Lkj1L6H5fdwYuGcjBpIPOfY1KNyKqRBRHzx0lOTWZWb/Ocu7z9fblq7ZfsfPUTk4mn6TLvC4u5x0UNYj4k/E0LdeUKXFT2HZim3NfRLEIoqtGE3c4jpDAEKbETXF57+etP2fi+okMjhpMqy9auezrWrsryanJVCtSjXk75rH24FrnvgohFRj8wGBif4ulVmgtxq4e6/LeaY9PY8aWGQyrN4y2sW25mHrRuS+6ajQhgSEE+wez5dgWFu66dst3WN4wpjSbwsT1E2lRsQWDlg5yOe+ERhOYt2MeAyIH0GthL46eP+rc16xcM2qF1iLxciKnkk8Rsy3Guc/0NSKiWAQx22Ky/TUiITGBNQlrdI3IodeIMavG0O3ebtn6GhF3OI7mMc11jfh/Oe0acSv/HTHrwBhmfd2FJg8H8vPP+Sl93w7uf3kEYx4dkiXXiKsZA10jrspJ14irsts1wjf1Nlo3h0GHDx92AI5169a5bB84cKAjIiLipu+pUKGCw9/f39GlSxfHxo0bHZ9//rmjYMGCjlGjRjmPSUtLcwwePNjh5eXl8PX1dXh5eTnGjRvncp5x48Y5GjVq5EhPT3c4HA5HyZIlHW+99daf1nrp0iVHYmKi83Xw4EEH4EhMTMzgt898y/ctN12CeADlTOymjIndlDHP8csvDkeBAg4HOBwPPeRwJCdnzecqY2KHxMTEW+4/jN+uebvS09MpXLgw7733Hj4+PtSuXZvDhw/z+uuvM3LkSAC++OILPv30Uz777DOqVKnCzz//TL9+/ShatCidOnVi06ZN/Oc//2Hz5s14eXnd0uf6+/vj7+9v51e7YwmJCaZLEA+gnIndlDGxmzLmOapXh0WLrElYli+Hf/wDvvoK/Pzs/VxlTEwzOrtmoUKF8PHx4fjx4y7bjx8/Tmho6E3fExYWRvny5Z23ZgJUqlSJY8eOkZKSAsDAgQMZMmQI7dq1o1q1anTo0IH+/fvz2muvAbB69WpOnDhBiRIl8PX1xdfXlwMHDvDPf/7zhuf7cpIG4Q1MlyAeQDkTuyljYjdlzLNERMCCBRAQYP3zmWcgLc3ez1TGxDSjTZ6fnx+1a9dm2bJlzm3p6eksW7aMyMjIm74nKiqK3bt3k56e7ty2c+dOwsLC8Pv/v5ZJTk7G29v1q/n4+Djf06FDB3799Vd+/vln56to0aIMHDiQxYsXZ/bXzDIfbP7AdAniAZQzsZsyJnZTxjxPvXowdy7kygVffgnPPw/X/adkplPGxDTjt2sOGDCATp06ce+99xIREcGkSZO4cOGCc7bNjh07UqxYMecoXPfu3ZkyZQp9+/ald+/e7Nq1i3HjxrksjfDEE08wduxYSpQoQZUqVdiyZQsTJ06kSxfrAc+QkBBCQkJc6siVKxehoaFUqFAhi7555rt+IgARuyhnYjdlTOymjHmmJk3g88+hTRv46CPImxfefhtu8cmd26KMiWnGm7y2bdvy+++/M2LECI4dO0bNmjVZtGgRRYoUASAhIcFlVK548eIsXryY/v37U716dYoVK0bfvn0ZPHiw85jJkyczfPhwevTowYkTJyhatChdu3ZlxIgRWf79RERERCR7aNnSavA6doQpU6xG7//HEUTcivEmD6BXr1706tXrpvtWrFhxw7bIyEh+/PHHPz1fvnz5mDRpEpMmTbrlGvbv33/Lx4qIiIhIzvTMM3DhAnTrBv/+N+TLBy+/bLoqkcxl9Jk8yVy1QmuZLkE8gHImdlPGxG7KmHTtCm+8Yf08dKh122ZmUsbENC+Hw+EwXUROlJSURHBwMImJiQQFBZkuB7Cm6y0RXMJ0GeLmlDOxmzImdlPG5KpRo2D0aOvnDz6ALl3+8vBbpoyJHW6n/9BInhtZsX+F6RLEAyhnYjdlTOymjMlVI0fCgAHWz88/b03MkhmUMTFNTZ4b0d8YSVZQzsRuypjYTRmTq7y8rNs2u3YFhwM6dIBvvrnz8ypjYpqaPDcycf1E0yWIB1DOxG7KmNhNGZPreXnBf/8L7dvDlSvwj3/AdUs4Z4gyJqapyRMRERERj+btbS2t8NRTcPkyNG8Oa9earkok49TkiYiIiIjH8/WFmBhr0fTkZGjWDDZvNl2VSMaoyRMRERERAfz94auv4MEHISkJGjeG//3PdFUit09LKGRQdlxCIfFSIsG5g02XIW5OORO7KWNiN2VM/k5SEjzyCGzcCGFhsHo1lClz6+9XxsQOWkLBQ02Om2y6BPEAypnYTRkTuylj8neCgmDRIqhaFY4etRq+gwdv/f3KmJimkbwMyo4jeSIiIiKSeY4dg3r1YNcuKF8eVq2CIkVMVyWeSiN5Hqp5THPTJYgHUM7EbsqY2E0Zk1sVGgrffw8lSsDOndCoEZw+/ffvU8bENDV5IiIiIiJ/okQJa9280FDYuhWaNrWe2RPJztTkiYiIiIj8hbJlYelSCAmBuDh44glrmQWR7EpNnoiIiIjI36haFRYvtiZlWbUKWrWyFk4XyY7U5LmRLrW6mC5BPIByJnZTxsRuyphkVO3asGABBAZas29GR8OVKzcep4yJaWryRERERERu0QMPwLx54OcHc+dCly6Qnm66KhFXavLcSJkCt7FKp0gGKWdiN2VM7KaMyZ1q2BC+/BJ8fGDWLOjZE65flEwZE9PU5LmRJXuWmC5BPIByJnZTxsRuyphkhubNrQbPywumTYNBg641esqYmKYmz42sPLDSdAniAZQzsZsyJnZTxiSzREfDe+9ZP7/xBrz6qvWzMiam+ZouQEREREQkp3r+eTh/Hvr3h5EjIW9eIMx0VeLpNJInIiIiInIH+vW7Nor3z3/C/u8fNVqPiJo8N1K6QGnTJYgHUM7EbsqY2E0ZEzsMHQqDB1s/b53Rg3HjXCdjEclKXg6H4pcRSUlJBAcHk5iYSFBQkOlyAEi8lEhw7mDTZYibU87EbsqY2E0ZE7s4HPDyy/Dvf1u/9+4NkyaBt4ZVJBPcTv+hyLmRmb/MNF2CeADlTOymjIndlDGxi5cXvPYatOy/CoDJk+Hpp+HyZcOFicdRk+dGokpEmS5BPIByJnZTxsRuypjYbejAvHz2GeTKBbNnw+OPw7lzpqsST6Imz42MWjHKdAniAZQzsZsyJnZTxsRuo1aMIjoavv0W8uSB77+HBg3gxAnTlYmnUJMnIiIiImKDxo1hxQooVAg2b4aoKNi713RV4gnU5ImIiIiI2OTee2HtWggPh927oW5d+Pln01WJu1OTJyIiIiJio/LlYd06qF4djh+HevVg+XLTVYk70xIKGZQdl1BISEygRHAJ02WIm1POxG7KmNhNGRO7/VnGEhOhRQtYuRL8/ODTT6F1awMFSo6kJRQ8lKaElqygnIndlDGxmzImdvuzjAUHw6JF0LIlpKRAmzYwdWoWFyceQSN5GZQdR/JEREREJPtLS4OePeHdd63fR4yAUaOsdfZE/oxG8jzUM189Y7oE8QDKmdhNGRO7KWNit7/LmI+PNYI3apT1+yuvQLduVvMnkhnU5LmRpMtJpksQD6Ccid2UMbGbMiZ2u5WMeXnByJFWs+flBe+9B//4B1y6lAUFittTkyciIiIiYki3bvDll9ZELHPnQpMmcPas6aokp1OTJyIiIiJiUKtWsHgxBAXBqlXWEgtHjpiuSnIyNXlupHVlzcEr9lPOxG7KmNhNGRO7ZSRjDRpYDV5oKGzdClFRsHNn5tcmnkFNnhvRmj+SFZQzsZsyJnZTxsRuGc1YjRrWoully8L+/Vajt2FD5tYmnkFNnhvx9fY1XYJ4AOVM7KaMid2UMbHbnWSsVClYuxZq14aTJ+Ghh2DJkkwsTjyCmjw3En8y3nQJ4gGUM7GbMiZ2U8bEbneascKFYflyaNgQLlyAxx6Dzz7LpOLEI6jJcyPz4ueZLkE8gHImdlPGxG7KmNgtMzKWLx8sWADt2sGVK9C+PUyadOe1iWdQkyciIiIikg35+cGnn0KfPtbv/fvDkCHgcJitS7I/NXkiIiIiItmUt7c1gvfaa9bv48dDly6Qmmq0LMnm1OS5kZDAENMliAdQzsRuypjYTRkTu2V2xry8rBG8GTPAxwc++gieegqSkzP1Y8SNeDkcGvDNiKSkJIKDg0lMTCQoKMh0OSIiIiLiAb75Btq0gUuXIDISvv0WChY0XZVkhdvpPzSS50bGrBpjugTxAMqZ2E0ZE7spY2I3OzP2xBOwbBkUKADr18MDD8DBg7Z9nORQavLcSHTVaNMliAdQzsRuypjYTRkTu9mdsbp1YfVqKFYMtm+3fv/tN1s/UnIYNXlupP/i/qZLEA+gnIndlDGxmzImdsuKjFWpAuvWQaVKcOiQNaK3fr3tHys5hJo8EREREZEcqEQJa0Tv/vvhzBl45BHrGT0RNXkiIiIiIjlUSAh8/z089hhcvAhPPmnNvimeTU2eiIiIiEgOlicPzJ0LnTpBWhp07mytp6c59D2XllDIoOy4hMLW41upVqSa6TLEzSlnYjdlTOymjIndTGXM4bDW05swwfq9Xz94801rQXXJ+bSEgodasmeJ6RLEAyhnYjdlTOymjIndTGXMy8sawZs40fp90iTo0AFSUoyUIwZpJC+DsuNIXnJqMoG5Ak2XIW5OORO7KWNiN2VM7JYdMvbpp/Dss3DlCjRuDLGxkC+f0ZLkDmkkz0MNXDLQdAniAZQzsZsyJnZTxsRu2SFj7dtbM23myQNLlsDDD8Pvv5uuSrKKmjw3cjDpoOkSxAMoZ2I3ZUzspoyJ3bJLxpo0gR9+gEKFYONGqFYNnnkG3nkHNm2C1FTTFYpdfE0XICIiIiIi9oiIgDVr4NFHYf9+6zbOTz+19gUEwL33QmSk9br/fggNNVquZBI1eSIiIiIibqxCBfjf/6xmb/16+PFH63X2rLWY+urV144ND7/W8EVGQo0a4OdnqnLJKDV5bqRR6UamSxAPoJyJ3ZQxsZsyJnbLjhkLDLQmYGnc2Po9PR3i461m72rjt22bNdq3fz/ExFjH5c4NtWu7jvYVLWrqW8itUpPnRqJKRJkuQTyAciZ2U8bEbsqY2C0nZMzbGypVsl6dO1vbkpIgLs618Tt9GtautV5XlSjhOtpXq5ZG+7IbNXlu5Oi5oxBmugpxd8qZ2E0ZE7spY2K3nJqxoCBo2NB6gbW4+q5dVsN3tenbuhUSEqzX7NnWcf7+cM89rqN9d99t7nuImjy3kpyabLoE8QDKmdhNGRO7KWNiN3fJmJcXlC9vvTp1sradOwcbNlwb7Vu/Hk6duvbzVXfffW2k7+poX+7cZr6HJ9ISCm5k1q+zTJcgHkA5E7spY2I3ZUzs5s4Zy5fPWnPv5Zfhm2+stfd27YKZM6F7d6uZ8/GBQ4esBdj/+U+oWxeCg62mr39/awQwIcEaKRR7ZIsm75133iE8PJzcuXNTp04d4uLi/vL4s2fP0rNnT8LCwvD396d8+fIsXLjQuT8tLY3hw4dTqlQpAgICKFOmDK+++iqO/09SamoqgwcPplq1auTJk4eiRYvSsWNHjhw5Yuv3FBERERFxJ15eULYsdOgA//0vbN4MiYmwfDm89ho0bw533QUpKfDTTzBpErRrByVLWqN9rVrBf/4DFy6Y/ibuxfjtmrNnz2bAgAFMmzaNOnXqMGnSJJo0aUJ8fDyFCxe+4fiUlBQaNWpE4cKFiY2NpVixYhw4cID8+fM7jxk/fjxTp07l448/pkqVKmzcuJHOnTsTHBxMnz59SE5OZvPmzQwfPpwaNWpw5swZ+vbtS/Pmzdm4cWMWfnsREREREfeSJw80aGC9wBqx27fP9dm+n3+GI0fgq6+s1/TpMGeOtdyD3DnjTd7EiRN54YUX6Pz/0/pMmzaNBQsWMGPGDIYMGXLD8TNmzOD06dOsW7eOXLlyARAeHu5yzLp162jRogWPPfaYc39MTIxzhDA4OJilS5e6vGfKlClERESQkPB/7d17WFV1vsfxz+YqqKCCXDSZtPGOFmo2iF0mTSWnzDSVQ0bZc0zFQm28VN46aabO0EzaYPVkl9Gk9ORophZh6tFM8ZoXRJsyTETzBiaKCL/zxx537lGrUZdrs3m/nmc/yfr99va76PNs+fLb67fyFRMTc61P87rw87H9fyeqAHIGq5ExWI2MwWpkzJ3DITVq5HwkJzuPlZRImzZJX3zhXN3budN5Y/bZs6WHHrK1XK9g68c1z549q02bNqnz+S18JPn4+Khz585ad+GVmxdYvHix4uPjlZqaqsjISMXGxurFF19UeXm5a06HDh2UnZ2tPXv2SJK2bdumNWvWKDEx8bK1FBUVyeFwuK0IXqi0tFTFxcVuD0/zYd8P7S4BVQA5g9XIGKxGxmA1MvbLgoOl22+XRo+Wtmxxrvr9+KPUp480bJjz4524crb+muHIkSMqLy9XZGSk2/HIyEjt3r37ks/55ptvtGLFCiUnJ2vp0qX6+uuvNWTIEJWVlWnChAmSpDFjxqi4uFjNmjWTr6+vysvLNXnyZCWf/9XBvzlz5oxGjx6tpKQkhYSEXHLOlClT9Pzzz190vO/8vvIPdq4oBvkH6f3e72vS6kkaEDdAg5YMcpv73O3PaUvhFvVu0VtT10xV3tE811hCgwT1aNZD2w9tV7B/sF7b9Jrbc/+3z/9q6tqpGhE/Qv0W9HMbG9p+qI6WHFVOQY7qBNXRhgM/XdMYGxGroe2HatneZWoa3lTT1k5ze+7sHrM1a+Msjb1jrB58/0GdqzjnGuvfur+C/YMVXTNaa/PXKuubn1Y/G4Q00PQu05WRk6EuN3XRcyuec3vdl7u+rHk75mnsHWP12KLHdLTkqGusR9MeahreVOcqzim/KF8Ldi1wjYUEhmjOg3M0afUkPXLzIxq6dKjb6068a6LW5q/VIzc/ogkrJ+ib49+4xu78zZ3qclMX/fP4P53ntmW223MXJy3WpNWT9GT7J9V/YX+3sRHxI5RflK+7brxLb25+U1sKt7jG4qLi9Hibx7Vy30rFhMYofV2623P/3vPvmrFhhsbeMVb3z7vfbWxA3ABJ0k21b9Kn//xUq75b5RprVLuRnr/reb277V0lxCRo4sqJbs+dee9MvbvtXY29Y6we/vBhFZf+9IuF3i16KyY0Rn4+fso7kqdFeYtcY2HBYXqrx1uatHqSkmKTNPyT4W6vO/nuyfr0n59q8K2DNfLTkdpfvN81dk+je5QQk6CDJw+qpKzE7cJxPx8/fdj3Q3V+t7Mye2dqwKIBbq87KmGU8o7kKbFxomZumKkdh3e4xtrXb6+k2CRtOLBBYcFhmrlhpttzM3tnKn1dukYnjFavD3q5jT3R9gmVlJWoVWQrLdq9SGv3/3SjnqZhTTW642gt2LVAcVFxmvx/k92eO+sPszR7y2yNvWOs+i7oq9Nlp11jSbFJCgsOU2hgqLYUbtHSvT9d1xtdI1oz752p9HXp6tGsh0ZljXJ73Wn3TNOi3Ys0In6Ehi4dqoM/HnSN3dv4XsVFxamotEhHS45q3o55rjG73yPa12+veTvmefx7hK/DV093eJr3iEr6HjFp9SQNajfIo98j9hzdoyZhTXiP+JfK9h5RGX6OeH/n+2oS1kQS7xHn/dJ7xIQ3RynohZpa9vbN+utfpXeX5qrtU9MUFHaUnyPOfw/Lfn3r5jDGvn1tCgoKVL9+fX3xxReKj493HR81apRWrVql9evXX/ScJk2a6MyZM/r222/l6+sryfmRz+nTp+vgQec3KDMzUyNHjtT06dPVsmVLbd26VcOGDVN6erpSzu//+i9lZWXq1auXvv/+e61cufKyTV5paalKS0tdXxcXF6tBgwYqKiq67HOutyMlRxQeHG53GfBy5AxWI2OwGhmD1cjYlVu8WHrkEefmLeHh0nvvSffcY3dVnqG4uFihoaG/qv+w9eOa4eHh8vX11aFDh9yOHzp0SFFRUZd8TnR0tJo0aeJq8CSpefPmKiws1Nl/reuOHDlSY8aMUb9+/dSqVSv1799fw4cP15QpU9xeq6ysTH369NF3332nrKysn/1mBQYGKiQkxO3haf79NyKAFcgZrEbGYDUyBquRsSt3//3OHTrbtJGOHJG6dpVeeEGqqLC7ssrF1iYvICBAbdu2VXZ2tutYRUWFsrOz3Vb2LpSQkKCvv/5aFRf8n96zZ4+io6MVEBAgSSopKZGPj/up+fr6uj3nfIO3d+9effbZZwoLC7uWpwYAAADgCjRqJK1dK/33fzt35hw/Xure3XnTdfw6tt8nb8SIEXrjjTf0zjvvKDc3V4MHD9apU6dcu20+8sgjeuaZZ1zzBw8erGPHjiktLU179uzRxx9/rBdffFGpqamuOffdd58mT56sjz/+WPv27dPChQuVnp6unj17SnI2eL1799bGjRs1d+5clZeXq7Cw0G01EAAAAIA9qlWTXn9devttKShIWr7cubr3C7fTxr/Yvr9r37599cMPP2j8+PEqLCzULbfcouXLl7s2Y8nPz3dblWvQoIE++eQTDR8+XK1bt1b9+vWVlpam0aNHu+bMmDFD48aN05AhQ3T48GHVq1dPTzzxhMaPHy9JOnDggBYvXixJuuWWW9zq+fzzz3XX+Zt6AAAAALBNSooUFyf17i3t3St17Ci9/LI0ZIjz1gy4NFs3XqnM/pMLH6+XNflr1DGmo91lwMuRM1iNjMFqZAxWI2PXXlGRNGCA88bpkpSU5Fzpq1HD3rqup0qz8Qqurbwjeb88CbhK5AxWI2OwGhmD1cjYtRcaKi1YIKWnS35+0rx5Uvv2Um6u3ZV5Jpo8L5LY+PI3eweuFXIGq5ExWI2MwWpkzBoOhzR8uLRypVSvnrPBu/VWZ8MHdzR5XuTfbwwJWIGcwWpkDFYjY7AaGbNWQoK0ZYt0993SqVPSf/2XNHSodMEtras8mjwvsuPwDrtLQBVAzmA1MgarkTFYjYxZLyJC+vRT6bnnnF+/+qp0xx1Sfr69dXkKmjwAAAAAlY6vrzRpkrRkiVS7tvP2CnFxztstVHU0eQAAAAAqre7dpc2bpXbtpGPHpHvvlSZMkMrL7a7MPjR5XqR9/fZ2l4AqgJzBamQMViNjsBoZu/5uvFFas0YaNEgyRvqf/5ESE6UffrC7MnvQ5HmRpNgku0tAFUDOYDUyBquRMViNjNkjMFDKyJD+/ncpOFjKypLatJHWrbO7suuPJs+LbDiwwe4SUAWQM1iNjMFqZAxWI2P2evhhaf16qWlT6fvvnRuyvPKKc4WvqqDJ8yJhwWF2l4AqgJzBamQMViNjsBoZs19srJSTI/XpI507J6WlSf36SSdP2l3Z9UGT50W4JwuuB3IGq5ExWI2MwWpkzDPUrCllZkp//avk5yd98IHz5uk7d9pdmfVo8gAAAAB4JYdDeuopafVq6YYbpLw8qX17ac4cuyuzFk0eAAAAAK8WH++8zcI990glJVL//tLgwdKZM3ZXZg2aPAAAAABer25dadky5z30HA5p1iypY0dp3z67K7v2HMZUpX1mrp3i4mKFhoaqqKhIISEhdpcjSSopK1Gwf7DdZcDLkTNYjYzBamQMViNjnm/5cik52Xnz9Nq1nbdd6N7d7qp+3n/Sf7CS50XS16XbXQKqAHIGq5ExWI2MwWpkzPN16yZt2eK8Pu/4cekPf5DGjpXKy+2u7NpgJe8KeeJKXll5mfx9/e0uA16OnMFqZAxWI2OwGhmrPEpLpaefll591fn13XdL770nRUbaW9elsJJXRfX6oJfdJaAKIGewGhmD1cgYrEbGKo/AQGnmTGdjV726tGKF1KaNtGaN3ZVdHZo8AAAAAFVaUpLz5unNm0sFBdJdd0np6VJl/cwjTR4AAACAKq95c2nDBmfDV17u/Bhn795SWZndlf3naPIAAAAAQFKNGtLcuc6PcPr7SxERzv9WNn52F4Br54m2T9hdAqoAcgarkTFYjYzBamSscnM4pNRUqUMHqUULu6u5MjR5XqSkrMTuElAFkDNYjYzBamQMViNj3iEuzu4Krhwf1/QirSJb2V0CqgByBquRMViNjMFqZAx2o8nzIot2L7K7BFQB5AxWI2OwGhmD1cgY7EaT50XW7l9rdwmoAsgZrEbGYDUyBquRMdiNJg8AAAAAvAhNHgAAAAB4EZo8L9I0rKndJaAKIGewGhmD1cgYrEbGYDeHMcbYXURlVFxcrNDQUBUVFSkkJMTuciRJR0qOKDw43O4y4OXIGaxGxmA1MgarkTFY4T/pP1jJ8yILdi2wuwRUAeQMViNjsBoZg9XIGOxGk+dF4qIq8R0bUWmQM1iNjMFqZAxWI2OwG02eF5n8f5PtLgFVADmD1cgYrEbGYDUyBrvR5AEAAACAF6HJAwAAAAAvQpMHAAAAAF6EWyhcIU+8hULByQLVq1nP7jLg5cgZrEbGYDUyBquRMViBWyhUUbO3zLa7BFQB5AxWI2OwGhmD1cgY7MZK3hXyxJU8AAAAAN6Jlbwqqu+CvnaXgCqAnMFqZAxWI2OwGhmD3WjyvMjpstN2l4AqgJzBamQMViNjsBoZg91o8gAAAADAi/jZXUBldf5SxuLiYpsr+UlZSZlH1QPvRM5gNTIGq5ExWI2MwQrnM/VrtlRh45Ur9P3336tBgwZ2lwEAAACgCtm/f79uuOGGn51Dk3eFKioqVFBQoJo1a8rhcNhdjoqLi9WgQQPt37+f3T5hGXIGq5ExWI2MwWpkDFYxxujkyZOqV6+efHx+/qo7Pq55hXx8fH6xg7ZDSEgIbyiwHDmD1cgYrEbGYDUyBiuEhob+qnlsvAIAAAAAXoQmDwAAAAC8CE2elwgMDNSECRMUGBhodynwYuQMViNjsBoZg9XIGDwBG68AAAAAgBdhJQ8AAAAAvAhNHgAAAAB4EZo8AAAAAPAiNHkAAAAA4EVo8rzEq6++qhtvvFHVqlXTbbfdpg0bNthdEiqBKVOm6NZbb1XNmjUVERGhBx54QHl5eW5zzpw5o9TUVIWFhalGjRrq1auXDh065DYnPz9f3bt3V3BwsCIiIjRy5EidO3fuep4KKomXXnpJDodDw4YNcx0jY7gWDhw4oIcfflhhYWEKCgpSq1attHHjRte4MUbjx49XdHS0goKC1LlzZ+3du9ftNY4dO6bk5GSFhISoVq1aevzxx/Xjjz9e71OBByovL9e4cePUsGFDBQUF6aabbtILL7ygC/cvJGPwJDR5XuD999/XiBEjNGHCBG3evFk333yzunbtqsOHD9tdGjzcqlWrlJqaqi+//FJZWVkqKytTly5ddOrUKdec4cOH66OPPtL8+fO1atUqFRQU6MEHH3SNl5eXq3v37jp79qy++OILvfPOO3r77bc1fvx4O04JHiwnJ0evvfaaWrdu7XacjOFqHT9+XAkJCfL399eyZcu0a9cu/fnPf1bt2rVdc6ZNm6ZXXnlFs2bN0vr161W9enV17dpVZ86ccc1JTk7Wzp07lZWVpSVLlmj16tUaOHCgHacEDzN16lRlZGRo5syZys3N1dSpUzVt2jTNmDHDNYeMwaMYVHrt27c3qamprq/Ly8tNvXr1zJQpU2ysCpXR4cOHjSSzatUqY4wxJ06cMP7+/mb+/PmuObm5uUaSWbdunTHGmKVLlxofHx9TWFjompORkWFCQkJMaWnp9T0BeKyTJ0+axo0bm6ysLHPnnXeatLQ0YwwZw7UxevRo07Fjx8uOV1RUmKioKDN9+nTXsRMnTpjAwEAzb948Y4wxu3btMpJMTk6Oa86yZcuMw+EwBw4csK54VArdu3c3AwYMcDv24IMPmuTkZGMMGYPnYSWvkjt79qw2bdqkzp07u475+Pioc+fOWrdunY2VoTIqKiqSJNWpU0eStGnTJpWVlbnlq1mzZoqJiXHla926dWrVqpUiIyNdc7p27ari4mLt3LnzOlYPT5aamqru3bu7ZUkiY7g2Fi9erHbt2umhhx5SRESE4uLi9MYbb7jGv/32WxUWFrrlLDQ0VLfddptbzmrVqqV27dq55nTu3Fk+Pj5av3799TsZeKQOHTooOztbe/bskSRt27ZNa9asUWJioiQyBs/jZ3cBuDpHjhxReXm52w8/khQZGandu3fbVBUqo4qKCg0bNkwJCQmKjY2VJBUWFiogIEC1atVymxsZGanCwkLXnEvl7/wYkJmZqc2bNysnJ+eiMTKGa+Gbb75RRkaGRowYoWeffVY5OTl66qmnFBAQoJSUFFdOLpWjC3MWERHhNu7n56c6deqQM2jMmDEqLi5Ws2bN5Ovrq/Lyck2ePFnJycmSRMbgcWjyAEhyrrTs2LFDa9assbsUeJH9+/crLS1NWVlZqlatmt3lwEtVVFSoXbt2evHFFyVJcXFx2rFjh2bNmqWUlBSbq4M3+OCDDzR37ly99957atmypbZu3aphw4apXr16ZAweiY9rVnLh4eHy9fW9aCe6Q4cOKSoqyqaqUNkMHTpUS5Ys0eeff64bbrjBdTwqKkpnz57ViRMn3OZfmK+oqKhL5u/8GKq2TZs26fDhw2rTpo38/Pzk5+enVatW6ZVXXpGfn58iIyPJGK5adHS0WrRo4XasefPmys/Pl/RTTn7u38qoqKiLNiw7d+6cjh07Rs6gkSNHasyYMerXr59atWql/v37a/jw4ZoyZYokMgbPQ5NXyQUEBKht27bKzs52HauoqFB2drbi4+NtrAyVgTFGQ4cO1cKFC7VixQo1bNjQbbxt27by9/d3y1deXp7y8/Nd+YqPj9f27dvd/uHKyspSSEjIRT90oerp1KmTtm/frq1bt7oe7dq1U3JysuvPZAxXKyEh4aLbv+zZs0e/+c1vJEkNGzZUVFSUW86Ki4u1fv16t5ydOHFCmzZtcs1ZsWKFKioqdNttt12Hs4AnKykpkY+P+4/Nvr6+qqiokETG4IHs3vkFVy8zM9MEBgaat99+2+zatcsMHDjQ1KpVy20nOuBSBg8ebEJDQ83KlSvNwYMHXY+SkhLXnEGDBpmYmBizYsUKs3HjRhMfH2/i4+Nd4+fOnTOxsbGmS5cuZuvWrWb58uWmbt265plnnrHjlFAJXLi7pjFkDFdvw4YNxs/Pz0yePNns3bvXzJ071wQHB5s5c+a45rz00kumVq1aZtGiRearr74yPXr0MA0bNjSnT592zenWrZuJi4sz69evN2vWrDGNGzc2SUlJdpwSPExKSoqpX7++WbJkifn222/Nhx9+aMLDw82oUaNcc8gYPAlNnpeYMWOGiYmJMQEBAaZ9+/bmyy+/tLskVAKSLvl46623XHNOnz5thgwZYmrXrm2Cg4NNz549zcGDB91eZ9++fSYxMdEEBQWZ8PBw8/TTT5uysrLrfDaoLP69ySNjuBY++ugjExsbawIDA02zZs3M66+/7jZeUVFhxo0bZyIjI01gYKDp1KmTycvLc5tz9OhRk5SUZGrUqGFCQkLMY489Zk6ePHk9TwMeqri42KSlpZmYmBhTrVo106hRI/Pcc8+53caFjMGTOIwxxs6VRAAAAADAtcM1eQAAAADgRWjyAAAAAMCL0OQBAAAAgBehyQMAAAAAL0KTBwAAAABehCYPAAAAALwITR4AAAAAeBGaPAAAAADwIjR5AAB4KIfDoX/84x92lwEAqGRo8gAAuIRHH31UDofjoke3bt3sLg0AgJ/lZ3cBAAB4qm7duumtt95yOxYYGGhTNQAA/Dqs5AEAcBmBgYGKiopye9SuXVuS86OUGRkZSkxMVFBQkBo1aqQFCxa4PX/79u26++67FRQUpLCwMA0cOFA//vij25zZs2erZcuWCgwMVHR0tIYOHeo2fuTIEfXs2VPBwcFq3LixFi9e7Bo7fvy4kpOTVbduXQUFBalx48YXNaUAgKqHJg8AgCs0btw49erVS9u2bVNycrL69eun3NxcSdKpU6fUtWtX1a5dWzk5OZo/f74+++wztyYuIyNDqampGjhwoLZv367Fixfrt7/9rdvf8fzzz6tPnz766quvdO+99yo5OVnHjh1z/f27du3SsmXLlJubq4yMDIWHh1+/bwAAwCM5jDHG7iIAAPA0jz76qObMmaNq1aq5HX/22Wf17LPPyuFwaNCgQcrIyHCN/e53v1ObNm30t7/9TW+88YZGjx6t/fv3q3r16pKkpUuX6r777lNBQYEiIyNVv359PfbYY5o0adIla3A4HBo7dqxeeOEFSc7GsUaNGlq2bJm6deum+++/X+Hh4Zo9e7ZF3wUAQGXENXkAAFzG73//e7cmTpLq1Knj+nN8fLzbWHx8vLZu3SpJys3N1c033+xq8CQpISFBFRUVysvLk8PhUEFBgTp16vSzNbRu3dr15+rVqyskJESHDx+WJA0ePFi9evXS5s2b1aVLFz3wwAPq0KHDFZ0rAMB70OQBAHAZ1atXv+jjk9dKUFDQr5rn7+/v9rXD4VBFRYUkKTExUd99952WLl2qrKwsderUSampqfrTn/50zesFAFQeXJMHAMAV+vLLLy/6unnz5pKk5s2ba9u2bTp16pRrfO3atfLx8VHTpk1Vs2ZN3XjjjcrOzr6qGurWrauUlBTNmTNHf/nLX/T6669f1esBACo/VvIAALiM0tJSFRYWuh3z8/NzbW4yf/58tWvXTh07dtTcuXO1YcMGvfnmm5Kk5ORkTZgwQSkpKZo4caJ++OEHPfnkk+rfv78iIyMlSRMnTtSgQYMUERGhxMREnTx5UmvXrtWTTz75q+obP3682rZtq5YtW6q0tFRLlixxNZkAgKqLJg8AgMtYvny5oqOj3Y41bdpUu3fvluTc+TIzM1NDhgxRdHS05s2bpxYtWkiSgoOD9cknnygtLU233nqrgoOD1atXL6Wnp7teKyUlRWfOnNHLL7+sP/7xjwoPD1fv3r1/dX0BAQF65plntG/fPgUFBen2229XZmbmNThzAEBlxu6aAABcAYfDoYULF+qBBx6wuxQAANxwTR4AAAAAeBGaPAAAAADwIlyTBwDAFeBqBwCAp2IlDwAAAAC8CE0eAAAAAHgRmjwAAAAA8CI0eQAAAADgRWjyAAAAAMCL0OQBAAAAgBehyQMAAAAAL0KTBwAAAABe5P8BK9gWvrjBbAYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate on test set"
      ],
      "metadata": {
        "id": "iukbgXq8ocs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss, test_recall, test_precision = evaluation(model, test_edge_index, test_edge_index_sparse, [train_edge_index, val_edge_index], args['topK'], args['lambda'])\n",
        "print('Evaluation on test set: loss: {:.4f}, recall: {:.4f}, precision: {:.4f}'\\\n",
        "        .format(test_loss, test_recall, test_precision))"
      ],
      "metadata": {
        "id": "wIdwM1XvoRl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f89e33-67eb-41d5-d4ce-961c5af19f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on test set: loss: 0.5310, recall: 0.0007, precision: 0.0001\n"
          ]
        }
      ]
    }
  ]
}