{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Google Map Restaurants Recommendation"
      ],
      "metadata": {
        "id": "J6xR8_3vcKfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Ron Wang, Yifan Geng, Hercy Shen*\n",
        "\n"
      ],
      "metadata": {
        "id": "X1uXSk3vcSKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The full code can be accessed: https://github.com/ronyw7/cs224w-proj. Our blog post is: https://medium.com/@yifangeng/95c9325f87fc."
      ],
      "metadata": {
        "id": "V4y6rQS7chhT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6Hdjc1FcADD"
      },
      "source": [
        "# 1. Preparing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5KwO49XcADE"
      },
      "source": [
        "In this project, we utilize pre-trained representations to augment GNN abilities.\n",
        "- We assume that `src/embeddings/data` already contains the image and text embeddings necessary for training.\n",
        "- Please refer to `README.md` and the blogpost on how we prepared the embeddings using OpenAI's CLIP encoders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h_LSJgZcADF"
      },
      "source": [
        "We use the multi-modal Google Restaurants dataset, collected by [Yan et al. (2022)](https://cseweb.ucsd.edu/~jmcauley/datasets.html#google_restaurants)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX-BTMDUcADF",
        "outputId": "2536a891-401e-4b7a-ac9e-d7f71ce0d950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of training data: (87013, 6)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# download the dataset and place it under data\n",
        "with open('data/filter_all_t.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "df = pd.DataFrame(data[\"train\"])\n",
        "\n",
        "print(\"Shape of training data:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwRbM7A2cADF"
      },
      "source": [
        "The original dataset ships with three splits. We will work with the training set as it contains enough datapoints for our purposes. We will split it into our own train, validation, and test sets.\n",
        "\n",
        "Each row in the original dataset has `business_id`, `user_id`, `rating`, `review_text`, `pics`, and `history_reviews`. As mentioned, we have preprocessed and stored each piece of review text and picture that can be easily looked up by their unique IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGK2wlS2cADG",
        "outputId": "e85f9b5a-2733-4561-b35f-5f4c2b11283b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>review_text</th>\n",
              "      <th>pics</th>\n",
              "      <th>history_reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60567465d335d0abfb415b26</td>\n",
              "      <td>101074926318992653684</td>\n",
              "      <td>4</td>\n",
              "      <td>The tang of the tomato sauce is outstanding. A...</td>\n",
              "      <td>[AF1QipM-2IRmvitARbcJr7deWfe5hyVBg_ArPMQSYvq0,...</td>\n",
              "      <td>[[101074926318992653684_6056272797d555cc6fb0d1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6050fa9f5b4ccec8d5cae994</td>\n",
              "      <td>117065749986299237881</td>\n",
              "      <td>5</td>\n",
              "      <td>Chicken and waffles were really good!</td>\n",
              "      <td>[AF1QipMpfxIZUT_aymQ3qPGO-QgGYzxbtLZGmHufAp2s]</td>\n",
              "      <td>[[117065749986299237881_605206f8d8c08f462b93e8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>604be10877e81aaed3cc9a1e</td>\n",
              "      <td>106700937793048450809</td>\n",
              "      <td>4</td>\n",
              "      <td>The appetizer of colossal shrimp was very good...</td>\n",
              "      <td>[AF1QipMNnqM5X9sSyZ9pXRZ1jvrURHN9bZhGdzuEXoP8,...</td>\n",
              "      <td>[[106700937793048450809_6044300b27f39b7b5d1dbf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60411e017cd8bf130362365a</td>\n",
              "      <td>101643045857250355161</td>\n",
              "      <td>5</td>\n",
              "      <td>The fish tacos here  omg! The salad was great ...</td>\n",
              "      <td>[AF1QipM-a6AGGp4Hgk5RD0gY5sDRp5kEfB1hZLvlRkft,...</td>\n",
              "      <td>[[101643045857250355161_604fbdd099686c10168c91...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>604139dd7cd8bf1303624208</td>\n",
              "      <td>109802745326785766951</td>\n",
              "      <td>4</td>\n",
              "      <td>Ribs are great, as are the mac and cheese, fri...</td>\n",
              "      <td>[AF1QipNVys4yq-5w_3EsDdHpSc9ZNb7Nl30Mfb6Y0Gup]</td>\n",
              "      <td>[[109802745326785766951_60524fa9f09a4ffff042f9...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                business_id                user_id  rating  \\\n",
              "0  60567465d335d0abfb415b26  101074926318992653684       4   \n",
              "1  6050fa9f5b4ccec8d5cae994  117065749986299237881       5   \n",
              "2  604be10877e81aaed3cc9a1e  106700937793048450809       4   \n",
              "3  60411e017cd8bf130362365a  101643045857250355161       5   \n",
              "4  604139dd7cd8bf1303624208  109802745326785766951       4   \n",
              "\n",
              "                                         review_text  \\\n",
              "0  The tang of the tomato sauce is outstanding. A...   \n",
              "1              Chicken and waffles were really good!   \n",
              "2  The appetizer of colossal shrimp was very good...   \n",
              "3  The fish tacos here  omg! The salad was great ...   \n",
              "4  Ribs are great, as are the mac and cheese, fri...   \n",
              "\n",
              "                                                pics  \\\n",
              "0  [AF1QipM-2IRmvitARbcJr7deWfe5hyVBg_ArPMQSYvq0,...   \n",
              "1     [AF1QipMpfxIZUT_aymQ3qPGO-QgGYzxbtLZGmHufAp2s]   \n",
              "2  [AF1QipMNnqM5X9sSyZ9pXRZ1jvrURHN9bZhGdzuEXoP8,...   \n",
              "3  [AF1QipM-a6AGGp4Hgk5RD0gY5sDRp5kEfB1hZLvlRkft,...   \n",
              "4     [AF1QipNVys4yq-5w_3EsDdHpSc9ZNb7Nl30Mfb6Y0Gup]   \n",
              "\n",
              "                                     history_reviews  \n",
              "0  [[101074926318992653684_6056272797d555cc6fb0d1...  \n",
              "1  [[117065749986299237881_605206f8d8c08f462b93e8...  \n",
              "2  [[106700937793048450809_6044300b27f39b7b5d1dbf...  \n",
              "3  [[101643045857250355161_604fbdd099686c10168c91...  \n",
              "4  [[109802745326785766951_60524fa9f09a4ffff042f9...  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pgE0s2EcADG"
      },
      "source": [
        "Let us map `user_id`s and `business_id`s to consecutive values so they are easier to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECCv74ZTcADG"
      },
      "outputs": [],
      "source": [
        "def create_id_mappings(data):\n",
        "  '''\n",
        "  Args:\n",
        "    data (pd.DataFrame)\n",
        "  Returns:\n",
        "    data (pd.DataFrame)\n",
        "    user_id_map (dict)\n",
        "    business_id_map (dict)\n",
        "  '''\n",
        "  # Create a dictionary mapping user_id to consecutive values [0,..., n]\n",
        "  user_id_map = {idx: i for i, idx in enumerate(data[\"user_id\"].unique())}\n",
        "  # Create a dictionary mapping business_id to consecutive values [0,..., m]\n",
        "  business_id_map = {idx: i for i, idx in enumerate(data[\"business_id\"].unique())}\n",
        "\n",
        "  # Extend the dataframe with new ids\n",
        "  data[\"u_id\"] = data[\"user_id\"].map(user_id_map)\n",
        "  data[\"b_id\"] = data[\"business_id\"].map(business_id_map)\n",
        "  data[\"r_id\"] = data.index\n",
        "\n",
        "  return data, user_id_map, business_id_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOhFCNkbcADG",
        "outputId": "c2092284-f96c-43ea-ed22-56ca3bc12175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of users:  29596\n",
            "The number of businesses:  27896\n",
            "The number of nodes:  57492\n"
          ]
        }
      ],
      "source": [
        "data, user_id_map, business_id_map = create_id_mappings(df)\n",
        "num_users, num_businesses = len(user_id_map), len(business_id_map)\n",
        "num_nodes = num_users + num_businesses\n",
        "\n",
        "print(\"The number of users: \", num_users)\n",
        "print(\"The number of businesses: \", num_businesses)\n",
        "print(\"The number of nodes: \", num_nodes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate feature embeddings for ndoes"
      ],
      "metadata": {
        "id": "vMQg4HP6ku5C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38M1N0QCcADG"
      },
      "source": [
        "We load image and text embeddings into memory. For extremely large datasets, it would be more efficient to utilize some vector store here.\n",
        "\n",
        "Additionally, we have pre-computed two dictionaries: one maps each business to its associated image IDs, and the other maps each user to their associated text IDs. The idea is to later represent each business as the aggregation of its image embeddings and each user as the aggregation of their review text embeddings. There are many other ways to do this, and we encourage readers to explore additional schemes. The only caveat here is to use the `.pkl` file format instead of `.npz`, as the latter is much slower to load into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4s9TMsWcADG",
        "outputId": "6d176bef-06b2-4f2a-a18f-1c2cff878796"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of businesses: 27896\n",
            "Number of users: 29596\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# load pre-computed embeddings, the code for computing embeddings can be seen in the github repo\n",
        "IMAGE_EMBEDDINGS_ = np.load('embeddings/data/embeddings_pics_train.pkl', allow_pickle=True)\n",
        "TEXT_EMBEDDINGS_ = np.load('embeddings/data/embeddings_text_train.pkl', allow_pickle=True)\n",
        "\n",
        "\n",
        "UNIQUE_BUSINESS_IDS = df['business_id'].unique()\n",
        "UNIQUE_USER_IDS = df['user_id'].unique()\n",
        "\n",
        "print(\"Number of businesses:\", len(UNIQUE_BUSINESS_IDS))\n",
        "print(\"Number of users:\", len(UNIQUE_USER_IDS))\n",
        "\n",
        "business_to_image_keys = pickle.load(open('data/business_image_mapping.pkl', 'rb'))\n",
        "business_to_image_keys = {item['business_id']: item['image_keys'] for item in business_to_image_keys}\n",
        "\n",
        "user_to_text_keys = pickle.load(open('data/user_text_mapping.pkl', 'rb'))\n",
        "user_to_text_keys = {item['user_id']: [item['text_key']] for item in user_to_text_keys}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHzyAlyvcADG",
        "outputId": "cc3ead8f-f569-4d2a-8ec4-65f6c8e6b7e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([27896, 768])\n",
            "torch.Size([29596, 768])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2203/3094676195.py:23: RuntimeWarning: Mean of empty slice.\n",
            "  text_tensors = torch.from_numpy(np.stack([TEXT_EMBEDDINGS_[k].mean(axis=0) for k in text_key]))\n",
            "/home/azureuser/.conda/envs/gnn/lib/python3.10/site-packages/numpy/_core/_methods.py:137: RuntimeWarning: invalid value encountered in divide\n",
            "  ret = um.true_divide(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# create embeddings for business\n",
        "business_features = torch.zeros((len(UNIQUE_BUSINESS_IDS), 768))\n",
        "print(business_features.shape)\n",
        "\n",
        "for i, biz_id in enumerate(UNIQUE_BUSINESS_IDS):\n",
        "    # Process image embeddings in batches\n",
        "    img_keys = business_to_image_keys.get(biz_id, None)\n",
        "    if img_keys:\n",
        "        # Convert all at once\n",
        "        img_tensors = torch.from_numpy(np.stack([IMAGE_EMBEDDINGS_[k] for k in img_keys]))\n",
        "        # Use mean aggregation of the image embeddings to represent a business\n",
        "        img_embedding = img_tensors.mean(dim=0)\n",
        "    else:\n",
        "    # deal with cases where images cannot be found\n",
        "        img_embedding = torch.zeros(768)\n",
        "    img_embedding = torch.nan_to_num(img_embedding, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "    business_features[i] = img_embedding\n",
        "\n",
        "# create embeddings for users\n",
        "user_features = torch.zeros((len(UNIQUE_USER_IDS), 768))\n",
        "print(user_features.shape)\n",
        "for i, user_id in enumerate(UNIQUE_USER_IDS):\n",
        "    # Process text embeddings in batches\n",
        "    text_key = user_to_text_keys.get(user_id, None)\n",
        "    if text_key:\n",
        "        text_tensors = torch.from_numpy(np.stack([TEXT_EMBEDDINGS_[k].mean(axis=0) for k in text_key]))\n",
        "        # Use mean aggregation of the text embeddings to represent a user\n",
        "        text_tensors = text_tensors.mean(axis=0)\n",
        "    else:\n",
        "    # deal with cases where texts cannot be found\n",
        "        text_tensors = torch.zeros(768)\n",
        "    text_tensors = torch.nan_to_num(text_tensors, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "    user_features[i] = text_tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO33_jz6cADG",
        "outputId": "37c56dac-356f-47d4-ef9e-5c7c67ca0d22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([57492, 768])\n",
            "torch.Size([57492, 768])\n"
          ]
        }
      ],
      "source": [
        "# check the sizes of the feature embeddings\n",
        "node_features = torch.cat([user_features, business_features], dim=0)\n",
        "print(node_features.shape)\n",
        "rand_embeddings = torch.nn.init.xavier_uniform_(torch.empty((num_nodes, node_features.shape[1])))\n",
        "print(rand_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augment the dataset"
      ],
      "metadata": {
        "id": "OKVfEyEij64K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXbd6niCcADH"
      },
      "source": [
        "We inspected the dataset and found the original dataset is very sparse, meaning, every user is only connected to one or two restaurants. This makes it not suitable for a recommendation task. To fix it, we augment the dataset by sampling additional (user, restaurant) edges. For one in very 10 restaurant nodes, we sample `k` additional nearest neighbors based on the cosine similarities of their feature vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfeBon2RcADH"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def find_k_similar(business_features: torch.Tensor,\n",
        "                  idx: int,\n",
        "                  k: int = 5,\n",
        "                  use_cosine: bool = True) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Find k most similar tensors to tensor at index idx\n",
        "    Returns: tuple of (indices, similarity_scores)\n",
        "    \"\"\"\n",
        "    # Get the query tensor\n",
        "    query = business_features[idx]\n",
        "\n",
        "    if use_cosine:\n",
        "        # Normalize the features (for cosine similarity)\n",
        "        normalized_features = F.normalize(business_features, p=2, dim=1)\n",
        "        normalized_query = F.normalize(query.unsqueeze(0), p=2, dim=1)\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        similarities = torch.mm(normalized_query, normalized_features.T).squeeze()\n",
        "    else:\n",
        "        # Compute dot product\n",
        "        similarities = torch.mm(query.unsqueeze(0), business_features.T).squeeze()\n",
        "\n",
        "    # Get top k (excluding the query itself)\n",
        "    similarities[idx] = -float('inf')  # Exclude self\n",
        "    top_k_similarities, top_k_indices = torch.topk(similarities, k)\n",
        "\n",
        "    return top_k_indices, top_k_similarities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26WCQDjqcADH"
      },
      "outputs": [],
      "source": [
        "from torch_sparse import SparseTensor\n",
        "\n",
        "def generate_edge_index(data, user_id_map, business_id_map, business_features, k=50):\n",
        "    '''\n",
        "    Args:\n",
        "        data              (pd.DataFrame)\n",
        "        user_id_map       (dict)\n",
        "        business_id_map   (dict)\n",
        "        business_features (torch.tensor)\n",
        "        k                 (int): number of similar businesses\n",
        "    Returns:\n",
        "        edge_index        (torch.tensor)\n",
        "        edge_index_sparse (SparseTensor)\n",
        "    '''\n",
        "    # Use a set to store unique edges\n",
        "    edge_set = set()\n",
        "    print(len(data))\n",
        "    for i in range(len(data)):\n",
        "        uid = user_id_map[data[\"user_id\"][i]]\n",
        "        bid = business_id_map[data[\"business_id\"][i]]\n",
        "\n",
        "        # Add original edge\n",
        "        edge_set.add((uid, bid + len(user_id_map)))\n",
        "\n",
        "        # Find similar businesses and add edges\n",
        "        if i % 10 == 0:\n",
        "          indices, _ = find_k_similar(business_features, bid, k)\n",
        "          for idx in indices:\n",
        "              edge_set.add((uid, idx.item() + len(user_id_map)))\n",
        "\n",
        "    # Convert set to edge_index format\n",
        "    edge_index = torch.tensor([[edge[0] for edge in edge_set],\n",
        "                             [edge[1] for edge in edge_set]])\n",
        "\n",
        "    # Create sparse tensor\n",
        "    num_nodes = len(user_id_map) + len(business_id_map)\n",
        "    edge_index_sparse = SparseTensor(\n",
        "        row=edge_index[0],\n",
        "        col=edge_index[1],\n",
        "        sparse_sizes=(num_nodes, num_nodes)\n",
        "    )\n",
        "\n",
        "    return edge_index, edge_index_sparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKEg2pJ6cADH"
      },
      "source": [
        "We can now create `PyG.data.Data` objects and use the `RandomLinkSplit` method to split our graph into train, val, and test components. Because the sampling procedure takes some time, we have saved the results as pickle objects that we can conveniently reload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbAKLJXbcADH"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "\n",
        "edge_index, edge_index_sparse = generate_edge_index(data, user_id_map, business_id_map, business_features)\n",
        "graph_data = Data(edge_index = edge_index, num_nodes = num_nodes)\n",
        "\n",
        "train_split, val_split, test_split = RandomLinkSplit(\n",
        "    is_undirected=True,\n",
        "    add_negative_train_samples=False,\n",
        "    num_val=0.2, num_test=0.1\n",
        ")(graph_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9fhM7sVcADH",
        "outputId": "e6b91104-ae7b-478d-d10e-bc33a334a9a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch_geometric.data.data.Data'>\n"
          ]
        }
      ],
      "source": [
        "import pickle as pkl\n",
        "train_split = pkl.load(open(\"data/train_split.pkl\", \"rb\"))\n",
        "val_split = pkl.load(open(\"data/val_split.pkl\", \"rb\"))\n",
        "test_split = pkl.load(open(\"data/test_split.pkl\", \"rb\"))\n",
        "\n",
        "print(type(train_split))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBOsy8TvcADH"
      },
      "source": [
        "This completes all the data preprocessing we need to start training. The `torch_geometric.data.data.Data` provides message-passing edges as well as supervision edges that are accessible via the `edge_index` and `edge_label_index` attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdlv7BmScADH"
      },
      "source": [
        "# 2. Creating a Custom LightGCN Recommender Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiPVB3y9cADH"
      },
      "source": [
        "The next step is to create a modded LightGCN model that supports using multi-modal features in training. We thought about different ways to this. One naive way is to initialize node embeddings with the pre-trained features. However, this means the model only uses this knowledge in initialization, and it gradually \"disregards\" the original information. We tested this empirically, and the result is no different from random (Xavier and He) initialization.\n",
        "\n",
        "Our solution is to find a way to incorporate this knowledge during training. Specifically, in each forward pass, the model gets the associated embeddings of each node via the `self.get_embedding` API. Our key modification is to also let the model learn a trainable weight matrix that transforms the pre-trained embeddings and adds them to the original node embeddings. In code, this is:\n",
        "\n",
        "```\n",
        "if self.has_clip_features:\n",
        "    x = self.feature_weight * x + (1 - self.feature_weight) * self.clip_features\n",
        "```\n",
        "\n",
        "This is effective for our dataset, and we intend to test on other benchmarks.\n",
        "\n",
        "Our implementation is in `src/lgcn/model.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp9onUCQcADH"
      },
      "outputs": [],
      "source": [
        "# You can also our implementations directly\n",
        "from lgcn.model import LightGCN\n",
        "from lgcn.utils.negative_sampling import get_negative_samples\n",
        "from lgcn.utils.recall import recall_at_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buhYinEJcADH"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Model adapted from LightGCN source code:\n",
        "https://pytorch-geometric.readthedocs.io/en/2.5.2/_modules/torch_geometric/nn/models/lightgcn.html#LightGCN\n",
        "\"\"\"\n",
        "from typing import Optional, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter, Embedding, ModuleList\n",
        "from torch.nn.modules.loss import _Loss\n",
        "\n",
        "from torch_geometric.nn.conv import LGConv\n",
        "from torch_geometric.typing import Adj, OptTensor\n",
        "from torch_geometric.utils import is_sparse, to_edge_index\n",
        "\n",
        "class LightGCN(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_nodes: int,\n",
        "        embedding_dim: int,\n",
        "        num_layers: int,\n",
        "        alpha: Optional[Union[float, Tensor]] = None,\n",
        "        has_clip_features = False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_nodes = num_nodes\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.has_clip_features = has_clip_features\n",
        "\n",
        "        # add clip_features to the model\n",
        "        if self.has_clip_features:\n",
        "            self.clip_features = Embedding(num_nodes, embedding_dim)\n",
        "            self.feature_weight = Parameter(torch.tensor(0.5))\n",
        "\n",
        "        if alpha is None:\n",
        "            alpha = 1. / (num_layers + 1)\n",
        "\n",
        "        if isinstance(alpha, Tensor):\n",
        "            assert alpha.size(0) == num_layers + 1\n",
        "        else:\n",
        "            alpha = torch.tensor([alpha] * (num_layers + 1))\n",
        "\n",
        "        self.register_buffer('alpha', alpha)\n",
        "\n",
        "        self.embedding = Embedding(num_nodes, embedding_dim)\n",
        "        self.convs = ModuleList([LGConv(**kwargs) for _ in range(num_layers)])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def initialize_clip_features(self, data: torch.Tensor):\n",
        "        r\"\"\"Initialize this model with pre-trained CLIP features.\"\"\"\n",
        "        if not self.has_clip_features:\n",
        "            raise ValueError(\"Model was not initialized with CLIP features support\")\n",
        "\n",
        "        self.clip_features.weight.data.copy_(data)\n",
        "\n",
        "    def initialize_node_embedding(self, data: torch.Tensor):\n",
        "        self.embedding.weight.data.copy_(data)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def get_embedding(self, edge_index: Adj) -> Tensor:\n",
        "        x = self.embedding.weight\n",
        "\n",
        "        # create a weighted combination of clip features and learned embeddings\n",
        "        if self.has_clip_features:\n",
        "            x = self.feature_weight * x + (1 - self.feature_weight) * self.clip_features.weight\n",
        "        out = x * self.alpha[0]\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            out = out + x * self.alpha[i + 1]\n",
        "\n",
        "        return out\n",
        "\n",
        "    def forward(self, edge_index: Adj,\n",
        "                edge_label_index: OptTensor = None) -> Tensor:\n",
        "        if edge_label_index is None:\n",
        "            if is_sparse(edge_index):\n",
        "                edge_label_index, _ = to_edge_index(edge_index)\n",
        "            else:\n",
        "                edge_label_index = edge_index\n",
        "\n",
        "        out = self.get_embedding(edge_index)\n",
        "\n",
        "        out_src = out[edge_label_index[0]]\n",
        "        out_dst = out[edge_label_index[1]]\n",
        "\n",
        "        return (out_src * out_dst).sum(dim=-1)\n",
        "\n",
        "    def predict_link(self, edge_index: Adj, edge_label_index: OptTensor = None,\n",
        "                     prob: bool = False) -> Tensor:\n",
        "\n",
        "        pred = self(edge_index, edge_label_index).sigmoid()\n",
        "        return pred if prob else pred.round()\n",
        "\n",
        "\n",
        "    def recommend(self, edge_index: Adj, src_index: OptTensor = None,\n",
        "                  dst_index: OptTensor = None, k: int = 1) -> Tensor:\n",
        "        out_src = out_dst = self.get_embedding(edge_index)\n",
        "\n",
        "        if src_index is not None:\n",
        "            out_src = out_src[src_index]\n",
        "\n",
        "        if dst_index is not None:\n",
        "            out_dst = out_dst[dst_index]\n",
        "\n",
        "        pred = out_src @ out_dst.t()\n",
        "        top_index = pred.topk(k, dim=-1).indices\n",
        "\n",
        "        if dst_index is not None:  # Map local top-indices to original indices.\n",
        "            top_index = dst_index[top_index.view(-1)].view(*top_index.size())\n",
        "\n",
        "        return top_index\n",
        "\n",
        "\n",
        "    def link_pred_loss(self, pred: Tensor, edge_label: Tensor,\n",
        "                       **kwargs) -> Tensor:\n",
        "        loss_fn = torch.nn.BCEWithLogitsLoss(**kwargs)\n",
        "        return loss_fn(pred, edge_label.to(pred.dtype))\n",
        "\n",
        "\n",
        "    def recommendation_loss(self, pos_edge_rank: Tensor, neg_edge_rank: Tensor,\n",
        "                            node_id: Optional[Tensor] = None,\n",
        "                            lambda_reg: float = 1e-4, **kwargs) -> Tensor:\n",
        "        r\"\"\"Computes the model loss for a ranking objective via the Bayesian\n",
        "        Personalized Ranking (BPR) loss.\"\"\"\n",
        "        loss_fn = BPRLoss(lambda_reg, **kwargs)\n",
        "        emb = self.embedding.weight\n",
        "        emb = emb if node_id is None else emb[node_id]\n",
        "        return loss_fn(pos_edge_rank, neg_edge_rank, emb)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.num_nodes}, '\n",
        "                f'{self.embedding_dim}, num_layers={self.num_layers})')\n",
        "\n",
        "\n",
        "class BPRLoss(_Loss):\n",
        "    r\"\"\"The Bayesian Personalized Ranking (BPR) loss.\n",
        "\n",
        "    The BPR loss is a pairwise loss that encourages the prediction of an\n",
        "    observed entry to be higher than its unobserved counterparts\n",
        "    (see `here <https://arxiv.org/abs/2002.02126>`__).\n",
        "\n",
        "    .. math::\n",
        "        L_{\\text{BPR}} = - \\sum_{u=1}^{M} \\sum_{i \\in \\mathcal{N}_u}\n",
        "        \\sum_{j \\not\\in \\mathcal{N}_u} \\ln \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})\n",
        "        + \\lambda \\vert\\vert \\textbf{x}^{(0)} \\vert\\vert^2\n",
        "\n",
        "    where :math:`lambda` controls the :math:`L_2` regularization strength.\n",
        "    We compute the mean BPR loss for simplicity.\n",
        "\n",
        "    Args:\n",
        "        lambda_reg (float, optional): The :math:`L_2` regularization strength\n",
        "            (default: 0).\n",
        "        **kwargs (optional): Additional arguments of the underlying\n",
        "            :class:`torch.nn.modules.loss._Loss` class.\n",
        "    \"\"\"\n",
        "    __constants__ = ['lambda_reg']\n",
        "    lambda_reg: float\n",
        "\n",
        "    def __init__(self, lambda_reg: float = 0, **kwargs):\n",
        "        super().__init__(None, None, \"sum\", **kwargs)\n",
        "        self.lambda_reg = lambda_reg\n",
        "\n",
        "    def forward(self, positives: Tensor, negatives: Tensor,\n",
        "                parameters: Tensor = None) -> Tensor:\n",
        "        r\"\"\"Compute the mean Bayesian Personalized Ranking (BPR) loss.\n",
        "\n",
        "        .. note::\n",
        "\n",
        "            The i-th entry in the :obj:`positives` vector and i-th entry\n",
        "            in the :obj:`negatives` entry should correspond to the same\n",
        "            entity (*.e.g*, user), as the BPR is a personalized ranking loss.\n",
        "\n",
        "        Args:\n",
        "            positives (Tensor): The vector of positive-pair rankings.\n",
        "            negatives (Tensor): The vector of negative-pair rankings.\n",
        "            parameters (Tensor, optional): The tensor of parameters which\n",
        "                should be used for :math:`L_2` regularization\n",
        "                (default: :obj:`None`).\n",
        "        \"\"\"\n",
        "        log_prob = F.logsigmoid(positives - negatives).mean()\n",
        "\n",
        "        regularization = 0\n",
        "        if self.lambda_reg != 0:\n",
        "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n",
        "            regularization = regularization / positives.size(0)\n",
        "\n",
        "        return -log_prob + regularization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUCcHkajcADH"
      },
      "source": [
        "Two additional functions we had to implement are `get_negative_samples`, which samples negative edges, and `recall_at_k`, which calculate the overlap between recommended edges and true positives.\n",
        "\n",
        "Our initial implementation had serious performance issues. We would like to acknowledge that [this notebook](https://colab.research.google.com/drive/1DhPrtHLggaObSyKjyCQNw0Z-k8vzXZHh?usp=sharing) on building a recommender system for Spotify tracks has a very efficient implementation. We learned the approach and found that using tensor operations like `gather()` greatly helped boost efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQjb79lWcADH"
      },
      "outputs": [],
      "source": [
        "def get_negative_samples(data, num_users, num_items):\n",
        "    \"\"\"\n",
        "    Generate negative samples for user-business interactions\n",
        "    Args:\n",
        "        data: PyG Graph data object containing positive edges\n",
        "        num_users: Total number of users\n",
        "        num_items: Total number of items\n",
        "    Returns:\n",
        "        tuple: (negative edge indices, negative edge labels)\n",
        "    \"\"\"\n",
        "    # Get positive interactions\n",
        "    pos_users, pos_items = data.edge_label_index\n",
        "    device = data.edge_label_index.device\n",
        "\n",
        "    # Initialize interaction matrix\n",
        "    interactions = torch.zeros(\n",
        "        (num_users, num_items),\n",
        "        device=device,\n",
        "        dtype=torch.bool,\n",
        "    )\n",
        "\n",
        "    # Mark positive interactions\n",
        "    business_indices = pos_items - num_users  # Adjust business indices\n",
        "    interactions[pos_users, business_indices] = True\n",
        "\n",
        "    # Find all possible negative interactions\n",
        "    available_negatives = torch.where(~interactions.reshape(-1))[0]\n",
        "\n",
        "    # Sample random negative interactions\n",
        "    num_samples = pos_users.size(0)\n",
        "    sampled_indices = available_negatives[\n",
        "        torch.randint(\n",
        "            0,\n",
        "            available_negatives.size(0),\n",
        "            size=(num_samples,),\n",
        "            device=device\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Convert linear indices to user-business pairs\n",
        "    sampled_users = sampled_indices // num_items\n",
        "    sampled_items = (sampled_indices % num_items) + num_users\n",
        "\n",
        "    # Create negative edge tensor\n",
        "    neg_edges = torch.stack((sampled_users, sampled_items), dim=0)\n",
        "    neg_labels = torch.zeros(neg_edges.shape[1], device=device)\n",
        "\n",
        "    return neg_edges, neg_labels\n",
        "\n",
        "def recall_at_k(data, model, num_users, k=500):\n",
        "    \"\"\"\n",
        "    Calculate recall@k for recommendations.\n",
        "    Returns average recall score across all users.\n",
        "\n",
        "    Args:\n",
        "        data: Graph data object containing edge indices and labels\n",
        "        model: Neural network model with get_embedding method\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get embeddings for users and items\n",
        "        embeddings = model.get_embedding(data.edge_index)\n",
        "        user_embeds, item_embeds = embeddings[:num_users], embeddings[num_users:]\n",
        "\n",
        "        # Calculate similarities and initialize truth matrix\n",
        "        similarities = torch.matmul(user_embeds, item_embeds.t())\n",
        "        truth = torch.zeros_like(similarities, dtype=torch.bool)\n",
        "\n",
        "        # Get training and supervision edge masks\n",
        "        train_edges = data.edge_index[:, data.edge_index[0] < num_users]\n",
        "        sup_edges = data.edge_label_index[:, data.edge_label_index[0] < num_users]\n",
        "\n",
        "        # Mask out training edges from recommendations\n",
        "        similarities[train_edges[0], train_edges[1] - num_users] = float('-inf')\n",
        "\n",
        "        # Mark ground truth edges\n",
        "        truth[sup_edges[0], sup_edges[1] - num_users] = True\n",
        "\n",
        "        # Calculate recall\n",
        "        topk_scores, topk_items = torch.topk(similarities, k, dim=1)\n",
        "        hits = truth.gather(1, topk_items).sum(dim=1)\n",
        "\n",
        "        # Calculate total relevant items per user\n",
        "        relevants = torch.bincount(sup_edges[0], minlength=num_users)\n",
        "\n",
        "        # Compute recall, handling users with no relevant items\n",
        "        recalls = torch.where(\n",
        "            relevants > 0,\n",
        "            hits.float() / relevants.float(),\n",
        "            torch.ones_like(relevants, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "        return recalls.mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "modRArfmcADH"
      },
      "source": [
        "# 3. Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RhkWMAGcADH"
      },
      "source": [
        "We study the effects of multi-modal feature integration and learnable feature fusion, as described in model implementation.\n",
        "We compare the performance of the augmented model with a baseline model, where we use Xavier initialization to generate node embeddings and do not incorporate the weight and feature matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXpMp_PucADI"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, train_split, val_split, num_epochs=81, run = None):\n",
        "  \"\"\"\n",
        "  Train the model using the given optimizer, train_split, num_epochs, and perform validation test on val_split\n",
        "\n",
        "  Args:\n",
        "    model: the model we want to train\n",
        "    train_split: the training data\n",
        "    val_split: the validation data\n",
        "    run: wandb run object\n",
        "  \"\"\"\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # sample negative edges\n",
        "        edge_index_negative, _ = get_negative_samples(train_split, num_users, num_businesses)\n",
        "\n",
        "        out = model.get_embedding(train_split.edge_index)\n",
        "\n",
        "        # perform gradient update using training data\n",
        "        train_src = out[train_split.edge_label_index[0]]\n",
        "        train_dst = out[train_split.edge_label_index[1]]\n",
        "        pos_scores = (train_src * train_dst).sum(dim=-1)\n",
        "\n",
        "        neg_src = out[edge_index_negative[0]]\n",
        "        neg_dst = out[edge_index_negative[1]]\n",
        "        neg_scores = (neg_src * neg_dst).sum(dim=-1)\n",
        "        loss = model.recommendation_loss(pos_scores, neg_scores)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # calculate the loss on validation data\n",
        "        edge_index_negative_val, _ = get_negative_samples(val_split, num_users, num_businesses)\n",
        "        val_src = out[val_split.edge_label_index[0]]\n",
        "        val_dst = out[val_split.edge_label_index[1]]\n",
        "        pos_scores = (val_src * val_dst).sum(dim=-1)\n",
        "\n",
        "        neg_src = out[edge_index_negative_val[0]]\n",
        "        neg_dst = out[edge_index_negative_val[1]]\n",
        "        neg_scores = (neg_src * neg_dst).sum(dim=-1)\n",
        "\n",
        "        val_loss = model.recommendation_loss(pos_scores, neg_scores)\n",
        "\n",
        "        # evaluate performance on validation every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            val_recall = recall_at_k(val_split, model, num_users)\n",
        "            print(f\"Epoch {epoch}, Train loss {loss}, Val loss {val_loss}, Val Recall@500 {val_recall}\")\n",
        "            if run:\n",
        "                run.log({\"epoch\": epoch, \"train/loss\": loss, \"val/loss\": val_loss, \"val/recall@500\": val_recall})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th2c4yVXcADI",
        "outputId": "77d92b83-3e07-4632-d236-3c440a8ee2ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 727582], num_nodes=57492, edge_label=[77954], edge_label_index=[2, 77954])"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model initialization\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LightGCN(num_nodes, 768, num_layers = 3, has_clip_features = True).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.5, weight_decay=1e-5)\n",
        "\n",
        "train_split.to(device)\n",
        "val_split.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightGCN + CLIP embeddings"
      ],
      "metadata": {
        "id": "-VnqZ4aXn7ht"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg3FHiNdcADI"
      },
      "outputs": [],
      "source": [
        "# initialize clip features\n",
        "if model.has_clip_features:\n",
        "    model.initialize_clip_features(node_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPFWLICBcADI",
        "outputId": "9ae941fe-747f-4d0a-badd-35b0509126d9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:rilvy1sd) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>train/loss</td><td>█▂▁▁▂▂▂▁▁▁▂▃▂</td></tr><tr><td>val/loss</td><td>▆▂▂▂▄▆▄▁▁▂▅█▂</td></tr><tr><td>val/recall@500</td><td>▁▇▇▇▆▅▆▇██▆▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>120</td></tr><tr><td>train/loss</td><td>0.08344</td></tr><tr><td>val/loss</td><td>0.19308</td></tr><tr><td>val/recall@500</td><td>0.67333</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">with_clip_features_8d_autoencoder</strong> at: <a href='https://wandb.ai/ronyw/224w-project/runs/rilvy1sd' target=\"_blank\">https://wandb.ai/ronyw/224w-project/runs/rilvy1sd</a><br/> View project at: <a href='https://wandb.ai/ronyw/224w-project' target=\"_blank\">https://wandb.ai/ronyw/224w-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241213_034254-rilvy1sd/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:rilvy1sd). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/azureuser/cs224w-proj/src/wandb/run-20241213_034733-irm8gjm4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ronyw/224w-project/runs/irm8gjm4' target=\"_blank\">with_clip_features</a></strong> to <a href='https://wandb.ai/ronyw/224w-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ronyw/224w-project' target=\"_blank\">https://wandb.ai/ronyw/224w-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ronyw/224w-project/runs/irm8gjm4' target=\"_blank\">https://wandb.ai/ronyw/224w-project/runs/irm8gjm4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Train loss 0.7028226852416992, Val loss 1.0535435676574707, Val Recall@500 0.617391049861908\n",
            "Epoch 10, Train loss 0.20973829925060272, Val loss 0.3511536121368408, Val Recall@500 0.5997821688652039\n",
            "Epoch 20, Train loss 0.17751699686050415, Val loss 0.22771747410297394, Val Recall@500 0.6284925937652588\n",
            "Epoch 30, Train loss 0.11669503152370453, Val loss 0.19768154621124268, Val Recall@500 0.6381189823150635\n",
            "Epoch 40, Train loss 0.08847761154174805, Val loss 0.16044510900974274, Val Recall@500 0.7018598318099976\n",
            "Epoch 50, Train loss 0.06255592405796051, Val loss 0.14117825031280518, Val Recall@500 0.7100704908370972\n",
            "Epoch 60, Train loss 0.0509820431470871, Val loss 0.1321713626384735, Val Recall@500 0.7304561138153076\n",
            "Epoch 70, Train loss 0.041059356182813644, Val loss 0.13186897337436676, Val Recall@500 0.730961263179779\n",
            "Epoch 80, Train loss 0.0361776165664196, Val loss 0.13592469692230225, Val Recall@500 0.7276843190193176\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# perform experiments using wandb\n",
        "run = wandb.init(\n",
        "    project=\"224w-project\",\n",
        "    config={'clip_features': model.has_clip_features},\n",
        "    reinit=True,\n",
        "    name='with_clip_features'\n",
        ")\n",
        "\n",
        "train(model, optimizer, train_split, val_split, num_epochs=81, run=run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u1H6t-pcADI",
        "outputId": "58107884-1947-4ce6-d818-2a90b0123d2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.731330156326294"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# evaluate on test split\n",
        "\n",
        "test_split.to(device)\n",
        "test_recall = recall_at_k(test_split, model, num_users)\n",
        "test_recall"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline LightGCN"
      ],
      "metadata": {
        "id": "JLXTRTU5n4YT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d697eP72cADI"
      },
      "source": [
        "Let us compare this with the baseline result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwpXEyzhcADI",
        "outputId": "1948aa15-466a-40ff-bd96-6cf0ba7d0360"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 727582], num_nodes=57492, edge_label=[77954], edge_label_index=[2, 77954])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model initialization\n",
        "HAS_CLIP_FEATURES = False\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LightGCN(num_nodes, 768, num_layers = 3, has_clip_features = HAS_CLIP_FEATURES).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.5, weight_decay=1e-5)\n",
        "\n",
        "train_split.to(device)\n",
        "val_split.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSFPsHApcADI",
        "outputId": "fc220ccc-28f5-4704-e389-23f03b57646b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:7hl6ywpi) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">with_clip_features</strong> at: <a href='https://wandb.ai/ronyw/224w-project/runs/7hl6ywpi' target=\"_blank\">https://wandb.ai/ronyw/224w-project/runs/7hl6ywpi</a><br/> View project at: <a href='https://wandb.ai/ronyw/224w-project' target=\"_blank\">https://wandb.ai/ronyw/224w-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241213_033908-7hl6ywpi/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:7hl6ywpi). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/azureuser/cs224w-proj/src/wandb/run-20241213_033926-sz8h43zt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ronyw/224w-project/runs/sz8h43zt' target=\"_blank\">with_xavier_init</a></strong> to <a href='https://wandb.ai/ronyw/224w-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ronyw/224w-project' target=\"_blank\">https://wandb.ai/ronyw/224w-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ronyw/224w-project/runs/sz8h43zt' target=\"_blank\">https://wandb.ai/ronyw/224w-project/runs/sz8h43zt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Train loss 0.6929919719696045, Val loss 0.7015377879142761, Val Recall@500 0.5252739191055298\n",
            "Epoch 10, Train loss 0.22964796423912048, Val loss 0.2945592999458313, Val Recall@500 0.6358852982521057\n",
            "Epoch 20, Train loss 0.27541419863700867, Val loss 0.32218989729881287, Val Recall@500 0.5855546593666077\n",
            "Epoch 30, Train loss 0.2691667377948761, Val loss 0.2984590530395508, Val Recall@500 0.5783751010894775\n",
            "Epoch 40, Train loss 0.25886270403862, Val loss 0.2817089259624481, Val Recall@500 0.5817865133285522\n",
            "Epoch 50, Train loss 0.26367825269699097, Val loss 0.2834400236606598, Val Recall@500 0.5841618180274963\n",
            "Epoch 60, Train loss 0.26217812299728394, Val loss 0.2826944589614868, Val Recall@500 0.5819141864776611\n",
            "Epoch 70, Train loss 0.26327621936798096, Val loss 0.2827521562576294, Val Recall@500 0.5821539163589478\n",
            "Epoch 80, Train loss 0.262026846408844, Val loss 0.2822498679161072, Val Recall@500 0.5828391313552856\n"
          ]
        }
      ],
      "source": [
        "# train using wandb\n",
        "run = wandb.init(\n",
        "    project=\"224w-project\",\n",
        "    config={'clip_features': model.has_clip_features},\n",
        "    reinit=True,\n",
        "    name='with_xavier_init'\n",
        ")\n",
        "\n",
        "train(model, optimizer, train_split, val_split, num_epochs=81, run = run)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimension Reduction"
      ],
      "metadata": {
        "id": "FRzx7XyOoGqI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW2ARCb3cADI"
      },
      "source": [
        "Additionally, we have explored whether _compressed_ feature vectors can still boost model performance. We studied two schemes, PCA and auto-encoders. We found that even when compressed, feature vectors and learnable feature fusion still jointly improve model performance over the base version. The downside is Recall@K, our key metric, seems to be more unstable."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA"
      ],
      "metadata": {
        "id": "bhPd9IhYoSeb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARvgbZsYcADI",
        "outputId": "d2a8b265-4e53-43c4-83e3-acc2bbb8c59f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([57492, 8])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# perform dimension reduction on features using PCA\n",
        "N_DIM = 8\n",
        "pca = PCA(n_components=N_DIM)\n",
        "compressed_features = pca.fit_transform(node_features.numpy())\n",
        "compressed_features = torch.tensor(compressed_features)\n",
        "print(compressed_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POHp7pzncADI",
        "outputId": "b55fb1f1-c12b-4001-be2e-2a022b5cfca5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:yaon6e2h) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>train/loss</td><td>▆▂▁▁▃█▄▁▂▄▂▂▁</td></tr><tr><td>val/loss</td><td>▂▁▁▁▃█▂▁▂▄▁▁▁</td></tr><tr><td>val/recall@500</td><td>▄▄▇█▅▄▁▅▅▄▅▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>120</td></tr><tr><td>train/loss</td><td>0.0373</td></tr><tr><td>val/loss</td><td>0.14707</td></tr><tr><td>val/recall@500</td><td>0.72104</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">with_clip_features128d</strong> at: <a href='https://wandb.ai/ronyw/224w-project/runs/yaon6e2h' target=\"_blank\">https://wandb.ai/ronyw/224w-project/runs/yaon6e2h</a><br/> View project at: <a href='https://wandb.ai/ronyw/224w-project' target=\"_blank\">https://wandb.ai/ronyw/224w-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241213_034103-yaon6e2h/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:yaon6e2h). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/azureuser/cs224w-proj/src/wandb/run-20241213_034117-ozu94ra7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ronyw/224w-project/runs/ozu94ra7' target=\"_blank\">with_clip_features8d</a></strong> to <a href='https://wandb.ai/ronyw/224w-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ronyw/224w-project' target=\"_blank\">https://wandb.ai/ronyw/224w-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ronyw/224w-project/runs/ozu94ra7' target=\"_blank\">https://wandb.ai/ronyw/224w-project/runs/ozu94ra7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized node features\n",
            "Epoch 0, Train loss 0.35747501254081726, Val loss 0.39174678921699524, Val Recall@500 0.6349913477897644\n",
            "Epoch 10, Train loss 0.1674979031085968, Val loss 0.2161165475845337, Val Recall@500 0.6307083964347839\n",
            "Epoch 20, Train loss 0.08177262544631958, Val loss 0.16880294680595398, Val Recall@500 0.6855728626251221\n",
            "Epoch 30, Train loss 0.05585501343011856, Val loss 0.16720707714557648, Val Recall@500 0.6998448967933655\n",
            "Epoch 40, Train loss 0.07032384723424911, Val loss 0.22334684431552887, Val Recall@500 0.6745778918266296\n",
            "Epoch 50, Train loss 0.1065654531121254, Val loss 0.36214086413383484, Val Recall@500 0.6568030118942261\n",
            "Epoch 60, Train loss 0.11154107749462128, Val loss 0.326681911945343, Val Recall@500 0.6498600244522095\n",
            "Epoch 70, Train loss 0.0769653245806694, Val loss 0.2024274319410324, Val Recall@500 0.6701334714889526\n",
            "Epoch 80, Train loss 0.06354409456253052, Val loss 0.16505947709083557, Val Recall@500 0.6929592490196228\n",
            "Epoch 90, Train loss 0.052804429084062576, Val loss 0.15962786972522736, Val Recall@500 0.7000872492790222\n",
            "Epoch 100, Train loss 0.04863937199115753, Val loss 0.17055948078632355, Val Recall@500 0.6952681541442871\n",
            "Epoch 110, Train loss 0.06165120005607605, Val loss 0.2269534170627594, Val Recall@500 0.6714633703231812\n",
            "Epoch 120, Train loss 0.11460597068071365, Val loss 0.40136444568634033, Val Recall@500 0.6485283970832825\n"
          ]
        }
      ],
      "source": [
        "# model initialization for training with compressed embeddings\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"224w-project\",\n",
        "    config={'clip_features': model.has_clip_features},\n",
        "    reinit=True,\n",
        "    name=f'with_clip_features{N_DIM}d'\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LightGCN(num_nodes, N_DIM, num_layers = 3, has_clip_features = True).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.5, weight_decay=1e-5)\n",
        "\n",
        "train_split.to(device)\n",
        "val_split.to(device)\n",
        "\n",
        "if model.has_clip_features:\n",
        "    model.initialize_clip_features(compressed_features)\n",
        "    print(\"Initialized node features\")\n",
        "\n",
        "train(model, optimizer, train_split, val_split, num_epochs=121, run=run)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auto-Encoders"
      ],
      "metadata": {
        "id": "EFcqThuuoqff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx8OgMeccADI",
        "outputId": "deda285c-93e4-4eba-da85-d0f445edc2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.3262\n",
            "Epoch 10, Loss: 0.2006\n",
            "Epoch 20, Loss: 0.1066\n",
            "Epoch 30, Loss: 0.0778\n",
            "Epoch 40, Loss: 0.0693\n",
            "Epoch 50, Loss: 0.0648\n",
            "Epoch 60, Loss: 0.0615\n",
            "Epoch 70, Loss: 0.0594\n",
            "Epoch 80, Loss: 0.0579\n",
            "Epoch 90, Loss: 0.0568\n",
            "Epoch 100, Loss: 0.0559\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Implement an autoencoder class\n",
        "N_DIM = 8\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=N_DIM):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, hidden_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "# Train autoencoder\n",
        "autoencoder = Autoencoder().to(device)\n",
        "optimizer = torch.optim.Adam(autoencoder.parameters())\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "all_features = torch.cat([business_features, user_features], dim=0).to(device)\n",
        "\n",
        "for epoch in range(101):\n",
        "    output = autoencoder(all_features)\n",
        "    loss = criterion(output, all_features)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "compressed_features = autoencoder.encode(all_features)\n",
        "compressed_business = compressed_features[:len(business_features)]\n",
        "compressed_users = compressed_features[len(business_features):]\n",
        "compressed_features = torch.cat([compressed_users, compressed_business], dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63O-lctNcADI",
        "outputId": "9d605449-aa75-4cbd-f90d-60270ad6fd45"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:mdea8bwl) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>train/loss</td><td>█▃▂▂▁▁▁▁▁▁▁▅▄</td></tr><tr><td>val/loss</td><td>▇▂▁▁▁▁▁▁▁▁▁█▅</td></tr><tr><td>val/recall@500</td><td>▁▁▃▆▇▇████▇▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>120</td></tr><tr><td>train/loss</td><td>0.42232</td></tr><tr><td>val/loss</td><td>0.98112</td></tr><tr><td>val/recall@500</td><td>0.65947</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">with_clip_features_128d_autoencoder</strong> at: <a href='https://wandb.ai/ronyw/224w-project/runs/mdea8bwl' target=\"_blank\">https://wandb.ai/ronyw/224w-project/runs/mdea8bwl</a><br/> View project at: <a href='https://wandb.ai/ronyw/224w-project' target=\"_blank\">https://wandb.ai/ronyw/224w-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241213_034239-mdea8bwl/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:mdea8bwl). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/azureuser/cs224w-proj/src/wandb/run-20241213_034254-rilvy1sd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ronyw/224w-project/runs/rilvy1sd' target=\"_blank\">with_clip_features_8d_autoencoder</a></strong> to <a href='https://wandb.ai/ronyw/224w-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ronyw/224w-project' target=\"_blank\">https://wandb.ai/ronyw/224w-project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ronyw/224w-project/runs/rilvy1sd' target=\"_blank\">https://wandb.ai/ronyw/224w-project/runs/rilvy1sd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized node features\n",
            "Epoch 0, Train loss 0.2952347993850708, Val loss 0.3198264241218567, Val Recall@500 0.5913599729537964\n",
            "Epoch 10, Train loss 0.07470818608999252, Val loss 0.1982196569442749, Val Recall@500 0.676962673664093\n",
            "Epoch 20, Train loss 0.06170197203755379, Val loss 0.1843721568584442, Val Recall@500 0.6863926649093628\n",
            "Epoch 30, Train loss 0.05988023430109024, Val loss 0.20966924726963043, Val Recall@500 0.6835346221923828\n",
            "Epoch 40, Train loss 0.07343178987503052, Val loss 0.25717443227767944, Val Recall@500 0.6688758730888367\n",
            "Epoch 50, Train loss 0.09906797111034393, Val loss 0.31763756275177, Val Recall@500 0.656664252281189\n",
            "Epoch 60, Train loss 0.08913768082857132, Val loss 0.24978135526180267, Val Recall@500 0.6618995070457458\n",
            "Epoch 70, Train loss 0.06840551644563675, Val loss 0.17765934765338898, Val Recall@500 0.6879534721374512\n",
            "Epoch 80, Train loss 0.05582723394036293, Val loss 0.16374439001083374, Val Recall@500 0.6975999474525452\n",
            "Epoch 90, Train loss 0.05359400436282158, Val loss 0.19325198233127594, Val Recall@500 0.6907408833503723\n",
            "Epoch 100, Train loss 0.08480742573738098, Val loss 0.3031216561794281, Val Recall@500 0.6639425158500671\n",
            "Epoch 110, Train loss 0.12765179574489594, Val loss 0.3947535753250122, Val Recall@500 0.6484917998313904\n",
            "Epoch 120, Train loss 0.08343861252069473, Val loss 0.1930844783782959, Val Recall@500 0.673333466053009\n"
          ]
        }
      ],
      "source": [
        "# model initialization\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"224w-project\",\n",
        "    config={'clip_features': model.has_clip_features},\n",
        "    reinit=True,\n",
        "    name=f'with_clip_features_{N_DIM}d_autoencoder'\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LightGCN(num_nodes, N_DIM, num_layers = 3, has_clip_features = True).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.5, weight_decay=1e-5)\n",
        "\n",
        "train_split.to(device)\n",
        "val_split.to(device)\n",
        "\n",
        "if model.has_clip_features:\n",
        "    model.initialize_clip_features(compressed_features)\n",
        "    print(\"Initialized node features\")\n",
        "\n",
        "train(model, optimizer, train_split, val_split, num_epochs=121, run=run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhU7gyHzcADI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gnn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}